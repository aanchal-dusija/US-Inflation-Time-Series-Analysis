[
  {
    "objectID": "arimax-sarimax-var.html",
    "href": "arimax-sarimax-var.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "In the previous modeling sections, we analyzed a univariate time series of monthly inflation rates in the United States, calculated from the Consumer Price Index (CPI) data spanning several decades. While ARIMA and SARIMA models provided insights into the inflation trends and seasonal patterns, we aim to enhance our understanding by incorporating endogenous variables into our analysis. Endogenous variables in the context of inflation might include factors like Disposable Income, unemployment rates, Personal Consumption, and major economic policies or events. These variables are determined within the economic system and are influenced by other variables in the system, often demonstrating interdependence and reacting to changes in other economic indicators."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Inflation, a general increase in prices and fall in the purchasing value of money, is a complex and multifaceted economic phenomenon. It impacts every segment of society, from consumers and businesses to policymakers and investors. This exploration focuses on the fluctuations and patterns of US inflation over the decades, tracing its roots, its implications, and its broader economic significance."
  },
  {
    "objectID": "deep-learning-for-TS.html",
    "href": "deep-learning-for-TS.html",
    "title": "Deep Learning for Time Series",
    "section": "",
    "text": "In this section, we shift our focus to the analysis of monthly inflation rates in the United States using Deep Learning techniques, building upon the principles applied in previous sections where time-series models were used for financial and other types of data. Our objective is to predict the monthly inflation rates using the same univariate time-series data framework previously utilized for ARMA/ARIMA/SARIMA models.\nFor this purpose, we will employ various Recurrent Neural Network (RNN) architectures, including Dense RNN, Gated Recurrent Unit (GRU), and Long Short-Term Memory Network (LSTM). These models will be tested both with and without the implementation of L2 regularization. L2 regularization is a technique used to prevent overfitting in machine learning models by penalizing larger weights in the model’s parameters, encouraging them to move towards zero.\nBy using these advanced deep learning models, we aim to assess their performance in predicting monthly inflation rates and compare their effectiveness against traditional univariate time-series models like ARIMA and SARIMA. This comparative analysis will help us understand the strengths and limitations of both traditional and deep learning approaches in the context of economic data.\nFurthermore, we will explore the impact of regularization on the RNN models’ predictions, evaluate how far into the future these models can reliably forecast inflation rates, and compare the outcomes with those obtained from traditional ARMA/ARIMA models.\nTo implement these models, we will use the Keras library in Python, which serves as an interface for the TensorFlow framework. Our approach will be guided by the methodologies and insights presented in Francois Chollet’s “Deep Learning in Python, Second Edition” (Chollet 2021), adapting and applying these concepts specifically to the domain of monthly inflation rate analysis in the United States."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusions",
    "section": "",
    "text": "In my project focused on inflation analysis in the United States, I sought to explore a central question: Can time-series modeling effectively forecast inflation rates? To address this, I utilized both univariate and multivariate time-series models, along with cutting-edge Deep Recurrent Neural Networks. My multivariate models were enriched with auxiliary datasets, including key economic indicators like the Federal Funds Rate, Consumer Price Index (CPI), and Unemployment Rate, alongside significant global events impacting the economy.\nThe initial phase of the project involved thorough exploratory data analysis to discern patterns and trends in inflation rates. I developed various time series models to analyze and predict inflation trends, taking into account different influencing factors.\nThe complexity of economic fluctuations, amplified by events like the COVID-19 pandemic, presented substantial challenges in my analysis. The COVID-19 pandemic, served as a significant outlier impacting economic conditions and inflation rates. This unprecedented event led to unique economic scenarios, such as massive fiscal stimulus, supply chain disruptions, and shifts in consumer behavior, which were critical in shaping the inflation trajectory.\nMy project delved into the impacts of the pandemic by examining inflation rates before and after the onset of COVID-19. I observed that the pandemic’s effects on inflation were profound, with initial deflationary trends due to decreased consumer spending followed by inflationary pressures as economies reopened and demand surged. The deep impact of COVID-19 was evident, disrupting typical economic patterns and presenting unique challenges in forecasting inflation rates using traditional models.\nThrough my analysis, I gained valuable insights into how extraordinary global events like the COVID-19 pandemic can drastically alter economic indicators. I also faced limitations and learned lessons about the dynamic nature of inflation and the factors influencing it. This project highlighted the importance of flexibility and adaptability in economic modeling, especially in the face of unexpected global crises."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aanchal Dusija",
    "section": "",
    "text": "Aanchal is a second-year graduate student pursuing an MS in Data Science & Analytics at Georgetown University. An enthusiastic analyst enjoys collecting, analyzing, interpreting large datasets, developing and improving processes. Strong knowledge of Microsoft Excel, Python Programming Language, R Programming Language, SQL, SPSS, Stata, PowerPoint, and other MS office tools along with proficiency at grasping new technical concepts quickly & utilizing them in a productive manner. I am passionate about my work because I love what I do. Always looking to challenge myself and grow further both professionally and personally. Always open for opportunities that will help me build a strong career foundation for future endeavours.\n\nEducation:\nNMIMS University | Mumbai, India\n\nB.Sc. Applied Statistics & Analytics | June 2016 - May 2019\n\nMeghnad Desai Academy of Economics | Mumbai, India\n\nPostgraduation Diploma in Economics and Finance | August 2019 - August 2020\n\nGeorgetown University | Washington DC, USA\n\nM.S Data Science & Analytics | Aug 2022 - May 2024 (anticipated)\n\n\n\nExperience:\n\nGlobalFoundries | Digital Strategic Planning Intern | June 2023 - Aug 2023\nNational Stock Exchange | Research Associate | Feb 2021 - Feb 2022\nNational Stock Exchange | Research Intern | Jan 2020 - Mar 2020"
  },
  {
    "objectID": "spectral-analysis-and-filtering.html",
    "href": "spectral-analysis-and-filtering.html",
    "title": "Spectral Analysis and Filtering",
    "section": "",
    "text": "This is Spectral Analysis and Filtering"
  },
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "When researching U.S. inflation trends and its underlying factors, a plethora of reputable data sources are available for analysts and scholars. Primary among these is the Federal Reserve Economic Data (FRED) managed by the Federal Reserve Bank of St. Louis, which offers extensive macroeconomic data, including information on inflation rates, money supply, and interest rates. The U.S. Bureau of Labor Statistics (BLS) is another crucial resource, providing detailed insights into the Consumer Price Index (CPI) and historical inflation datasets. For a global perspective or to compare U.S. trends with other nations, the World Bank and the Organization for Economic Co-operation and Development (OECD) databases are invaluable. These platforms, along with academic research, historical accounts, and financial news websites, collectively provide a comprehensive view of U.S. inflationary dynamics over time.\nIn addition to the aforementioned primary data repositories, secondary data sources, such as academic journals, economic think tanks, and specialized research institutions, offer valuable insights. Institutions like the National Bureau of Economic Research (NBER) regularly publish studies and working papers on inflation and its ramifications. Furthermore, historical archives, like the Library of Congress or university research libraries, house newspapers and periodicals that can shed light on inflationary sentiments and public perceptions during different epochs. Industry reports and trade publications can also be instrumental in understanding sector-specific impacts of inflation. By synthesizing information from both primary datasets and secondary analytical research, one gains not only a quantitative understanding of inflationary trends but also a qualitative grasp of their broader societal and economic implications."
  },
  {
    "objectID": "data-vizes-in-TS.html",
    "href": "data-vizes-in-TS.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "This is Data Vizes in TS"
  },
  {
    "objectID": "data-vizes-in-TS.html#data-visualization-with-stock-data",
    "href": "data-vizes-in-TS.html#data-visualization-with-stock-data",
    "title": "Data Vizes in TS",
    "section": "Data Visualization with Stock Data",
    "text": "Data Visualization with Stock Data\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"F\",\"NVDA\",\"BAC\" )\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-10-01\",\n             to = \"2022-12-01\")}\n\nx &lt;- list(\n  title = \"date\"\n)\ny &lt;- list(\n  title = \"value\"\n)\n\nstock &lt;- data.frame(F$F.Adjusted,\n                    NVDA$NVDA.Adjusted,\n                    BAC$BAC.Adjusted)\n\n\nstock &lt;- data.frame(stock,rownames(stock))\ncolnames(stock) &lt;- append(tickers,'Dates')\n\nstock$date&lt;-as.Date(stock$Dates,\"%Y-%m-%d\")\nhead(stock)\n\n                  F     NVDA      BAC      Dates       date\n2012-10-01 6.043308 3.009157 7.435025 2012-10-01 2012-10-01\n2012-10-02 5.958104 3.020625 7.410129 2012-10-02 2012-10-02\n2012-10-03 6.049393 2.990809 7.559494 2012-10-03 2012-10-03\n2012-10-04 6.152853 3.123835 7.808433 2012-10-04 2012-10-04\n2012-10-05 6.183283 3.050442 7.733751 2012-10-05 2012-10-05\n2012-10-08 6.116341 3.020625 7.700558 2012-10-08 2012-10-08"
  },
  {
    "objectID": "data-vizes-in-TS.html#most-active-companies-stock-prices",
    "href": "data-vizes-in-TS.html#most-active-companies-stock-prices",
    "title": "Data Vizes in TS",
    "section": "Most Active Companies Stock Prices",
    "text": "Most Active Companies Stock Prices"
  },
  {
    "objectID": "data-vizes-in-TS.html#xrp-plot-using-plotly",
    "href": "data-vizes-in-TS.html#xrp-plot-using-plotly",
    "title": "Data Vizes in TS",
    "section": "XRP Plot using Plotly",
    "text": "XRP Plot using Plotly\n\n\n           XRP-USD.Open XRP-USD.High XRP-USD.Low XRP-USD.Close XRP-USD.Volume\n2021-09-15     1.096939     1.129321    1.080565      1.121353     2997048680\n2021-09-16     1.121244     1.129202    1.068098      1.091498     3168621354\n2021-09-17     1.090878     1.102314    1.053171      1.065598     2989925804\n2021-09-18     1.065913     1.094616    1.057388      1.075668     2308814476\n2021-09-19     1.075168     1.082036    1.041807      1.048428     2148537763\n2021-09-20     1.048984     1.050406    0.887850      0.917152     5775337511\n           XRP-USD.Adjusted\n2021-09-15         1.121353\n2021-09-16         1.091498\n2021-09-17         1.065598\n2021-09-18         1.075668\n2021-09-19         1.048428\n2021-09-20         0.917152\n\n\n[1] \"2021-09-15\"\n\n\n[1] \"2023-09-09\""
  },
  {
    "objectID": "data-vizes-in-TS.html#plotting-xrp-usd-using-plotly",
    "href": "data-vizes-in-TS.html#plotting-xrp-usd-using-plotly",
    "title": "Data Vizes in TS",
    "section": "Plotting XRP-USD using Plotly",
    "text": "Plotting XRP-USD using Plotly\n\n\n           XRP.USD.Open XRP.USD.High XRP.USD.Low XRP.USD.Close XRP.USD.Volume\n2021-09-15     1.096939     1.129321    1.080565      1.121353     2997048680\n2021-09-16     1.121244     1.129202    1.068098      1.091498     3168621354\n2021-09-17     1.090878     1.102314    1.053171      1.065598     2989925804\n2021-09-18     1.065913     1.094616    1.057388      1.075668     2308814476\n2021-09-19     1.075168     1.082036    1.041807      1.048428     2148537763\n2021-09-20     1.048984     1.050406    0.887850      0.917152     5775337511\n           XRP.USD.Adjusted rownames.bitc.\n2021-09-15         1.121353     2021-09-15\n2021-09-16         1.091498     2021-09-16\n2021-09-17         1.065598     2021-09-17\n2021-09-18         1.075668     2021-09-18\n2021-09-19         1.048428     2021-09-19\n2021-09-20         0.917152     2021-09-20\n\n\n           XRP.USD.Open XRP.USD.High XRP.USD.Low XRP.USD.Close XRP.USD.Volume\n2021-09-15     1.096939     1.129321    1.080565      1.121353     2997048680\n2021-09-16     1.121244     1.129202    1.068098      1.091498     3168621354\n2021-09-17     1.090878     1.102314    1.053171      1.065598     2989925804\n2021-09-18     1.065913     1.094616    1.057388      1.075668     2308814476\n2021-09-19     1.075168     1.082036    1.041807      1.048428     2148537763\n2021-09-20     1.048984     1.050406    0.887850      0.917152     5775337511\n           XRP.USD.Adjusted       date\n2021-09-15         1.121353 2021-09-15\n2021-09-16         1.091498 2021-09-16\n2021-09-17         1.065598 2021-09-17\n2021-09-18         1.075668 2021-09-18\n2021-09-19         1.048428 2021-09-19\n2021-09-20         0.917152 2021-09-20\n\n\n'data.frame':   725 obs. of  7 variables:\n $ XRP.USD.Open    : num  1.1 1.12 1.09 1.07 1.08 ...\n $ XRP.USD.High    : num  1.13 1.13 1.1 1.09 1.08 ...\n $ XRP.USD.Low     : num  1.08 1.07 1.05 1.06 1.04 ...\n $ XRP.USD.Close   : num  1.12 1.09 1.07 1.08 1.05 ...\n $ XRP.USD.Volume  : num  3.00e+09 3.17e+09 2.99e+09 2.31e+09 2.15e+09 ...\n $ XRP.USD.Adjusted: num  1.12 1.09 1.07 1.08 1.05 ...\n $ date            : Date, format: \"2021-09-15\" \"2021-09-16\" ..."
  },
  {
    "objectID": "data-vizes-in-TS.html#candlestick-chart-for-xrp-usd",
    "href": "data-vizes-in-TS.html#candlestick-chart-for-xrp-usd",
    "title": "Data Vizes in TS",
    "section": "Candlestick Chart for XRP-USD",
    "text": "Candlestick Chart for XRP-USD"
  },
  {
    "objectID": "data-vizes-in-TS.html#precipitation-and-snow-over-time",
    "href": "data-vizes-in-TS.html#precipitation-and-snow-over-time",
    "title": "Data Vizes in TS",
    "section": "Precipitation and Snow over Time",
    "text": "Precipitation and Snow over Time\n\n\n\n\n\n\nIn the Climate data, we can see the snow and precipitation values for the year 2021 for Washington DC. It is visible that Washington DC received the highest amount of rainfall during September 2021. Additionally, Washington DC received snowfall in the months of January and February."
  },
  {
    "objectID": "data-vizes-in-TS.html#plotting-lior3m-using-plotly",
    "href": "data-vizes-in-TS.html#plotting-lior3m-using-plotly",
    "title": "Data Vizes in TS",
    "section": "Plotting LIOR3M using plotly",
    "text": "Plotting LIOR3M using plotly\n\n\n\n\n\n\nThe chart appears to plot the LIOR3m against time, from January 1, 1970 to October 1, 2016. It is clearly visible that the LIOR3M rates were quite high randing from 10-18% from 1970s till 2008. After 2008, we can see a massive dip over the years going to less than 1% LIOR3M."
  },
  {
    "objectID": "exploratory-data-analysis.html",
    "href": "exploratory-data-analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Inflation is an integral part of the economic landscape. It reflects how prices of goods and services change over time. Analyzing inflation rate trends can provide insights into the health and trajectory of an economy. In this analysis, we utilize the Consumer Price Index (CPI) from the FRED database to explore monthly inflation rates.\nIn this Exploratory Data Analysis, we delve into the monthly inflation rate over the years. Our primary objective is to understand the underlying patterns, trends, and seasonality in the data, and to prepare it for further statistical analysis or forecasting.\nIntroduction: Our exploration into economic indicators brings us to a fundamental metric: the Monthly Inflation Rate, visualized using data sourced from the Federal Reserve Economic Data (FRED). The rate is calculated using the Consumer Price Index (CPI) and gives us an indication of the monthly percentage changes in prices for goods and services consumed by households.\nThe inflation rate is derived from the Consumer Price Index (CPI) using a straightforward method: by differencing the logarithmic transformation of the CPI and then multiplying by 100. This formula helps capture the rate of change in prices in percentage terms."
  },
  {
    "objectID": "data-visualisation.html",
    "href": "data-visualisation.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The US economy, being one of the largest and most influential economies globally, is driven by a plethora of interlinked factors. To fully grasp its dynamics, one must consider a vast array of economic indicators. The visualizations presented here encapsulate multiple crucial aspects of the US economy, each shedding light on various sectors and metrics.\nIn the ever-changing landscape of the global economy, keeping an eye on macro-economic indicators is crucial for investors, policymakers, and the general public. These indicators provide a snapshot of an economy’s health, giving insights into its future direction.\n\n\n\nDefinition: The Consumer Price Index (CPI) measures the average change in prices paid by urban consumers for a basket of goods and services.\nSignificance: It’s a primary tool to gauge the cost of living and purchasing power. High inflation can erode savings and purchasing power, while deflation can indicate an economic slowdown.\nInfluencing Factors: Supply chain disruptions, increased demand, production costs, and monetary policies.\n\n\nUnemployment Rate (UNRATE):\n\nDefinition: The percentage of the labor force that is jobless but is actively looking for work.\nSignificance: A high unemployment rate can signal economic distress, while a very low rate might indicate a booming economy but can also hint at potential overheating.\nInfluencing Factors: Economic cycles, technological changes, global events, and governmental policies.\n\nPersonal Saving Rate (PSAVERT):\n\nDefinition: The portion of personal income that households are saving.\nSignificance: A rising rate may indicate cautious consumers who are holding back on spending, whereas a falling rate might signal increased consumer confidence.\nInfluencing Factors: Economic outlook, interest rates, and household debt.\n\nMoney Supply (M2):\n\nDefinition: The total amount of monetary assets in the economy, including cash, checking deposits, and savings.\nSignificance: Rapid growth in M2 might indicate future inflation, while a decline might hint at economic slowdowns.\nInfluencing Factors: Central bank policies, public demand for money, and bank lending activities.\n\nReal Disposable Income (DSPIC96):\n\nDefinition: The income available to households after adjusting for inflation and taxes.\nSignificance: It shows consumers’ ability to spend and save. Rising real disposable income encourages consumer spending.\nInfluencing Factors: Employment levels, wage growth, taxation, and inflation.\n\nPersonal Consumption Expenditure (PCE):\n\nDefinition: The value of goods and services consumed by households.\nSignificance: It’s a primary measure of consumer spending and thus indicates consumer confidence and potential economic growth.\nInfluencing Factors: Disposable income, consumer confidence, and credit availability.\n\nTreasury Yield (GS10):\n\nDefinition: The return on investment for a 10-year U.S. government bond.\nSignificance: It can indicate the risk appetite of investors. Lower yields might suggest a flight to safety, while higher yields could indicate optimism.\nInfluencing Factors: Central bank policies, economic outlook, and global investment flows.\n\nFederal Funds Rate (FEDFUNDS):\n\nDefinition: The interest rate at which banks lend money to each other overnight.\nSignificance: It directly impacts interest rates throughout the economy, influencing borrowing costs and investment decisions.\nInfluencing Factors: Inflation, employment levels, and economic growth.\n\nConstruction Spending (TTLCONS):\n\nDefinition: The total dollar amount spent on construction projects.\nSignificance: High spending can indicate economic growth and confidence in the real estate sector.\nInfluencing Factors: Interest rates, economic outlook, and property demand.\n\nIndustrial Production Index (INDPRO):\n\n\nDefinition: A measure of the production output of factories, mines, and utilities.\nSignificance: It reflects the health of the industrial sector, which is a significant component of GDP.\nInfluencing Factors: Demand for goods, technological advancements, and supply chain factors.\n\n\nCore CPI (CPILFESL):\n\n\nDefinition: The CPI minus the volatile food and energy sectors.\nSignificance: It offers a clearer view of the underlying inflation trends without the noise of volatile sectors.\nInfluencing Factors: Same as CPI but without the direct impacts of food and energy prices."
  },
  {
    "objectID": "data-visualisation.html#data-visualization-with-stock-data",
    "href": "data-visualisation.html#data-visualization-with-stock-data",
    "title": "Data Visualization",
    "section": "Data Visualization with Stock Data",
    "text": "Data Visualization with Stock Data\nHere is a snippet of the stocks: Ford, NVDIA and Bank of america from 2012 to 2022.\n\n\n                  F     NVDA      BAC      Dates       date\n2012-10-01 6.043308 3.009157 7.435025 2012-10-01 2012-10-01\n2012-10-02 5.958104 3.020624 7.410128 2012-10-02 2012-10-02\n2012-10-03 6.049393 2.990808 7.559493 2012-10-03 2012-10-03\n2012-10-04 6.152853 3.123835 7.808434 2012-10-04 2012-10-04\n2012-10-05 6.183283 3.050441 7.733752 2012-10-05 2012-10-05\n2012-10-08 6.116341 3.020624 7.700563 2012-10-08 2012-10-08"
  },
  {
    "objectID": "data-visualisation.html#most-active-companies-stock-prices",
    "href": "data-visualisation.html#most-active-companies-stock-prices",
    "title": "Data Visualization",
    "section": "Most Active Companies Stock Prices",
    "text": "Most Active Companies Stock Prices\n\n\n\n\n\nThis is a time series plot in R for Ford, NVDIA and Bank of America adjusted closing price over time. This plot shows Ford, NVDIA and Bank of America stock from 2012 till 2022. We can see that all 3 stocks have increased since 2012. NVDIA has been on the rise from 2017 to 2021 and has now dropped severely but still has the highest closing price.\n\n\n\n\n\n\nThis is a dynamic time series plot in Plotly for Ford, NVDIA and Bank of America adjusted closing price over time. This plot shows Ford, NVDIA and Bank of America stock from 2012 till 2022. We can see that all 3 stocks have increased since 2012. NVDIA has been on the rise from 2017 to 2021 and has now dropped severely but still has the highest closing price."
  },
  {
    "objectID": "data-visualisation.html#xrp-usd-plot-using-plotly",
    "href": "data-visualisation.html#xrp-usd-plot-using-plotly",
    "title": "Data Visualization",
    "section": "XRP-USD Plot using Plotly",
    "text": "XRP-USD Plot using Plotly\nNow we see XRP-USD stock prices graphically."
  },
  {
    "objectID": "data-visualisation.html#plotting-xrp-usd-using-plotly",
    "href": "data-visualisation.html#plotting-xrp-usd-using-plotly",
    "title": "Data Visualization",
    "section": "Plotting XRP-USD using Plotly",
    "text": "Plotting XRP-USD using Plotly\n\n\n\n\n\n\nThis line plot shows the XRP-USD Adjusted price vs the date from September 2021 to September 2023. As we can see the prices rose till Oct 2021 then dropped till Sept 2022 were slowly on the rise till Jul 2023 and have again fallen till Sept 2023."
  },
  {
    "objectID": "data-visualisation.html#candlestick-chart-for-xrp-usd",
    "href": "data-visualisation.html#candlestick-chart-for-xrp-usd",
    "title": "Data Visualization",
    "section": "Candlestick Chart for XRP-USD",
    "text": "Candlestick Chart for XRP-USD\n\n\n\n\n\n\nWe can see the candlestick plot of XRP-USD from September 2021 to September 2023 where the green color indicates a rise in price of XRP on that day and the red color indicates a fall in price of XRP on that day. The body of the candlestick for each day represents the volatility between the open and close price of XRP on that day. As we can see, the volatility of the stock has reduced in the recent past."
  },
  {
    "objectID": "data-visualisation.html#precipitation-and-snow-over-time",
    "href": "data-visualisation.html#precipitation-and-snow-over-time",
    "title": "Data Visualization",
    "section": "Precipitation and Snow over Time",
    "text": "Precipitation and Snow over Time\n\n\n\n\n\n\nIn the Climate data, we can see the snow and precipitation values for the year 2021 for Washington DC. It is visible that Washington DC received the highest amount of rainfall during September 2021. Additionally, Washington DC received snowfall in the months of January and February."
  },
  {
    "objectID": "data-visualisation.html#plotting-lior3m-using-plotly",
    "href": "data-visualisation.html#plotting-lior3m-using-plotly",
    "title": "Data Visualization",
    "section": "Plotting LIOR3M using plotly",
    "text": "Plotting LIOR3M using plotly\n\n\n\n\n\n\nThe chart appears to plot the LIOR3m against time, from January 1, 1970 to October 1, 2016. It is clearly visible that the LIOR3M rates were quite high randing from 10-18% from 1970s till 2008. After 2008, we can see a massive dip over the years going to less than 1% LIOR3M."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Aanchal Dusija",
    "section": "",
    "text": "Aanchal is a second-year graduate student pursuing an MS in Data Science & Analytics at Georgetown University. An enthusiastic analyst enjoys collecting, analyzing, interpreting large datasets, developing and improving processes. Strong knowledge of Microsoft Excel, Python Programming Language, R Programming Language, SQL, SPSS, Stata, PowerPoint, and other MS office tools along with proficiency at grasping new technical concepts quickly & utilizing them in a productive manner. I am passionate about my work because I love what I do. Always looking to challenge myself and grow further both professionally and personally. Always open for opportunities that will help me build a strong career foundation for future endeavours."
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html",
    "href": "financial-time-series-models(arch-garch).html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "The study of inflation dynamics in the United States through financial time series models, such as ARCH (Autoregressive Conditional Heteroscedasticity) and GARCH (Generalized Autoregressive Conditional Heteroskedasticity), offers vital insights into the volatility and variability of inflation rates. Inflation, a measure of the rate at which the general level of prices for goods and services is rising, and subsequently, purchasing power is falling, is inherently volatile and subject to various economic forces."
  },
  {
    "objectID": "arma-arima-sarima-models.html",
    "href": "arma-arima-sarima-models.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Forecasting Inflation with ARMA/ARIMA/SARIMA Models\nInflation forecasting is of paramount importance for policymakers, investors, and consumers alike. Accurate predictions can aid in monetary policy decisions, investment strategies, and household budgeting. To forecast inflation using time series data, several models can be employed, notably ARMA, ARIMA, and SARIMA models.\n\nAutoregressive (AR) and Moving Average (MA) Models:\n\nAR: This model predicts future inflation based on its own past values. It assumes that the inflation rate is a linear function of its previous values.\nMA: Contrary to AR, the MA model predicts inflation based on past white noise or error terms. This captures the shock effects observed in the past.\n\nAutoregressive Moving Average (ARMA) Model:\n\nCombines both AR and MA components. Suitable for time series data that exhibit patterns not captured by AR or MA models alone.\n\nAutoregressive Integrated Moving Average (ARIMA) Model:\n\nAn extension of the ARMA model that includes “integration” (I). This represents the number of differences needed to make the time series stationary.\nEspecially pertinent for inflation data, which may have trends or cycles that render the data non-stationary.\n\nSeasonal Autoregressive Integrated Moving Average (SARIMA) Model:\n\nExtends ARIMA by accounting for seasonality, which can be pivotal for inflation data affected by seasonal factors (e.g., holiday-driven consumer spending or agricultural harvest cycles).\n\n\nChecking Stationarity: For accurate forecasting, it’s vital that the inflation data is stationary, meaning its statistical properties like mean and variance remain constant over time.\n\nUse the Autocorrelation Function (ACF) plot to test for stationarity. If significant correlations persist over several lags, the data may be non-stationary.\nDifferencing the data, often first or even second differences, can help in achieving stationarity by eliminating trends or cyclical patterns.\n\nIn our EDA section, an ACF plot was generated to examine stationarity in the inflation data. After ensuring stationarity, either naturally or through transformations, the ACF and Partial Autocorrelation Function (PACF) plots of the stationary data are pivotal in determining the optimal parameters for our ARIMA or SARIMA models."
  },
  {
    "objectID": "introduction.html#us-inflation-over-the-decades",
    "href": "introduction.html#us-inflation-over-the-decades",
    "title": "Introduction",
    "section": "",
    "text": "Inflation, a general increase in prices and fall in the purchasing value of money, is a complex and multifaceted economic phenomenon. It impacts every segment of society, from consumers and businesses to policymakers and investors. This exploration focuses on the fluctuations and patterns of US inflation over the decades, tracing its roots, its implications, and its broader economic significance."
  },
  {
    "objectID": "introduction.html#the-big-picture-economic-impacts-and-broader-repercussions",
    "href": "introduction.html#the-big-picture-economic-impacts-and-broader-repercussions",
    "title": "Introduction",
    "section": "The Big Picture: Economic Impacts and Broader Repercussions",
    "text": "The Big Picture: Economic Impacts and Broader Repercussions\nInflation doesn’t just influence prices at the supermarket or the gas station. It’s a key indicator of economic health and carries implications for monetary policy, interest rates, and overall economic stability. High or hyperinflation can erode savings, distort spending behavior, and create uncertainty, while deflation can signal a contracting economy and lead to decreased consumer spending. Understanding inflation’s trajectory is crucial for predicting its future implications and formulating responsive strategies."
  },
  {
    "objectID": "introduction.html#literature-review",
    "href": "introduction.html#literature-review",
    "title": "Introduction",
    "section": "Literature Review:",
    "text": "Literature Review:\nInflation has been a major subject of economic discourse for decades.Numerous studies have also debated the cause-and-effect relationship between inflation and employment, often referencing the Phillips Curve. However, as economies evolve and face new challenges, from technological advancements to global pandemics, the determinants and repercussions of inflation become ever more complex.\nThe literature has broadly evolved from macroeconomic theories of the past to more nuanced, data-driven analyses today. Yet, despite extensive research, predicting inflation remains a complex endeavor due to its multifaceted nature and global interdependencies.\n\nHistorical Monetary Perspectives: Beyond Friedman and Schwartz’s seminal assertion that inflation is fundamentally a monetary phenomenon, earlier works by Irving Fisher (1911) in “The Purchasing Power of Money” presented the Quantity Theory of Money, postulating a direct relationship between the money supply and the overall price level.\nStructuralist vs. Monetarist Views: The 1970s saw a divergence between structuralists and monetarists. While monetarists, led by figures like Milton Friedman, insisted on the primacy of money supply, structuralists argued for the role of other factors, like wage and price rigidities. The Latin American hyperinflations of the 1980s further intensified this debate, with authors such as Dornbusch and Edwards (1990) exploring the structural and external debt drivers of such inflationary episodes.\nGlobalization and Inflation: As economies became more intertwined, researchers like Rogoff (2003) began to examine how globalization impacts inflation. They found that increased trade and financial openness often put downward pressure on prices, given the competitive forces at play.\nBehavioral Economics & Inflation Expectations: Akerlof, Dickens, and Perry (1996) introduced behavioral elements, suggesting that people’s inflation expectations could be sticky due to cognitive biases. Their work underscores that beyond raw economic factors, human psychology plays a crucial role in inflation dynamics.\nThe Role of Central Banks: Blinder (1998) has stressed the pivotal role of central banks in managing inflation expectations, advocating for transparency and credibility in monetary policy. This idea was further echoed by Taylor (1993) with the introduction of the Taylor rule, emphasizing the importance of systematic monetary policy in response to inflation and output fluctuations.\nInflation in a Post-Crisis World: Post the 2008 financial crisis, discussions around inflation took a new turn. Authors like Ball and Mazumder (2011) explored the flattened Phillips Curve, observing weaker links between unemployment and inflation. In more recent times, with economic downturns triggered by global events like the COVID-19 pandemic, scholars have started probing the disinflationary effects of such demand shocks and their potential transition to longer-term inflationary pressures due to fiscal stimuli and supply chain disruptions.\n\nIn sum, while historical perspectives on inflation laid the groundwork for our understanding, ongoing research continues to highlight its multifaceted determinants, influenced by both domestic policy decisions and global economic dynamics."
  },
  {
    "objectID": "introduction.html#analytical-angles",
    "href": "introduction.html#analytical-angles",
    "title": "Introduction",
    "section": "Analytical Angles:",
    "text": "Analytical Angles:\n\nHistorical Context:\na. Objective: To discern the trajectory of inflation within the U.S., pinpointing significant periods of hyperinflation or deflation, and identifying the underlying causes for these shifts.\nb. Key Considerations:\n\nPeriods of rapid inflation post-World Wars, the Great Depression’s deflationary effects, the stagflation of the 1970s, and more recent trends post the 2008 financial crisis.\nHistorical events, such as the abandonment of the Gold Standard and the Bretton Woods system, and their ramifications.\n\n\n\n\nPolicy and Inflation:\na. Objective: To understand the role of the government, especially the Federal Reserve, in shaping the inflationary landscape through monetary and fiscal policies.\nb. Key Considerations:\n\nThe influence of interest rates, money supply, quantitative easing, and fiscal stimuli on inflation rates.\nThe effectiveness and unintended consequences of policies like Paul Volcker’s efforts in the 1980s or the unconventional monetary policies post the 2008 crisis.\n\n\n\n\nGlobal Events and Inflation:\na. Objective: To gauge how exogenous shocks, be they geopolitical or economic, have impacted the U.S. inflationary environment.\nb. Key Considerations:\n\nThe 1973 and 1979 oil crises and their inflationary aftereffects.\nThe impact of global recessions and recoveries, such as the dot-com bubble burst or the 2008 financial meltdown. Geopolitical events like wars or trade tensions and their implications on the U.S. inflation rate.\n\n\nEach of these angles offers a distinct vantage point, together ensuring a comprehensive exploration of the multifaceted nature of U.S. inflation."
  },
  {
    "objectID": "introduction.html#guiding-questions",
    "href": "introduction.html#guiding-questions",
    "title": "Introduction",
    "section": "Guiding Questions:",
    "text": "Guiding Questions:\n\nHow has the pattern of inflation rates in the United States evolved over the last 50 years?\nWhat economic sectors have been most affected by inflationary changes in the United States over the last 50 years?\nWhat is the relationship between inflation and key economic indicators such as unemployment, wage growth, and GDP over the past five decades?\nCan univariate time-series models (like ARIMA/SARIMA) effectively predict future inflation rates in the United States based on past data from 1970-2023?\nCan multivariate time series models, including ARIMAX and VAR, improve predictions of yearly inflation rates in the United States by including variables like Federal Reserve interest rate decisions, GDP growth, and unemployment rates?\nWhat is the correlation over time between inflation rates and key economic indicators like unemployment or GDP growth in the United States? How well do VAR models capture this relationship?\nHow do predictions of yearly inflation rates from a comprehensive ARIMAX model compare with those from simpler time series models and advanced methods like Deep Learning?\nUsing volatility models like ARCH/GARCH, to what extent can financial market indicators provide insights into inflation trends in the United States?\nCan Deep Learning models, particularly Deep Recurrent Neural Networks, effectively capture the underlying patterns of monthly inflation rates from 1970-2023? Which models perform best and why?\nHow far into the future can Deep Learning models, specifically Recurrent Neural Networks, accurately forecast monthly inflation rates? Which models are most effective and why?"
  },
  {
    "objectID": "introduction.html#economic-impacts-and-broader-repercussions",
    "href": "introduction.html#economic-impacts-and-broader-repercussions",
    "title": "Introduction",
    "section": "Economic Impacts and Broader Repercussions",
    "text": "Economic Impacts and Broader Repercussions\nInflation doesn’t just influence prices at the supermarket or the gas station. It’s a key indicator of economic health and carries implications for monetary policy, interest rates, and overall economic stability. High or hyperinflation can erode savings, distort spending behavior, and create uncertainty, while deflation can signal a contracting economy and lead to decreased consumer spending. Understanding inflation’s trajectory is crucial for predicting its future implications and formulating responsive strategies."
  },
  {
    "objectID": "introduction.html#u.s.-inflation-early-history",
    "href": "introduction.html#u.s.-inflation-early-history",
    "title": "Introduction",
    "section": "U.S. Inflation: Early History",
    "text": "U.S. Inflation: Early History\nBetween the founding of the U.S. in 1776 to the year 1914, one thing was for sure—wartime periods were met with high inflation.\nAt the time, the U.S. operated under a classical Gold Standard regime, with the dollar’s value tied to gold. During the Civil War and World War I, the U.S. went off the Gold Standard in order to print money and finance the war. When this occurred, it triggered inflationary episodes, with prices rising upwards of 20% in 1918.\n\n\n\n\n\n\n\nSource: Macrotrends (June, 2021) *As measured by the Consumer Price Index (CPI)\nHowever, when the government returned to a modified Gold Standard, deflationary periods followed, leading prices to effectively stabilize, on average, leading up to World War II."
  },
  {
    "objectID": "introduction.html#the-move-to-bretton-woods",
    "href": "introduction.html#the-move-to-bretton-woods",
    "title": "Introduction",
    "section": "The Move to Bretton Woods",
    "text": "The Move to Bretton Woods\nLike post-World War I, the Great Depression of the 1930s coincided with deflationary pressures on prices. Due to the rigidity of the monetary system at the time, countries had difficulty increasing money supply to help boost their economy. Many countries exited the Gold Standard during this time, and by 1933 the U.S. abandoned it completely.\nA decade later, with the Bretton Woods Agreement in 1944, global currency exchange values pegged to the dollar, while the dollar was pegged to gold. The U.S. held the majority of gold reserves, and the global reserve currency transitioned from the sterling pound to the dollar."
  },
  {
    "objectID": "introduction.html#s-regime-change",
    "href": "introduction.html#s-regime-change",
    "title": "Introduction",
    "section": "1970’s Regime Change",
    "text": "1970’s Regime Change\nBy 1971, the ability for gold to cover the supply of U.S. dollars in circulation became an increasing concern.\nLeading up to this point, a surplus of money supply was created due to military expenses, foreign aid, and others. In response, President Richard Nixon abandoned the Bretton Woods Agreement in 1971 for a floating exchange, known as the “Nixon shock”. Under a floating exchange regime, rates fluctuate based on supply and demand relative to other currencies.\nA few years later, oil shocks of 1973 and 1974 led inflation to soar past 12%. By 1979, inflation surged in excess of 13%."
  },
  {
    "objectID": "introduction.html#the-volcker-era",
    "href": "introduction.html#the-volcker-era",
    "title": "Introduction",
    "section": "The Volcker Era",
    "text": "The Volcker Era\nIn 1979, Federal Reserve Chair Paul Volcker was sworn in, and he introduced stark changes to combat inflation that differed from previous regimes.\nInstead of managing inflation through interest rates, which the Federal Reserve had done previously, inflation would be managed through controlling the money supply. If the money supply was limited, this would cause interest rates to increase.\nWhile interest rates jumped to 20% in 1980, by 1983 inflation dropped below 4% as the economy recovered from the recession of 1982, and oil prices rose more moderately. Over the last four decades, inflation levels have remained relatively stable since the measures of the Volcker era were put in place."
  },
  {
    "objectID": "introduction.html#u.s.-inflation-today",
    "href": "introduction.html#u.s.-inflation-today",
    "title": "Introduction",
    "section": "U.S. Inflation: Today",
    "text": "U.S. Inflation: Today\nAs the U.S. economy reopens, consumer demand has strengthened.\nMeanwhile, supply bottlenecks, from semiconductor chips to lumber, are causing strains on automotive and tech industries. While this points towards increasing inflation, some suggest that it may be temporary, as prices were depressed in 2020.\nAt the same time, the Federal Reserve is following an “average inflation targeting” regime, which means that if a previous inflation shortfall occurred in the previous year, it would allow for higher inflationary periods to make up for them. As the last decade has been characterized by low inflation and low interest rates, any prolonged period of inflation will likely have pronounced effects on investors and financial markets.\nRecently, national attention has shifted to the increases in inflation which has been attributed to a number of factors such as the war in Ukraine, supply chain disruptions, and monetary spending. Many of these factors were a result of the COVID-19 pandemic and subsequent government stimulus packages.\nEconomist point to three main drivers of inflation, the first of which is demand-pull inflation where it is theorized that excess consumer demand leads to companies raising their prices and thus increasing inflation. Next is cost-push inflation which says that when the cost for raw and intermediate goods increases then that pushes companies to raise their prices and pass the cost onto consumers. Finally, monetary policy affects inflation where increases in the money supply reduce the value of the dollar meaning consumers can buy less with the same amount of money, effectively increasing prices and inflation."
  },
  {
    "objectID": "introduction.html#u.s.-inflation-history-and-key-monetary-changes",
    "href": "introduction.html#u.s.-inflation-history-and-key-monetary-changes",
    "title": "Introduction",
    "section": "U.S. Inflation History and Key Monetary Changes",
    "text": "U.S. Inflation History and Key Monetary Changes\n\nEarly U.S. History (1776-1914): U.S. inflation was largely influenced by wartime periods. The Gold Standard was in place, tying the dollar’s value to gold. Wars like the Civil War and WWI led to temporary abandonments of this standard, resulting in inflationary spikes, notably a 20% rise in 1918. However, post-war periods and a return to the Gold Standard often brought about deflation and stabilization of prices.\nTransition to Bretton Woods (1930s-1944): The Great Depression saw deflationary trends due to the monetary system’s rigidity. By 1933, the U.S. completely left the Gold Standard. In 1944, the Bretton Woods Agreement established the dollar as the main global reserve currency, pegged to gold.\n1970’s Changes: Concerns arose in the 1970s about gold’s ability to cover the increasing U.S. dollar supply. Factors like military expenses caused money supply surpluses. In 1971, President Nixon abandoned Bretton Woods, introducing a floating exchange rate system. Subsequent oil shocks in the 1970s resulted in inflation rates exceeding 12% and later 13%.\nVolcker’s Approach (1979 Onwards): Paul Volcker, Federal Reserve Chair, implemented a new strategy in 1979. Instead of controlling inflation via interest rates, the focus shifted to controlling the money supply. This led to high interest rates, reaching 20% in 1980. However, by 1983, inflation fell below 4%, and the measures introduced during the Volcker era have helped maintain relatively stable inflation levels over the subsequent decades."
  },
  {
    "objectID": "data-sources.html#federal-reserve-economic-data-fred",
    "href": "data-sources.html#federal-reserve-economic-data-fred",
    "title": "Data Sources",
    "section": "Federal Reserve Economic Data (FRED)",
    "text": "Federal Reserve Economic Data (FRED)\nThe FRED database stands out as an indispensable tool for economists, researchers, policymakers, and students alike, primarily because of its comprehensive coverage of economic indicators. It’s more than just a repository; FRED’s user-friendly interface allows for the customization of graphs, making data visualization and interpretation significantly more accessible. The platform also offers features like data aggregation, which enables users to compare different economic metrics side by side. Furthermore, its continuous updates ensure that stakeholders are working with the most recent and relevant information, facilitating real-time economic analyses. This level of detail and accessibility ensures that FRED remains a premier resource for understanding the nuances and trajectories of the U.S. economy. Managed by the Federal Reserve Bank of St. Louis, FRED provides a vast database of economic data, including U.S. inflation rates, money supply, interest rates, and more. It’s invaluable for tracking macroeconomic trends.\nURL: https://fred.stlouisfed.org/ U.S."
  },
  {
    "objectID": "data-sources.html#bureau-of-labor-statistics-bls",
    "href": "data-sources.html#bureau-of-labor-statistics-bls",
    "title": "Data Sources",
    "section": "Bureau of Labor Statistics (BLS)",
    "text": "Bureau of Labor Statistics (BLS)\nThe Bureau of Labor Statistics (BLS) stands as a cornerstone in the U.S. economic data landscape, particularly when it comes to understanding inflation through the lens of the Consumer Price Index (CPI). CPI, meticulously tracked and updated by the BLS, represents the average change over time in the prices paid by urban consumers for a representative basket of goods and services. This encompasses everything from daily necessities like food and housing to discretionary items such as entertainment and apparel. With a robust historical archive, the BLS offers researchers, policymakers, and the public an intricate view of how purchasing power has evolved over the years. Beyond just raw numbers, the detailed breakdowns by commodity and category facilitate nuanced analyses, allowing stakeholders to discern inflationary trends specific to certain sectors or product groups, making BLS an indispensable resource for comprehensive inflationary studies.\nURL: https://www.bls.gov/"
  },
  {
    "objectID": "data-sources.html#the-world-bank-data",
    "href": "data-sources.html#the-world-bank-data",
    "title": "Data Sources",
    "section": "The World Bank Data",
    "text": "The World Bank Data\n\nThe World Bank Data, a comprehensive and trusted source of international economic data, provides invaluable insights into the economic dynamics of countries across the globe. With its extensive repository of metrics, this database offers detailed information on inflation rates, GDP growth, trade balances, and many other crucial economic indicators. For researchers and analysts looking to juxtapose the U.S. economic trajectory with that of other nations, this resource proves indispensable. Its standardized data collection methods ensure comparability, enabling nuanced examinations of global economic trends and patterns. By leveraging this tool, one can discern not only standalone country performances but also the intricacies of interconnected global economies, trade relationships, and shared economic challenges.\nURL: https://data.worldbank.org/"
  },
  {
    "objectID": "data-sources.html#organization-for-economic-co-operation-and-development-oecd",
    "href": "data-sources.html#organization-for-economic-co-operation-and-development-oecd",
    "title": "Data Sources",
    "section": "Organization for Economic Co-operation and Development (OECD)",
    "text": "Organization for Economic Co-operation and Development (OECD)\nThe Organization for Economic Co-operation and Development (OECD) stands as an invaluable reservoir of economic data and analysis for its member countries and the world at large. Catering to a wide array of economic domains, one of its key offerings is comprehensive inflation data. This data encompasses various metrics, including the Consumer Price Index (CPI), Producer Price Index (PPI), and core inflation, all stratified monthly, quarterly, and annually. Additionally, OECD contextualizes this raw data with analytical insights, trends, and forecasts, enabling policymakers, researchers, and financial experts to discern inflationary patterns, compare cross-country inflation trajectories, and formulate responsive strategies. The depth and breadth of OECD’s inflation data make it a go-to resource for a holistic understanding of global inflationary dynamics.\nURL: https://data.oecd.org/"
  },
  {
    "objectID": "data-sources.html#inflationdata.com",
    "href": "data-sources.html#inflationdata.com",
    "title": "Data Sources",
    "section": "InflationData.com",
    "text": "InflationData.com\nInflationData.com serves as a specialized resource concentrating specifically on historical U.S. inflation data, providing researchers and analysts with valuable insights into the multifarious patterns and occurrences of inflation throughout the years. The site goes beyond mere presentation of data, furnishing detailed breakdowns and well-rounded elucidations pertaining to diverse inflationary epochs. It is particularly useful for those looking to comprehend the nuanced intricacies and underlying causes of different inflationary phases, allowing users to delve into meticulous analyses of the socio-economic and policy-driven factors that have catalyzed inflation fluctuations over the decades. The rich content available on this platform, encompassing both numerical data and interpretative context, makes it a substantial asset for anyone looking to enhance their understanding of the historical trajectory and multifaceted nature of U.S. inflation.\nURL: https://inflationdata.com/"
  },
  {
    "objectID": "data-sources.html#u.s.-department-of-the-treasury",
    "href": "data-sources.html#u.s.-department-of-the-treasury",
    "title": "Data Sources",
    "section": "U.S. Department of the Treasury",
    "text": "U.S. Department of the Treasury\nThe U.S. Department of the Treasury plays a pivotal role in shaping the nation’s financial and economic landscape. As the executive agency responsible for the federal government’s economic policy, it possesses a vast trove of historical documents, policies, and data sets that provide invaluable insights into the evolution of the U.S. monetary system. From the nation’s earliest financial legislation, such as the Coinage Act of 1792, to contemporary decisions about debt issuance and management, the Treasury’s archives offer a comprehensive view of America’s fiscal journey. For researchers, policymakers, and enthusiasts alike, delving into these resources can provide a profound understanding of the intricate web of decisions, challenges, and strategies that have steered the course of the U.S. economy over the centuries.\nURL: https://www.treasury.gov/"
  },
  {
    "objectID": "data-sources.html#international-monetary-fund-imf",
    "href": "data-sources.html#international-monetary-fund-imf",
    "title": "Data Sources",
    "section": "International Monetary Fund (IMF)",
    "text": "International Monetary Fund (IMF)\nThe International Monetary Fund (IMF) stands as one of the foremost international financial institutions, playing a pivotal role in fostering global monetary cooperation, ensuring financial stability, and providing a platform for economic research and dialogue. Its rich assortment of global economic data is invaluable for scholars and policymakers alike, offering a macroeconomic perspective that encompasses a multitude of countries. Specifically concerning inflation, the IMF’s World Economic Outlook Database and various country reports furnish detailed data and analyses on inflation rates, underlying determinants, and projected trends. When studying U.S. inflation in a global context, the IMF’s resources not only aid in juxtaposing American trends with those of other nations but also in understanding the international forces that may influence domestic inflationary pressures. Through its holistic lens, the IMF underscores the interconnectedness of the global economy and the ripple effects that monetary decisions in one region can have on others.\nURL: https://www.imf.org/"
  },
  {
    "objectID": "data-sources.html#academic-and-research-institutions",
    "href": "data-sources.html#academic-and-research-institutions",
    "title": "Data Sources",
    "section": "Academic and Research Institutions",
    "text": "Academic and Research Institutions\nUniversities and esteemed research entities such as Harvard, MIT, and the National Bureau of Economic Research (NBER) serve as vital hubs for intellectual exploration on economic phenomena like inflation. These institutions are equipped with world-renowned faculty, research scholars, and economic think tanks dedicated to delving deep into intricate facets of inflationary dynamics. Their output often includes working papers, which present cutting-edge research prior to formal publication, thus offering timely insights into the evolving landscape of inflationary studies. These publications, combined with conferences, seminars, and workshops hosted by these institutions, foster a collaborative academic environment. This allows for a cross-pollination of ideas and methodologies, ensuring a multifaceted and comprehensive understanding of inflation’s causes, effects, and potential trajectories."
  },
  {
    "objectID": "data-sources.html#financial-news-websites",
    "href": "data-sources.html#financial-news-websites",
    "title": "Data Sources",
    "section": "Financial News Websites",
    "text": "Financial News Websites\nFinancial news websites such as Bloomberg, CNBC, and Reuters stand at the forefront of real-time economic reporting, making them indispensable tools for anyone aiming to understand the ever-evolving dynamics of inflation. These platforms have the advantage of aggregating insights from global economists, market analysts, and financial experts, offering a multifaceted view on inflation’s immediate drivers, implications, and future trajectories. Beyond raw data, they present nuanced narratives, tying together global events, policy decisions, and market reactions to paint a comprehensive picture of the inflationary landscape. Moreover, their regular interviews with central bank officials, policymakers, and industry leaders provide direct insights into the thought processes and strategies that influence monetary policy and market responses. In a rapidly changing global economy, these sites serve as vital pulse-checks, capturing the intricacies of inflationary shifts as they happen."
  },
  {
    "objectID": "data-visualisation.html#macro-economic-indicators-for-inflation-analysis-a-deep-dive",
    "href": "data-visualisation.html#macro-economic-indicators-for-inflation-analysis-a-deep-dive",
    "title": "Data Visualization",
    "section": "",
    "text": "The US economy, being one of the largest and most influential economies globally, is driven by a plethora of interlinked factors. To fully grasp its dynamics, one must consider a vast array of economic indicators. The visualizations presented here encapsulate multiple crucial aspects of the US economy, each shedding light on various sectors and metrics.\nIn the ever-changing landscape of the global economy, keeping an eye on macro-economic indicators is crucial for investors, policymakers, and the general public. These indicators provide a snapshot of an economy’s health, giving insights into its future direction.\n\n\n\nDefinition: The Consumer Price Index (CPI) measures the average change in prices paid by urban consumers for a basket of goods and services.\nSignificance: It’s a primary tool to gauge the cost of living and purchasing power. High inflation can erode savings and purchasing power, while deflation can indicate an economic slowdown.\nInfluencing Factors: Supply chain disruptions, increased demand, production costs, and monetary policies.\n\n\nUnemployment Rate (UNRATE):\n\nDefinition: The percentage of the labor force that is jobless but is actively looking for work.\nSignificance: A high unemployment rate can signal economic distress, while a very low rate might indicate a booming economy but can also hint at potential overheating.\nInfluencing Factors: Economic cycles, technological changes, global events, and governmental policies.\n\nPersonal Saving Rate (PSAVERT):\n\nDefinition: The portion of personal income that households are saving.\nSignificance: A rising rate may indicate cautious consumers who are holding back on spending, whereas a falling rate might signal increased consumer confidence.\nInfluencing Factors: Economic outlook, interest rates, and household debt.\n\nMoney Supply (M2):\n\nDefinition: The total amount of monetary assets in the economy, including cash, checking deposits, and savings.\nSignificance: Rapid growth in M2 might indicate future inflation, while a decline might hint at economic slowdowns.\nInfluencing Factors: Central bank policies, public demand for money, and bank lending activities.\n\nReal Disposable Income (DSPIC96):\n\nDefinition: The income available to households after adjusting for inflation and taxes.\nSignificance: It shows consumers’ ability to spend and save. Rising real disposable income encourages consumer spending.\nInfluencing Factors: Employment levels, wage growth, taxation, and inflation.\n\nPersonal Consumption Expenditure (PCE):\n\nDefinition: The value of goods and services consumed by households.\nSignificance: It’s a primary measure of consumer spending and thus indicates consumer confidence and potential economic growth.\nInfluencing Factors: Disposable income, consumer confidence, and credit availability.\n\nTreasury Yield (GS10):\n\nDefinition: The return on investment for a 10-year U.S. government bond.\nSignificance: It can indicate the risk appetite of investors. Lower yields might suggest a flight to safety, while higher yields could indicate optimism.\nInfluencing Factors: Central bank policies, economic outlook, and global investment flows.\n\nFederal Funds Rate (FEDFUNDS):\n\nDefinition: The interest rate at which banks lend money to each other overnight.\nSignificance: It directly impacts interest rates throughout the economy, influencing borrowing costs and investment decisions.\nInfluencing Factors: Inflation, employment levels, and economic growth.\n\nConstruction Spending (TTLCONS):\n\nDefinition: The total dollar amount spent on construction projects.\nSignificance: High spending can indicate economic growth and confidence in the real estate sector.\nInfluencing Factors: Interest rates, economic outlook, and property demand.\n\nIndustrial Production Index (INDPRO):\n\n\nDefinition: A measure of the production output of factories, mines, and utilities.\nSignificance: It reflects the health of the industrial sector, which is a significant component of GDP.\nInfluencing Factors: Demand for goods, technological advancements, and supply chain factors.\n\n\nCore CPI (CPILFESL):\n\n\nDefinition: The CPI minus the volatile food and energy sectors.\nSignificance: It offers a clearer view of the underlying inflation trends without the noise of volatile sectors.\nInfluencing Factors: Same as CPI but without the direct impacts of food and energy prices."
  },
  {
    "objectID": "data-visualisation.html#us-inflation-rate-a-historical-perspective",
    "href": "data-visualisation.html#us-inflation-rate-a-historical-perspective",
    "title": "Data Visualization",
    "section": "US Inflation Rate: A Historical Perspective",
    "text": "US Inflation Rate: A Historical Perspective\nThe inflation rate, a reflection of the rate at which the general level of prices for goods and services rise, serves as a key indicator of a nation’s economic health. By examining the US inflation rate over the years, we can discern how various geopolitical and economic events have shaped the country’s financial landscape.\nThe first visualization presents the US inflation rate over the years, highlighting critical events. Inflation rate changes have historically been influenced by a combination of geopolitical events, domestic policies, and global economic trends. From the OPEC Oil Embargo in 1974 to the Financial Crisis in 2007, and the recent impact of the COVID-19 pandemic, understanding the inflation rate is key to interpreting the broader economic climate.\n\nHistorical Peaks in Inflation\nThe provided graph plots the US inflation rate across multiple decades and highlights certain key events which have had a significant influence on the economy.\n\n1969: Booming Economy\nThe late 1960s witnessed a booming US economy, characterized by increased consumer spending, substantial economic growth, and low unemployment. However, it was also a precursor to the inflationary pressures of the 1970s.\n1973: OPEC Oil Embargo\nThe dramatic spike in inflation around 1973 can be attributed to the OPEC (Organization of the Petroleum Exporting Countries) oil embargo, a geopolitical event that saw a rapid rise in oil prices and consequently, global inflation.\n1979: Iranian Revolution & Iraq-Iran War\nThe late 1970s again saw a surge in inflation. The Iranian Revolution, followed by the outbreak of the Iraq-Iran War, disrupted global oil supplies, driving prices and inflation upwards.\n1990: First Gulf War\nThe onset of the First Gulf War led to disruptions in oil production, echoing the oil crises of the 1970s, and pushing inflation rates up temporarily.\n2007: Financial Crisis\nOften termed the worst financial crisis since the Great Depression, the 2007-2008 financial crisis had profound effects on global economies. In the US, it led to recessionary pressures, impacting inflation rates.\n2020: COVID-19 Pandemic Impact\nThe novel coronavirus pandemic brought unprecedented challenges, causing disruptions in global supply chains and changing consumption patterns. Its effects on the inflation rate are notable, as governments around the world pumped fiscal stimulus into their economies, leading to varied inflationary outcomes.\n\n\n\nCode\n# Fetching the inflation rates for the specified years\nannotation_data &lt;- subset(inflation_data, Year %in% c(1969, 1973, 1979, 1990, 2007, 2020))\nannotation_data$label &lt;- c(\"Booming Economy\", \"OPEC Oil Embargo\", \"Iraq-Iran War\", \"First Gulf War\", \"Financial Crisis\", \"COVID-19 Pandemic Impact\")\n\n# Plotting the data\ninflation_plot &lt;- ggplot(inflation_data, aes(x = Year, y = Inflation.Rate)) +\n  geom_line(color = \"blue\") +\n  geom_point(data = annotation_data, aes(x = Year, y = Inflation.Rate), color = \"red\", size = 3) +\n  geom_text(data = annotation_data, aes(x = Year, y = Inflation.Rate, label = label), vjust = -2, color = \"darkred\", size = 4, nudge_y = 0.2) +\n  labs(title = \"US Inflation Rate Over the Years\", \n       x = \"Year\", \n       y = \"Inflation Rate (%)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face=\"bold\", size=14), \n        axis.title = element_text(face=\"bold\", size=12))\n\n# Display\nprint(inflation_plot)\n\n\n\n\n\nThe US inflation rate serves as a reflective mirror, capturing the essence of global events, whether they be economic, political, or natural. By understanding its fluctuations and the reasons behind them, one can gain insights into the broader narrative of global economic history and be better prepared for future challenges."
  },
  {
    "objectID": "data-visualisation.html#macro-economic-overview-a-visual-snapshot",
    "href": "data-visualisation.html#macro-economic-overview-a-visual-snapshot",
    "title": "Data Visualization",
    "section": "Macro-Economic Overview: A Visual Snapshot",
    "text": "Macro-Economic Overview: A Visual Snapshot\nThe macroeconomic landscape is vast and complex, comprised of numerous indicators that help economists, policymakers, and the public at large understand the health and direction of an economy. Here, we delve into a visual analysis of various US macroeconomic indicators over time, providing an insightful snapshot of the country’s economic journey.\n\nDiving into the Indicators\n1. Unemployment Rate:\n\nHigh Rate: Indicates economic distress with fewer job opportunities.\nLow Rate: Suggests a booming economy, but very low rates might hint at a tight labor market, potentially leading to wage inflation.\n\n2. Personal Saving Rate:\n\nIncrease: Indicates cautious consumers possibly due to economic uncertainty.\nDecrease: Suggests increased consumer confidence or a need to spend more due to rising costs.\n\n3. M2 Money Supply:\n\nIncrease: Can be linked to inflationary pressures and might hint at expansionary monetary policies.\nDecrease: Might indicate contractionary policies or reduced liquidity in the economy.\n\n4. Disposable Income:\n\nRise: Indicates an increase in household purchasing power.\nFall: May suggest reduced earnings, higher taxation, or other economic challenges.\n\n5. Personal Consumption Expenditure:\n\nRise: Reflects increased consumer spending, hinting at economic growth.\nFall: Indicates reduced consumer confidence or purchasing power.\n\n6. 10Y Treasury Yield:\n\nRise: Could suggest anticipated inflation, economic growth, or reduced demand for bonds.\nFall: May indicate economic uncertainty, deflation, or increased bond demand.\n\n6. Federal Funds Rate:\n\nIncrease: Suggests a tightening monetary policy, possibly to combat inflation or cool down an overheating economy.\nDecrease: Indicates an expansionary policy, likely to stimulate economic growth.\n\n7. Construction Spending:\n\nRise: Reflects a booming construction sector, hinting at economic growth and possible future housing supply.\nFall: May signal economic challenges or reduced demand for housing.\n\n8. Industrial Production Index:\n\nRise: Shows a healthy industrial sector, suggesting economic growth.\nFall: Indicates potential challenges in the manufacturing sector, possibly due to reduced demand or supply chain disruptions.\n\n\n\nCode\n# A vector of colors, here we are using the Set1 palette, but you can choose another one.\ncolors &lt;- brewer.pal(9, \"Set1\")\n\nplot_macro_data &lt;- function(data, title, color) {\n  colname &lt;- colnames(data)[1]\n  df &lt;- data.frame(Date = index(data), Value = as.numeric(data[, colname]))\n  \n  p &lt;- ggplot(df, aes(x = Date, y = Value)) + \n       geom_line(color = color, size = 1) +\n       labs(title = title) +\n       theme(plot.title = element_text(size = 8, hjust = 0.5, face = \"bold\"),\n             panel.grid.major = element_blank(),\n             panel.grid.minor = element_blank(),\n             panel.background = element_rect(fill = \"white\"),\n             plot.background = element_rect(fill = \"white\"),\n             panel.border = element_blank())  # Removing the grid boxes\n  return(p)\n}\n\nplot_unemployment &lt;- plot_macro_data(unemployment_data, \"Unemployment Rate\", colors[2])\nplot_personal_saving &lt;- plot_macro_data(personal_saving_rate_data, \"Personal Saving Rate\", colors[2])\nplot_m2 &lt;- plot_macro_data(m2_data, \"M2 Money Supply\", colors[2])\nplot_disposable_income &lt;- plot_macro_data(disposable_income_data, \"Disposable Income\", colors[2])\nplot_personal_consumption_expenditure &lt;- plot_macro_data(personal_consumption_expenditure_data, \"Personal Consumption Expenditure\", colors[2])\nplot_treasury_yield &lt;- plot_macro_data(treasury_yield_data, \"10Y Treasury Yield\", colors[2])\nplot_fed_rate &lt;- plot_macro_data(fed_rate_data, \"Federal Funds Rate\", colors[2])\nplot_construction_spending &lt;- plot_macro_data(construction_spending_data, \"Construction Spending\", colors[2])\nplot_industrial_production_index &lt;- plot_macro_data(industrial_production_index_data, \"Industrial Production Index\", colors[2])\n\nfinal_plot &lt;- (plot_unemployment | plot_personal_saving | plot_m2) /\n              (plot_disposable_income | plot_personal_consumption_expenditure | plot_treasury_yield) /\n              (plot_fed_rate | plot_construction_spending | plot_industrial_production_index)\n\nprint(final_plot)\n\n\n\n\n\nThese visual representations offer a comprehensive yet concise look at various facets of the US economy. Each plot captures the ebb and flow of its respective indicator, reflecting the economic events, policy changes, and global happenings that have steered the nation’s course. For investors, policymakers, and curious minds, such a mosaic of data provides a foundational understanding, facilitating informed decisions and future predictions.\nThere is a simultaneous rise in the Unemployment Rate and Personal Saving Rate during 2020, with a decrease in the Industrial Production Index and Construction Spending: This period reflects the economic impact of the COVID-19 pandemic. The increased unemployment rate shows the immediate effect of lockdowns and reduced business activities. The elevated personal saving rate is likely a result of consumer uncertainty during these times, leading to reduced spending and increased saving. A declining Industrial Production Index suggests disruptions in manufacturing, possibly due to supply chain challenges or reduced workforce availability. The dip in construction spending further reflects the economic slowdown during this period, with reduced investments in infrastructure or real estate."
  },
  {
    "objectID": "data-visualisation.html#visualizing-the-evolution-of-consumer-price-index-components",
    "href": "data-visualisation.html#visualizing-the-evolution-of-consumer-price-index-components",
    "title": "Data Visualization",
    "section": "Visualizing the Evolution of Consumer Price Index Components",
    "text": "Visualizing the Evolution of Consumer Price Index Components\n\nIntroduction\nConsumer Price Index (CPI) is a significant economic indicator, gauging the average change over time in the prices paid by urban consumers for a basket of consumer goods and services. By deconstructing the CPI, one can glean insights into various sectors’ price movement and better understand the nuances of inflationary trends. In the interactive visualization presented, we breakdown the CPI into its core components, offering a deeper dive into the dynamics of consumer spending.\nLastly, the area plot dissects the Consumer Price Index (CPI), laying bare the price movements of various consumer goods and services categories. From housing and apparel to medical care and education, this visualization lets us explore the nuances of consumer spending and inflationary trends in different sectors.\n\n\nUnderstanding the Visualization\nThe area plot vividly captures the journey of seven core CPI components:\n\nAll Items: This provides a comprehensive view, representing the cumulative change in prices of all goods and services.\nFood & Beverages: Tracking the prices of essentials like food items and drinks showcases the dynamics of basic necessities’ pricing.\nHousing: Housing costs, including rents and home values, have a massive impact on consumer budgets. Monitoring this can indicate broader real estate market trends.\nApparel: The prices of clothing and accessories give insights into discretionary spending and can hint at consumer confidence.\nTransportation: Whether it’s the cost of buying vehicles or fuel prices, transportation costs can be indicative of energy market dynamics and manufacturing trends.\nMedical Care: With healthcare being a significant concern, its pricing can reflect policy changes, innovations, and overall health sector dynamics.\nEducation & Communication: This captures the costs associated with educational services and communication tools, resonating with the investment in human capital and technological trends.\n\nThe colors and areas under each line in the plot represent the value of each CPI component over time. As you move along the x-axis (Date), the changing areas give a sense of how each component’s pricing has evolved.\n\n\nCode\n# Merging all the xts data by Date\nmerged_data &lt;- merge(all_items, food_beverages, housing, apparel, transportation, medical_care, education_communication)\n\n# Convert the merged xts object to a data.frame\ndf &lt;- data.frame(Date=index(merged_data), coredata(merged_data))\n\n## Renaming columns in df\ndf &lt;- df %&gt;%\n  rename(\n    all_items = CPIAUCSL,\n    food_beverages = CPIUFDSL,\n    housing = CPIHOSSL,\n    apparel = CPIAPPSL,\n    transportation = CPITRNSL,\n    medical_care = CPIMEDSL,\n    education_communication = CPIEDUSL\n  )\n\nlibrary(plotly)\n\n# Create an area plot\nplot_ly(df, x = ~Date) %&gt;%\n  add_trace(y = ~all_items, name = \"All Items\", fill = 'tozeroy', type = 'scatter', mode = 'lines') %&gt;%\n  add_trace(y = ~food_beverages, name = \"Food & Beverages\", fill = 'tozeroy', type = 'scatter', mode = 'lines') %&gt;%\n  add_trace(y = ~housing, name = \"Housing\", fill = 'tozeroy', type = 'scatter', mode = 'lines') %&gt;%\n  add_trace(y = ~apparel, name = \"Apparel\", fill = 'tozeroy', type = 'scatter', mode = 'lines') %&gt;%\n  add_trace(y = ~transportation, name = \"Transportation\", fill = 'tozeroy', type = 'scatter', mode = 'lines') %&gt;%\n  add_trace(y = ~medical_care, name = \"Medical Care\", fill = 'tozeroy', type = 'scatter', mode = 'lines') %&gt;%\n  add_trace(y = ~education_communication, name = \"Education & Communication\", fill = 'tozeroy', type = 'scatter', mode = 'lines') %&gt;%\n  layout(title = \"Area Plot of CPI Components Over Time\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Value\"),\n         showlegend = TRUE)\n\n\n\n\n\n\nLet’s go through a brief overview of the changes in CPI for these categories from 1947 to 2023:\n\nMedical Care:\n\nThe cost of medical care in the U.S. has seen significant increases over the decades. Factors like technological advancements, an aging population, increased demand for services, and rising pharmaceutical prices have driven costs upward.\nOver the years, the U.S. has also grappled with challenges in its healthcare system, including inefficiencies, regulatory changes, and debates over health insurance and coverage. All these factors have contributed to the rise in medical care costs.\n\nHousing:\n\nHousing costs have also experienced substantial growth since the 1940s, influenced by various factors like population growth, urbanization, land use regulations, and economic cycles.\nThere have been periods of rapid price appreciation, such as during housing bubbles, followed by corrections or crashes. Additionally, the cost of renting has also been on an upward trajectory in many urban areas, in part due to supply constraints and increasing demand.\n\nFood and Beverage:\n\nThe cost of food and beverages has been influenced by factors like changes in agricultural productivity, global trade policies, supply chain disruptions, and changing consumer preferences.\nFor instance, droughts, diseases, or trade disputes can lead to temporary spikes in certain food items. Moreover, the rise of organic and specialty foods in recent decades has also had an impact on average prices.\n\nTransportation:\n\nTransportation costs encompass a wide range of expenses, including the price of vehicles, fuel, public transportation, and more.\nOil price shocks, technological advancements in vehicles, infrastructure spending, and changing consumer habits (like the move towards electric vehicles) can influence this category. For instance, the oil crises of the 1970s led to substantial increases in transportation costs.\n\n\nIt’s essential to note that while these categories have seen increases in nominal terms (i.e., the absolute dollar amount), it’s also crucial to consider the real increase (i.e., adjusting for inflation). Over the years, wages and incomes have also risen, so the relative burden of these costs on households can vary.\nAdditionally, regional variations can be substantial. For example, housing or transportation costs in urban areas might be much higher than in rural areas.\nBy visualizing the individual components of the CPI, we gain a more granular perspective on the inflationary trends that affect consumers. It provides a roadmap for analysts, policymakers, and consumers to make informed decisions, whether it’s setting monetary policy, making investment choices, or planning household budgets."
  },
  {
    "objectID": "exploratory-data-analysis.html#visualizing-monthly-inflation-rates",
    "href": "exploratory-data-analysis.html#visualizing-monthly-inflation-rates",
    "title": "Exploratory Data Analysis",
    "section": "Visualizing Monthly Inflation Rates",
    "text": "Visualizing Monthly Inflation Rates\nThe initial plot showcases the monthly inflation rates across the years. This visualization allows for a quick grasp of the overall inflation trends, understanding its fluctuations and identifying any patterns or anomalies.\n\n\nCode\n# Convert the time series to a data frame\ninflation_df &lt;- data.frame(Date = index(inflation_monthly), \n                           InflationRate = coredata(inflation_monthly))\n\n# Use the data frame for plotting\nplot_ly(data = inflation_df, x = ~Date, y = ~CPIAUCNS, type = 'scatter', mode = 'lines') %&gt;%\n  layout(title = \"Monthly Inflation Rate over Time\",\n         xaxis = list(title = \"Year\"),\n         yaxis = list(title = \"Inflation Rate (%)\"))\n\n\n\n\n\n\n\nKey Observations:\n\nOverall Trend: The graph show an upward trajectory, suggesting that prices have generally been increasing over the years. However, the steepness of this trajectory would indicate the pace of this inflation. A steeper line might suggest rapid inflation, while a flatter line could indicate a slower inflation rate.\nShort-term Fluctuations: While the general trend is upward, we see numerous dips and peaks. These fluctuations represent monthly changes, and they might be influenced by seasonal factors, such as holiday shopping seasons or energy price variations in winter and summer.\nSignificant Spikes: A sharp upward spike in the graph could suggest a period of hyperinflation. This could be due to various reasons, such as economic policies leading to an oversupply of money, supply chain disruptions causing scarcity, or geopolitical events causing sudden economic shocks.\nProlonged Low Points: We notice extended periods where the inflation rate remains notably low or even goes negative (deflation), it indicates economic recessions or periods of stagnated growth. Deflationary periods are associated with decreased consumer demand or overproduction.\nRecent Trends: By focusing on the most recent data points, we infer the current state of the economy. For instance, a rising inflation rate in the most recent months is a signal for policymakers to intervene to prevent potential overheating of the economy.\n\nThere’s a visible peak in the inflation rate around 2008. This is linked to the global financial crisis, where many economies faced recessionary pressures. Conversely, there was a dip in early 2020, it is attributed to the economic implications of the COVID-19 pandemic, with reduced consumer spending and global economic slowdown.\nThe Monthly Inflation Rate chart is a lens through which we can view the historical and current health of the economy. While the line plot is a powerful visualization tool, the real value lies in understanding the stories and causes behind the data points. As we analyze the chart, it becomes evident that inflation is not just a number; it’s a reflection of collective economic decisions, challenges, and triumphs."
  },
  {
    "objectID": "exploratory-data-analysis.html#lag-analysis-of-monthly-inflation",
    "href": "exploratory-data-analysis.html#lag-analysis-of-monthly-inflation",
    "title": "Exploratory Data Analysis",
    "section": "Lag Analysis of Monthly Inflation",
    "text": "Lag Analysis of Monthly Inflation\nBy creating a lag plot of the monthly inflation data, we can better understand its structure. Lag plots help identify any potential autoregressive patterns in the time series data. From the lag plot, one can observe if there’s a structured pattern or if the data appears random.\n\n\nCode\n# Convert the inflation data to a time series object\ninflation_monthly_ts &lt;- ts(coredata(inflation_monthly), start = c(1913, 2), frequency = 12)\n\n# Generate the lag plot\ngglagplot(inflation_monthly_ts, do.lines=FALSE) +\n  ggtitle(\"Lag Plot of Monthly Inflation Data\") +\n  xlab(\"Lagged Inflation Rate\") +\n  ylab(\"Inflation Rate\") +\n  theme(axis.text.x = element_text(angle=45, hjust=1))\n\n\n\n\n\n\nKey Observations:\n\nPattern Description: Points clustering or aligning closely around a diagonal line from the bottom-left to the top-right indicates a strong positive auto-correlation. In simpler terms, this means that a value at any given time t is closely related to the value at time t-1 (the prior time step).\nEconomic Implications: A positive auto-correlation in the inflation rate indicates that if the inflation rate is high in one month, it’s likely to be high in the subsequent month as well. Similarly, if it’s low in a particular month, it’s expected to be low in the next month. This continuity could be the result of several factors:\n\nLagged Economic Responses: Economic policies or external shocks often don’t manifest their full impact immediately. So, for instance, if certain fiscal or monetary policies lead to an increase in inflation, the repercussions might be felt over several subsequent months.\nInertia in Economic Indicators: Some economic indicators have a natural inertia, meaning they change slowly over time due to the size and complexity of economies. Inflation, being a measure of price levels across a wide array of goods and services, can often exhibit such behavior.\nBehavioral Factors: If businesses expect inflation to continue rising, they might preemptively increase prices, leading to a self-fulfilling prophecy. Similarly, if consumers expect higher prices in the future, they might make purchases now rather than later, driving demand and potentially contributing to the ongoing inflation.\n\n\n\n\nConclusion:\nA strong positive auto-correlation in monthly inflation data indicates persistence and momentum in the inflation rates from month to month. This insight can be valuable for economic analysts, policymakers, and forecasters in both understanding past inflation trends and making predictions about future movements."
  },
  {
    "objectID": "exploratory-data-analysis.html#decomposing-the-time-series",
    "href": "exploratory-data-analysis.html#decomposing-the-time-series",
    "title": "Exploratory Data Analysis",
    "section": "Decomposing the Time Series",
    "text": "Decomposing the Time Series\nDecomposition of the time series into its underlying components can provide profound insights:\n\nTrend Component: This reveals the long-term progression of the series, which can either be upward, downward, or constant.\nSeasonal Component: This showcases any cyclical patterns that repeat over a known, fixed period of time.\nRandom Component: This captures the noise or randomness which isn’t accounted for by the trend or seasonality.\n\n\n\nCode\ndecomposed &lt;- decompose(inflation_monthly_ts, type = \"additive\")\n\n# Create separate data frames for each component\ndata_trend &lt;- data.frame(Date = index(inflation_monthly), Value = decomposed$trend, stringsAsFactors = FALSE)\ndata_seasonal &lt;- data.frame(Date = index(inflation_monthly), Value = decomposed$seasonal, stringsAsFactors = FALSE)\ndata_random &lt;- data.frame(Date = index(inflation_monthly), RandomValue = decomposed$random, stringsAsFactors = FALSE)\ncolnames(data_random) &lt;- c(\"Date\", \"Value\")\n\n# Create individual plots\nplot_trend &lt;- plot_ly(data = data_trend, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines', name = \"Trend\", line = list(color = 'blue')) %&gt;%\n  layout(title = \"Trend Component of Monthly Inflation Rate\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Value\"))\n\nplot_seasonal &lt;- plot_ly(data = data_seasonal, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines', name = \"Seasonal\", line = list(color = 'green')) %&gt;%\n  layout(title = \"Seasonal Component of Monthly Inflation Rate\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Value\"))\n\nplot_random &lt;- plot_ly(data = data_random, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines', name = \"Random\", line = list(color = 'red')) %&gt;%\n  layout(title = \"Random Component of Monthly Inflation Rate\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Value\"))\n\n# Use subplot to display them\nsubplot(plot_trend, plot_seasonal, plot_random, nrows = 3, margin = 0.05)\n\n\n\n\n\n\n\n1. Trend Component:\n\nVisual Representation: Blue line chart titled “Trend Component of Monthly Inflation Rate”.\nInterpretation: The trend in the inflation rate is not static but rather exhibits changes over time. This implies that the long-term average level of inflation has been shifting, suggesting periods of rising and falling inflation rates. Such shifts in the trend could be attributed to various macroeconomic factors, including changes in monetary policies, technological advancements, and shifts in the global economy.\nEconomic Implications: A dynamic trend indicates that the economic environment is evolving. The factors contributing to these changes could be multifaceted:\n\nMonetary Policy: Central banks might be adjusting interest rates or employing other tools that impact the money supply and, consequently, the inflation rate.\nGlobal Factors: Changes in the global economic landscape, such as shifts in global trade patterns, can influence domestic inflation rates.\nStructural Changes: Technological innovations or significant market entries and exits can cause long-term changes in demand and supply dynamics, impacting prices.\n\n\n\n\n2. Seasonal Component:\n\nVisual Representation: Green line chart titled “Seasonal Component of Monthly Inflation Rate”.\nInterpretation: The consistency in the seasonal component suggests that there are regular and predictable fluctuations in the inflation rate that recur at specific intervals. These fluctuations remain fairly constant over time, indicating well-established seasonal patterns.\nEconomic Implications: A constant seasonal component indicates predictable, recurring influences on inflation:\n\nConsumer Behavior: As before, regular shopping seasons, holidays, or other annual events might drive these patterns.\nSupply Side Regularities: Seasonal factors such as agricultural cycles or periodic maintenance shutdowns in industries might be at play.\n\n\n\n\n3. Random (or Residual) Component:\n\nVisual Representation: Red line chart titled “Random Component of Monthly Inflation Rate”.\nInterpretation: The variability in the random component over time indicates that there are irregular factors or shocks influencing the inflation rate. These are events or factors that aren’t accounted for by the trend or seasonal components.\nEconomic Implications: A varying random component suggests an economy that, while having predictable trend and seasonal elements, is also influenced by unexpected events:\n\nExternal Shocks: These could be stronger or more frequent than initially thought, causing more pronounced variations in the data.\nShort-term Dynamics: The economy might be reacting to short-lived news, events, or temporary market conditions that cause abrupt changes in prices.\nData Collection: Sometimes, irregularities in data collection or adjustments can lead to variations that appear random.\n\n\n\n\nConclusion:\nThe decomposition provides a layered understanding of the inflation rate’s behavior. The changing trend suggests an evolving economic environment, the consistent seasonality provides predictability, and the varying random component indicates the presence of unforeseen factors or events."
  },
  {
    "objectID": "exploratory-data-analysis.html#autocorrelation-functions-of-monthly-inflation",
    "href": "exploratory-data-analysis.html#autocorrelation-functions-of-monthly-inflation",
    "title": "Exploratory Data Analysis",
    "section": "Autocorrelation Functions of Monthly Inflation",
    "text": "Autocorrelation Functions of Monthly Inflation\nAutocorrelation functions, both ACF and PACF, provide an understanding of the correlation of a series with its lags. This is crucial for building and diagnosing time series models:\n\nACF (Autocorrelation Function): It measures the linear relationship between the time series and its lagged values.\nPACF (Partial Autocorrelation Function): It measures the correlation between the time series and its lagged values after removing the effects of previous lags.\n\n\n\nCode\n# Compute ACF and PACF\nacf_vals &lt;- acf(inflation_monthly_ts, plot = FALSE)\npacf_vals &lt;- pacf(inflation_monthly_ts, plot = FALSE)\n\n# ACF Plot\nplot_ly(y = acf_vals$acf[-1], type = 'scatter', mode = 'lines+markers', name = 'ACF') %&gt;%\n  layout(title = \"ACF of Monthly Inflation Rate\",\n         xaxis = list(title = \"Lag\"),\n         yaxis = list(title = \"ACF Value\"))\n\n\n\n\n\n\nCode\n# PACF Plot\nplot_ly(y = pacf_vals$acf[-1], type = 'scatter', mode = 'lines+markers', name = 'PACF') %&gt;%\n  layout(title = \"PACF of Monthly Inflation Rate\",\n         xaxis = list(title = \"Lag\"),\n         yaxis = list(title = \"PACF Value\"))\n\n\n\n\n\n\nThe Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) are integral tools in time series analysis, especially when characterizing and identifying the nature of a given series. In this context, we’re inspecting the Monthly Inflation Rate.\nAutocorrelation Function (ACF):\n\nThe ACF plot graphically presents the correlation between the inflation rate and its own lags. By visually inspecting the ACF plot for the inflation rate, one can discern patterns and structures hidden within the data.\nA clear observation is the series’ non-stationary nature. For a stationary time series, we would expect the ACF values to decline relatively quickly and hover around zero. In the case of inflation, however, there’s a lingering correlation across multiple lags, suggesting that the series might benefit from differencing or other transformations to achieve stationarity.\nNo clear seasonality in inflation is apparent from the ACF plot, as we don’t observe recurring significant spikes at regular intervals. This means that inflation rates might not be inherently tied to specific months or seasons in a predictable pattern.\n\nPartial Autocorrelation Function (PACF):\n\nThe PACF, on the other hand, provides insights into the direct effect of previous lags on the current observation, after discounting the influence of intermediate lags. For the inflation data, we notice that the PACF plot shows significant autocorrelations only for the initial few lags, particularly lags 1 and 2.\nThe rapid decay and oscillation around zero in the PACF plot after the initial lags suggests that the inflation rate might be influenced primarily by its very recent past values, rather than longer historical values. This potentially points to an autoregressive nature of the inflation series of order 1 or 2.\nThe fact that the PACF values remain within the confidence bands (typically represented by blue dashed lines, though not explicitly mentioned here) from lag 2 onwards suggests that these correlations are not statistically significant, reinforcing the potential autoregressive nature hinted at the initial lags.\n\nConclusion: Given the behavior observed in the ACF and PACF plots, the Monthly Inflation Rate appears to possess a non-stationary nature. While there isn’t a clear seasonal pattern, the data does hint towards an autoregressive process of order 1 or 2, given the significant initial lags in the PACF plot. Policymakers and financial analysts can use this insight to understand that recent changes in inflation are more influential than distant historical rates. It also underscores the importance of regular monitoring and swift policy adjustments to address short-term inflationary pressures."
  },
  {
    "objectID": "exploratory-data-analysis.html#adf-test-of-monthly-inflation",
    "href": "exploratory-data-analysis.html#adf-test-of-monthly-inflation",
    "title": "Exploratory Data Analysis",
    "section": "ADF Test of Monthly Inflation",
    "text": "ADF Test of Monthly Inflation\n\n\nCode\nadf_test &lt;- adf.test(inflation_monthly_ts)\nprint(adf_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  inflation_monthly_ts\nDickey-Fuller = -5.8365, Lag order = 10, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe Augmented Dickey-Fuller (ADF) test is a commonly used technique to determine the stationarity of a time series. The stationarity of a time series means that its properties do not change over time, i.e., the mean, variance, and covariance are constant over time. The ADF test evaluates the null hypothesis that a unit root is present in a time series sample, which implies non-stationarity.\nDickey-Fuller Value:\n\nThe Dickey-Fuller test statistic value is -5.8342. This value is more negative than the typical critical values (e.g., -3.5, -2.9, -2.6 for 1%, 5%, and 10% significance levels, respectively, though exact values can vary based on sample size and other factors).\n\nLag Order:\n\nThe lag order of 10 indicates that 10 lags of the dependent variable (inflation_monthly_ts in this case) were included in the test regression. The Augmented Dickey-Fuller test introduces lags to account for autocorrelation and ensure that the residuals (errors) of the regression are white noise.\n\np-value:\n\nA p-value of 0.01 (or 1% level of significance) is quite low, typically below common significance levels such as 0.05 or 0.10. A low p-value like this leads to the rejection of the null hypothesis.\n\nConclusion: Given the Dickey-Fuller test statistic value of -5.8342 and a p-value of 0.01, we reject the null hypothesis that a unit root is present in the series at the 1% significance level. The data provides strong evidence to suggest that the time series inflation_monthly_ts is stationary. This means that the mean, variance, and autocorrelation structure of the inflation rate do not change over time."
  },
  {
    "objectID": "exploratory-data-analysis.html#detrending-techniques-for-monthly-inflation",
    "href": "exploratory-data-analysis.html#detrending-techniques-for-monthly-inflation",
    "title": "Exploratory Data Analysis",
    "section": "Detrending Techniques for Monthly Inflation",
    "text": "Detrending Techniques for Monthly Inflation\nDetrending is an essential pre-processing step for many time series analyses:\n\nFirst-order Differencing: It removes the trend in the time series by taking the difference between consecutive observations.\nTrend Model Fitting: By fitting a linear trend model, one can extract the trend and study the detrended series.\n\n\n\nCode\n# First order difference\ninflation_diff &lt;- diff(inflation_monthly_ts)\n\n# Plot\nplot(inflation_diff, main=\"Detrended Inflation Data using First-order Differencing\", ylab=\"First-order Differences\")\n\n\n\n\n\n\n\nCode\n# Fit a trend model\ntrend_model &lt;- lm(inflation_monthly_ts ~ time(inflation_monthly_ts))\nsummary(trend_model)\n\n\n\nCall:\nlm(formula = inflation_monthly_ts ~ time(inflation_monthly_ts))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4337 -0.2498 -0.0326  0.2715  5.4724 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                -1.1721938  1.0774089  -1.088    0.277\ntime(inflation_monthly_ts)  0.0007273  0.0005473   1.329    0.184\n\nResidual standard error: 0.6379 on 1327 degrees of freedom\nMultiple R-squared:  0.001329,  Adjusted R-squared:  0.0005764 \nF-statistic: 1.766 on 1 and 1327 DF,  p-value: 0.1841\n\n\nCode\n# Convert to data frame\ndetrended_df &lt;- data.frame(\n  Date = time(inflation_monthly_ts),\n  DetrendedValue = resid(trend_model)\n)\n\n# Plot using ggplot\nggplot(data = detrended_df, aes(x = Date, y = DetrendedValue)) +\n  geom_line() +\n  ggtitle(\"Detrended Inflation Data by Subtracting Fitted Trend\") +\n  ylab(\"Detrended Values\") +\n  xlab(\"Date\") +\n  theme_minimal()\n\n\n\n\n\nThe output from the linear regression model indicates:\n\nThe formula for the linear trend fitted to the data:\nInflation =−1.2033+0.0007432×time\nThis suggests a very slight upward trend in inflation over time, but the coefficient of time (0.0007432) is small and not statistically significant given its p-value (0.176).\nResiduals, which are the differences between the observed values and the values predicted by the model, vary between -3.4333 and 5.4725. These residuals, when plotted, represent the detrended series. The median of the residuals is quite close to zero (-0.0329), which is a good indication, but the range is relatively wide.\nThe R-square value, which represents the proportion of variation in the inflation rate that can be explained by time, is very low (0.001382 or 0.1382%). This means that the linear model doesn’t capture much of the variability in the inflation rate.\n\nThe comparison between the first-order differencing and the trend model fitting techniques reveals interesting insights:\n\nThe detrended series from the linear model looks quite similar to the original inflation series, suggesting that the model might not have efficiently captured and removed the trend. The low �2R2 value further supports this point.\nThe mention of a quadratic model or first differencing indicates that the trend in inflation is possibly nonlinear. A quadratic model, with its curved line fit, might be better suited to capture this.\nEven after removing the trend, if seasonality is evident in the detrended data, it indicates that the original time series comprises multiple components, and just detrending isn’t sufficient for analysis."
  },
  {
    "objectID": "exploratory-data-analysis.html#autocorrelation-after-detrending-monthly-inflation",
    "href": "exploratory-data-analysis.html#autocorrelation-after-detrending-monthly-inflation",
    "title": "Exploratory Data Analysis",
    "section": "Autocorrelation after Detrending Monthly Inflation",
    "text": "Autocorrelation after Detrending Monthly Inflation\n\n\nCode\n# ACF\nacf(inflation_diff, main=\"ACF of First-order Differenced Monthly Inflation\")\n\n\n\n\n\nCode\n# PACF\npacf(inflation_diff, main=\"PACF of First-order Differenced Monthly Inflation\")\n\n\n\n\n\nConclusion: Based on our analysis, the first-order differenced series emerges as the preferred choice for building autoregressive models, owing to the enhanced stationarity it offers. However, the nuances captured by the original series, as suggested by its PACF plot, should not be overlooked. It might be valuable to also explore modeling the original series directly with AR models, potentially tapping into insights that the differenced series might not capture fully. In essence, while the differenced series presents a clearer and more straightforward path, the original series holds an allure of depth and complexity that might prove beneficial in understanding the intricate dynamics of inflation."
  },
  {
    "objectID": "exploratory-data-analysis.html#simple-moving-average-smoothing",
    "href": "exploratory-data-analysis.html#simple-moving-average-smoothing",
    "title": "Exploratory Data Analysis",
    "section": "Simple Moving Average Smoothing",
    "text": "Simple Moving Average Smoothing\nMoving averages smooth out short-term fluctuations and highlight longer-term trends:\n\n3-Point, 5-Point, 7-Point, and 9-Point Moving Averages: These visualizations provide smoothed curves, with each MA capturing different degrees of fluctuations.\n\n\n\nCode\n# Compute the 12-month moving average using the filter function from the stats package\nsma_12 &lt;- stats::filter(inflation_monthly_ts, rep(1/12, 12), sides=2)\n\n\n# Create a data frame for plotting\ndata_plot &lt;- data.frame(Date = time(inflation_monthly_ts),\n                        Inflation = as.numeric(inflation_monthly_ts),\n                        SMA_12 = as.numeric(sma_12))\n\n# Compute 3-MA\nma3 &lt;- autoplot(inflation_monthly_ts, series=\"Data\") +\n  autolayer(ma(inflation_monthly_ts,3), series=\"3-MA\") +\n  xlab(\"Year\") + ylab(\"Inflation Rate (%)\") +\n  ggtitle(\"3-MA: Monthly Inflation Rate\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3-MA\"))+\n   theme_minimal() +  # This theme provides a minimal styling\n  theme(\n    #panel.background = element_blank(),  # Remove background\n    panel.grid.major = element_blank(),   # Remove major grid\n    #panel.grid.minor = element_blank(),   # Remove minor grid\n    strip.background = element_blank()    # Remove background for facet labels (if any)\n  )\n\n# Compute 5-MA\nma5 &lt;- autoplot(inflation_monthly_ts, series=\"Data\") +\n  autolayer(ma(inflation_monthly_ts,5), series=\"5-MA\") +\n  xlab(\"Year\") + ylab(\"Inflation Rate (%)\") +\n  ggtitle(\"5-MA: Monthly Inflation Rate\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5-MA\"))+ theme_minimal() +  # This theme provides a minimal styling\n  theme(\n    #panel.background = element_blank(),  # Remove background\n    panel.grid.major = element_blank(),   # Remove major grid\n    #panel.grid.minor = element_blank(),   # Remove minor grid\n    strip.background = element_blank()    # Remove background for facet labels (if any)\n  )\n\n# Compute 7-MA\nma7 &lt;- autoplot(inflation_monthly_ts, series=\"Data\") +\n  autolayer(ma(inflation_monthly_ts,7), series=\"7-MA\") +\n  xlab(\"Year\") + ylab(\"Inflation Rate (%)\") +\n  ggtitle(\"7-MA: Monthly Inflation Rate\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"7-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"7-MA\"))+ theme_minimal() +  # This theme provides a minimal styling\n  theme(\n    #panel.background = element_blank(),  # Remove background\n    panel.grid.major = element_blank(),   # Remove major grid\n    #panel.grid.minor = element_blank(),   # Remove minor grid\n    strip.background = element_blank()    # Remove background for facet labels (if any)\n  )\n\n# Compute 9-MA\nma9 &lt;- autoplot(inflation_monthly_ts, series=\"Data\") +\n  autolayer(ma(inflation_monthly_ts,9), series=\"9-MA\") +\n  xlab(\"Year\") + ylab(\"Inflation Rate (%)\") +\n  ggtitle(\"9-MA: Monthly Inflation Rate\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"9-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"9-MA\"))+ theme_minimal() +  # This theme provides a minimal styling\n  theme(\n    #panel.background = element_blank(),  # Remove background\n    panel.grid.major = element_blank(),   # Remove major grid\n    #panel.grid.minor = element_blank(),   # Remove minor grid\n    strip.background = element_blank()    # Remove background for facet labels (if any)\n  )\n\n# Arrange plots in a grid\ngrid.arrange(ma3, ma5, ma7, ma9, nrow = 2, ncol=2)\n\n\n\n\n\nKey Observations:\n\nConsistency in Data and MA Values: A striking feature across all the MA plots is the closeness between the actual inflation data and the computed moving averages. This implies that short-term fluctuations in the inflation data are minimal, and the general trend of inflation has been consistent over the time period observed.\nGranularity of Smoothing:\n\n3-Point MA: Being the most granular of all the MAs used, the 3-Point MA closely trails the actual data, capturing most of its minor fluctuations. This might be helpful in environments where understanding small changes is critical.\n5-Point and 7-Point MA: As we increase the window size, the smoothed line becomes less reactive to short-term fluctuations, offering a more generalized view. The 5-Point and 7-Point MA lines provide a balanced perspective, revealing the broader trend without being overly smoothed.\n9-Point MA: The 9-Point MA, being the broadest, offers the most generalized view of the inflation trend. It suppresses most of the minor variations, focusing primarily on long-term movements.\n\nUnderlying Trend: The consistent proximity of the MAs to the original data and the overlapping nature of various MAs suggest that the inflation rate has been relatively stable. There haven’t been drastic shifts or volatile periods that deviate significantly from the central trajectory.\nUsability: The selection of a particular MA for further analysis or modeling would depend on the specific research question or business problem. For instance, if the goal is to understand very recent shifts in inflation, the 3-Point or 5-Point MAs might be more informative. However, if one is looking at longer-term economic strategies, the 7-Point or 9-Point MAs can offer a clearer picture by emphasizing broader movements and de-emphasizing short-term noise.\n\nConclusion: The SMA analysis on the monthly inflation data provides a rich perspective on the behavior of inflation over time. The close alignment of the moving averages with the actual data signifies a steady, predictable trend in inflation, with minimal short-term disruptions."
  },
  {
    "objectID": "exploratory-data-analysis.html#moving-average-smoothing-with-windowing-2x4",
    "href": "exploratory-data-analysis.html#moving-average-smoothing-with-windowing-2x4",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing with Windowing (2x4)",
    "text": "Moving Average Smoothing with Windowing (2x4)\n\n4-Point and 2x4-Point Moving Averages: This helps in understanding the short-term and long-term patterns respectively.\n\n\n\nCode\ninflation_monthly_ts_2 &lt;- window(inflation_monthly_ts, start= c(1913, 1))\n\n# 4-point moving average\nma4 &lt;- ma(inflation_monthly_ts_2, order=4, centre=FALSE)\n\n# 2x4-point moving average\nma2x4 &lt;- ma(inflation_monthly_ts_2, order=4, centre=TRUE)\n\nMA_2x4 = data.frame(inflation_monthly_ts_2, ma4, ma2x4)\n\n# Plot\nautoplot(inflation_monthly_ts_2, series=\"Data\") +\n  autolayer(ma(inflation_monthly_ts_2, order=4, centre=FALSE), series=\"4-MA\") +\n  autolayer(ma(inflation_monthly_ts_2, order=4, centre=TRUE), series=\"2x4-MA\") +\n  xlab(\"Year\") + ylab(\"Inflation Rate (%)\") +\n  ggtitle(\"Monthly Inflation Rate Over Time\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\"4-MA\"=\"red\",\"2x4-MA\"=\"blue\"),\n                      breaks=c(\"Data\",\"4-MA\",\"2x4-MA\")) +\n  theme_minimal() +  # This theme provides a minimal styling\n  theme(\n    #panel.background = element_blank(),  # Remove background\n    panel.grid.major = element_blank(),   # Remove major grid\n    #panel.grid.minor = element_blank(),   # Remove minor grid\n    strip.background = element_blank()    # Remove background for facet labels (if any)\n  )\n\n\n\n\n\n\nNew Time Frame: The decision to window the data starting from 1913 allows us to focus on a specific period which could have its unique characteristics or may be deemed more relevant to the current analysis.\nUnderstanding 4-MA and 2x4-MA:\n\n4-Point MA: The 4-Point Moving Average (4-MA) provides a relatively granular smoothing of the data. Given that it’s not centered, it tends to lag the actual data slightly and can be influenced more by the recent values in the series.\n2x4-Point MA: This technique involves applying a centered 4-month moving average, followed by another centered 2-month moving average. This “moving average of a moving average” approach, especially when m is even, is used to make the smoothing symmetric. The 2x4-MA smoothens the series further, thereby minimizing seasonal fluctuations and revealing the broader trend more clearly.\n\nSymmetry in Moving Averages: When the moving average order is even (like 4 in this case), the average isn’t naturally symmetric around the time point being estimated. By applying a secondary 2-month centered moving average (2x4-MA), symmetry is achieved, leading to a smoother representation that centers better with the original data points.\nResidual Seasonality: Despite the double smoothing with 2x4-MA, there appears to be some seasonality or cyclic patterns still present in the smoothed series. This suggests that the chosen moving average techniques, although effective to some extent, aren’t capturing all the underlying patterns in the data.\nNext Steps and Improvements: The persisting seasonality in the smoothed overlay hints at the complexity of the inflation data’s underlying patterns. To better represent the data and remove this seasonality, alternative moving averaging windows or more advanced time series decomposition techniques might be necessary. Moreover, the choice of smoothing technique should ideally align with the goal of the analysis—whether it’s to uncover the underlying trend, adjust for seasonality, or prepare the series for forecasting.\n\nConclusion: The inflation data from 1913 onwards provides a valuable look into historical economic trends. The application of both 4-MA and 2x4-MA offers varying degrees of smoothing, with each revealing different aspects of the data’s structure."
  },
  {
    "objectID": "exploratory-data-analysis.html#moving-average-smoothing-with-windowing-2x6",
    "href": "exploratory-data-analysis.html#moving-average-smoothing-with-windowing-2x6",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing with Windowing (2x6)",
    "text": "Moving Average Smoothing with Windowing (2x6)\n\n6-Point and 2x6-Point Moving Averages: Similarly, they provide insights into different aspects of the data’s patterns.\n\n\n\nCode\n# Calculate the 6-point moving average and 2x6-point moving average\nma6 &lt;- ma(inflation_monthly_ts_2, order=6, centre=FALSE)\nma2x6 &lt;- ma(inflation_monthly_ts_2, order=6, centre=TRUE)\nMA_2x6 = data.frame(inflation_monthly_ts_2, ma6, ma2x6)\n\n# Plot\nautoplot(inflation_monthly_ts_2, series=\"Data\") +\n  autolayer(ma(inflation_monthly_ts_2, order=6, centre=FALSE), series=\"6-MA\") +\n  autolayer(ma(inflation_monthly_ts_2, order=6, centre=TRUE), series=\"2x6-MA\") +\n  xlab(\"Year\") + ylab(\"Inflation Rate (%)\") +\n  ggtitle(\"Monthly Inflation Rate Over Time with 6-Point and 2x6-Point MA\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\"6-MA\"=\"red\",\"2x6-MA\"=\"blue\"),\n                      breaks=c(\"Data\",\"6-MA\",\"2x6-MA\"))+\n   theme_minimal() +  # This theme provides a minimal styling\n  theme(\n    #panel.background = element_blank(),  # Remove background\n    panel.grid.major = element_blank(),   # Remove major grid\n    #panel.grid.minor = element_blank(),   # Remove minor grid\n    strip.background = element_blank()    # Remove background for facet labels (if any)\n  )\n\n\n\n\n\nKey Insights:\n\nExtended Smoothing with 6-MA: A 6-month centered moving average (6-MA) extends the smoothing window, allowing for the minimization of shorter-term fluctuations. This broader window can be particularly effective in teasing out more pronounced seasonality or semi-annual trends inherent in the inflation data.\nCentered Nature of 6-MA: Given that the moving average is centered, it means that the average value at a given time point is computed by considering an equal number of months before and after that point. This symmetry ensures that the smoothed value aligns well with the central month, offering a more accurate representation of trends around that time.\nComparison with Previous Methods: Compared to the previously employed 4-MA and 2x4-MA methods, the 6-MA provides a balance. It offers a more extended smoothing than the 4-MA, thereby capturing broader trends. Yet, it’s not as granular as the 2x4-MA, making it a potential middle ground that retains some shorter-term nuances while highlighting more overarching trends.\nResidual Patterns: While the 6-MA will likely remove many of the short-term fluctuations seen in the monthly inflation data, it’s essential to evaluate how well it adjusts for persisting cyclic patterns or any remaining seasonality. There might still be traces of longer-term trends or patterns that even the 6-MA might not entirely iron out.\nPractical Implications: Adopting a 6-MA could be particularly beneficial if analysts aim to understand semi-annual inflation behaviors or want to derive insights relevant for bi-annual strategic planning. However, for capturing more granular monthly changes, this method might smooth out too much detail.\n\nConclusion: Using a 6-month centered moving average to smooth out the inflation data allows for a more holistic view of semi-annual trends in the economic landscape. This extended smoothing can provide valuable insights, especially when the objective is to uncover broader patterns rather than monthly fluctuations."
  },
  {
    "objectID": "exploratory-data-analysis.html#moving-average-smoothing-with-windowing-2x8",
    "href": "exploratory-data-analysis.html#moving-average-smoothing-with-windowing-2x8",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing with Windowing (2x8)",
    "text": "Moving Average Smoothing with Windowing (2x8)\n\n8-Point and 2x8-Point Moving Averages: Similarly, they provide insights into different aspects of the data’s patterns.\n\n\n\nCode\n# Calculate the 8-point moving average and 2x8-point moving average\nma8 &lt;- ma(inflation_monthly_ts_2, order=8, centre=FALSE)\nma2x8 &lt;- ma(inflation_monthly_ts_2, order=8, centre=TRUE)\nMA_2x8 = data.frame(inflation_monthly_ts_2, ma8, ma2x8)\n\n# Plot\nautoplot(inflation_monthly_ts_2, series=\"Data\") +\n  autolayer(ma(inflation_monthly_ts_2, order=8, centre=FALSE), series=\"8-MA\") +\n  autolayer(ma(inflation_monthly_ts_2, order=8, centre=TRUE), series=\"2x8-MA\") +\n  xlab(\"Year\") + ylab(\"Inflation Rate (%)\") +\n  ggtitle(\"Monthly Inflation Rate Over Time with 8-Point and 2x8-Point MA\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\"8-MA\"=\"red\",\"2x8-MA\"=\"blue\"),\n                      breaks=c(\"Data\",\"8-MA\",\"2x8-MA\"))+\n   theme_minimal() +  # This theme provides a minimal styling\n  theme(\n    #panel.background = element_blank(),  # Remove background\n    panel.grid.major = element_blank(),   # Remove major grid\n    #panel.grid.minor = element_blank(),   # Remove minor grid\n    strip.background = element_blank()    # Remove background for facet labels (if any)\n  )\n\n\n\n\n\nKey Insights:\n\nBenefits of Extended Smoothing: The employment of a centered 8-month moving average is a step further into the domain of smoothing. By increasing the moving average window, you’re essentially amplifying the degree of smoothing, which can be beneficial to more clearly highlight longer-term trends in the data.\nComparison to Previous Methods: When contrasted with the prior 4-MA and 2x4-MA approaches, the 8-month moving average produces a smoother curve. This curve will be less sensitive to monthly fluctuations and will more distinctly reveal broader, overarching patterns. The longer window diminishes the influence of short-term fluctuations, seasonal or otherwise, to provide a clearer picture of long-term tendencies.\nImplications of Centering: Opting for a centered moving average, especially with an 8-month window, ensures that the smoothed value is symmetrically aligned with the original data points in time. This alignment aids in preserving the chronological essence of the time series, preventing any potential misleading lags that could be introduced with trailing moving averages.\nSeeking Optimal Smoothing: Your pursuit of the centered 8-month moving average stems from an observation that the prior techniques, while effective, still left some room for capturing longer-term dynamics better. The inflation data, being multifaceted in nature, demands such iterative refinement in its representation.\nResidual Patterns: As with any smoothing technique, it’s pivotal to assess the residuals or what’s left after the smoothing. If the 8-month moving average does a commendable job, it will leave behind noise with minimal discernible patterns. However, any remaining systematic pattern might indicate a need for further modeling or alternative techniques.\n\nConclusion: The use of a centered 8-month moving average on the monthly inflation data signifies an analytical dedication to uncovering deeper, more subtle trends beneath the monthly variations. While the prior methods provided valuable insights into the series’ structure, the adoption of this extended smoothing technique aims to offer an even clearer lens into the long-term trajectories of inflation. This continued exploration showcases the dynamic nature of inflation trends and the necessity of iterative, refined techniques to genuinely grasp its intricacies.\nExploring the inflation rates through various visualizations and analyses equips us with a deeper understanding of its patterns, trends, and underlying components. The tools and techniques applied here can also be extended to other time series datasets for a comprehensive analysis."
  },
  {
    "objectID": "arma-arima-sarima-models.html#summary",
    "href": "arma-arima-sarima-models.html#summary",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Forecasting Inflation with ARMA/ARIMA/SARIMA Models\nInflation forecasting is of paramount importance for policymakers, investors, and consumers alike. Accurate predictions can aid in monetary policy decisions, investment strategies, and household budgeting. To forecast inflation using time series data, several models can be employed, notably ARMA, ARIMA, and SARIMA models.\n\nAutoregressive (AR) and Moving Average (MA) Models:\n\nAR: This model predicts future inflation based on its own past values. It assumes that the inflation rate is a linear function of its previous values.\nMA: Contrary to AR, the MA model predicts inflation based on past white noise or error terms. This captures the shock effects observed in the past.\n\nAutoregressive Moving Average (ARMA) Model:\n\nCombines both AR and MA components. Suitable for time series data that exhibit patterns not captured by AR or MA models alone.\n\nAutoregressive Integrated Moving Average (ARIMA) Model:\n\nAn extension of the ARMA model that includes “integration” (I). This represents the number of differences needed to make the time series stationary.\nEspecially pertinent for inflation data, which may have trends or cycles that render the data non-stationary.\n\nSeasonal Autoregressive Integrated Moving Average (SARIMA) Model:\n\nExtends ARIMA by accounting for seasonality, which can be pivotal for inflation data affected by seasonal factors (e.g., holiday-driven consumer spending or agricultural harvest cycles).\n\n\nChecking Stationarity: For accurate forecasting, it’s vital that the inflation data is stationary, meaning its statistical properties like mean and variance remain constant over time.\n\nUse the Autocorrelation Function (ACF) plot to test for stationarity. If significant correlations persist over several lags, the data may be non-stationary.\nDifferencing the data, often first or even second differences, can help in achieving stationarity by eliminating trends or cyclical patterns.\n\nIn our EDA section, an ACF plot was generated to examine stationarity in the inflation data. After ensuring stationarity, either naturally or through transformations, the ACF and Partial Autocorrelation Function (PACF) plots of the stationary data are pivotal in determining the optimal parameters for our ARIMA or SARIMA models."
  },
  {
    "objectID": "arma-arima-sarima-models.html#arima-modelling-for-inflation",
    "href": "arma-arima-sarima-models.html#arima-modelling-for-inflation",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Arima Modelling for Inflation",
    "text": "Arima Modelling for Inflation\n\nSplitting the data into Train and Test for Model Validation Process\nUpon completing the cleaning and aggregation process of the U.S. Monthly Inflation Data, we are now poised to split this time series dataset into distinct training and testing subsets. This segregation is vital to ensure our forecasting model is both trained on a comprehensive data set and validated against a segment of data it has not previously encountered.\nThe data spans from 1913 to 2023 for every month. We split the data into a 70% train and 30% test.This allocation strategy will allow us to forecast and validate our model’s performance on inflation trends spanning over years.\n\n\nCode\ntrain_length &lt;- floor(0.9 * length(inflation_monthly))\ntrain_series &lt;- inflation_monthly[1:train_length]\ntest_series &lt;- inflation_monthly[(train_length + 1):length(inflation_monthly)]\n\n\n\n\nACF and PACF Plots for Monthly Inflation\n\n\nCode\ntrain_series %&gt;% \n  ggtsdisplay(main=\"ACF and PACF Plots of Monthly Inflation\")\n\n\n\n\n\n\n\nADF Test for Monthly Inflation\nH0: The time series is non-stationary. In other words, it has some time-dependent structure and does not have constant variance over time.\nv/s\nH1: The time series is stationary. In other words, it has no time-dependent structure and does have constant variance over time.\n\n\nCode\nadf.test(train_series)\n\n\nWarning in adf.test(train_series): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  train_series\nDickey-Fuller = -5.5685, Lag order = 10, p-value = 0.01\nalternative hypothesis: stationary\n\n\nBecause the p-value from the ADF test is less than α = 0.05, we reject the null hypothesis and conclude that the monthly inflation series is stationary.Although the ADF states that the original series is stationary, the ACF plots, which clearly indicate seasonality and trend, are more reliable than the ADF test. Therefore, it is safe to conclude that the series non-stationary as per the ACF section above.\n\n\nLog Transformation of Monthly Inflation and its first and second order diferrencing\n\n\nCode\nlx = log(train_series + abs(min(train_series)) + 0.01) # since inflation has negative values\ndlx = diff(lx)\nddlx = diff(dlx, 12) \n\nx = train_series\n\n# Plot original series\nplot.ts(x, main=\"Original Series\")\n\n\n\n\n\nCode\n# Plot log-transformed series\nplot.ts(lx, main=\"Log-transformed Series\")\n\n\n\n\n\nCode\n# Plot first difference of log-transformed series\nplot.ts(dlx, main=\"First Difference of Log-transformed Series\")\n\n\n\n\n\nCode\n# Plot second difference of first difference\nplot.ts(ddlx, main=\"Second Difference of First Difference\")\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,1))\nmonthplot(dlx); monthplot(ddlx)\n\n\n\n\n\nSimply taking log of the number of monthly infaltion does not make it stationary. First-differencing the log number of monthly inflation does, however, make the series stationary and this series should be employed for building our time series model. Keep in mind that because first-differencing was enough to make the series stationary, we do not need to second-difference it, helping us avoid over differencing the number of monthly inflation\n\n\nADF Test for Log First Order Differencing Monthly Inflation\nH0: The time series is non-stationary. In other words, it has some time-dependent structure and does not have constant variance over time.\nv/s\nH1: The time series is stationary. In other words, it has no time-dependent structure and does have constant variance over time.\n\n\nCode\nadf.test(dlx)\n\n\nWarning in adf.test(dlx): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dlx\nDickey-Fuller = -16.463, Lag order = 10, p-value = 0.01\nalternative hypothesis: stationary\n\n\nBecause the p-value from the ADF test is less than α = 0.05, we reject the null hypothesis and conclude that the log first-differenced monthly inflation series is stationary. Let us now check whether the ACF plots supports this hypothesis.\n\n\nACF and PACF Plots of Log First-Differenced Monthly Inflation\n\n\nCode\ndlx %&gt;% \n  ggtsdisplay(main=\"ACF and PACF Plots of Log First-Differenced Monthly Inflation\")\n\n\n\n\n\np values obtained from PACF are 0, 1, 2, 3, 4 q values obtained from ACF are: 0, 1 d (Difference): 1\n\n\nFitting ARIMA (P,D,Q)\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*24),nrow=24) # roughly nrow = 3x4x2\n\n# p=0,1,2,3,4 \n# q=0,1,2,3 (although we only found q=1 to be significant in ACF, we may want to compare a complex ARIMA model with greater \"q\" value compared to a simpler ARIMA model)\n# d=1\n\n  {\nfor (p in 1:5)\n{\n  for(q in 1:4)\n    for(d in 1)\n    {\n      \n      if(p-1+d+q-1&lt;=8)\n      {\n        \n        model&lt;- Arima(lx,order=c(p-1,d,q-1)) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp &lt;- temp[order(temp$BIC, decreasing = FALSE),] \nknitr::kable(temp)\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n7\n1\n1\n2\n-46.40664\n-26.0630351\n-46.37303\n\n\n11\n2\n1\n2\n-45.49580\n-20.0662959\n-45.44534\n\n\n8\n1\n1\n3\n-45.37218\n-19.9426758\n-45.32172\n\n\n15\n3\n1\n2\n-43.56630\n-13.0508907\n-43.49559\n\n\n12\n2\n1\n3\n-42.64466\n-12.1292475\n-42.57395\n\n\n19\n4\n1\n2\n-41.40796\n-5.8066500\n-41.31360\n\n\n18\n4\n1\n1\n-34.62703\n-4.1116249\n-34.55633\n\n\n6\n1\n1\n1\n-18.32448\n-3.0667737\n-18.30433\n\n\n14\n3\n1\n1\n-27.86882\n-2.4393084\n-27.81835\n\n\n3\n0\n1\n2\n-16.92151\n-1.6638090\n-16.90136\n\n\n20\n4\n1\n3\n-41.36933\n-0.6821168\n-41.24791\n\n\n10\n2\n1\n1\n-21.00188\n-0.6582723\n-20.96826\n\n\n2\n0\n1\n1\n-10.53575\n-0.3639459\n-10.52568\n\n\n4\n0\n1\n3\n-18.58800\n1.7556010\n-18.55439\n\n\n16\n3\n1\n3\n-23.97866\n11.6226453\n-23.88431\n\n\n17\n4\n1\n0\n52.89847\n78.3279760\n52.94893\n\n\n13\n3\n1\n0\n72.04419\n92.3877970\n72.07780\n\n\n9\n2\n1\n0\n122.29070\n137.5484074\n122.31085\n\n\n5\n1\n1\n0\n218.52444\n228.6962410\n218.53451\n\n\n1\n0\n1\n0\n493.69542\n498.7813227\n493.69877\n\n\n21\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n22\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n23\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n24\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nBest Model in terms of AIC:\n\n\nCode\ntemp[which.min(temp$AIC),] \n\n\n  p d q       AIC       BIC      AICc\n7 1 1 2 -46.40664 -26.06304 -46.37303\n\n\nBest Model in terms of AICc:\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q       AIC       BIC      AICc\n7 1 1 2 -46.40664 -26.06304 -46.37303\n\n\nBest Model in terms of BIC:\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q       AIC       BIC      AICc\n7 1 1 2 -46.40664 -26.06304 -46.37303\n\n\nModel summary and error metrics of ARIMA(2, 1, 1):\n\n\nCode\nfit &lt;- Arima(lx, order=c(1,1,2)) # no drift included \nsummary(fit)\n\n\nSeries: lx \nARIMA(1,1,2) \n\nCoefficients:\n         ar1      ma1     ma2\n      0.8797  -1.6559  0.6576\ns.e.  0.0462   0.0687  0.0664\n\nsigma^2 = 0.05595:  log likelihood = 27.2\nAIC=-46.41   AICc=-46.37   BIC=-26.06\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.002588435 0.2361338 0.1155882 0.6293244 14.72852 0.8532608\n                   ACF1\nTraining set 0.01477834\n\n\nThe best model with the lowest BIC metric is the ARIMA(1,1,2).\n\nAR(2): This indicates an autoregressive term of order 1. It means the model uses the two most recent values (lags) of the time series in the prediction equation. The AR(1) component suggests that there’s a relationship between an observation and the one preceding observations.\nI(1): This represents the integrated component of order 1. It indicates that the time series has been differenced once to make it stationary. Differencing is the process of subtracting the current value from the previous value. In the context of inflation or economic data, this can be thought of as the change in value from one period to the next.\nMA(2): This is a moving average term of order 2. The MA component accounts for the relationship between an observation and a residual error from an MA process applied to lagged observations. In simpler terms, it’s a way to model the error of the prediction as a linear combination of error terms from the recent past.\n\nThe ARIMA(1,1,2) model can be represented by the following equation:\n\\[\n[(1 - 0.885B)(1 - B)X_t = (1 + 1.6633B - 0.6648B^2)Z_t]\n\\]\nWhere:\n- \\(( B )\\) is the backshift operator, which represents a lag of 1 period. \\(( B^2 )\\) would represent a lag of 2 periods, and so on.\n- \\(( X_t )\\) is the time series value at time \\(( t )\\).\n- \\(( Z_t )\\) is the white noise error term at time \\(( t )\\).\n- The coefficients 0.8931, 1.6770, and -0.6786 are the estimated parameters for the AR(1), MA(1), and MA(2) terms, respectively, based on the provided output.\n- The term \\(( (1 - B) )\\) represents the differencing of order 1.\n\n\nModel Diagnostics of ARIMA(1,1,2)\n\n\nCode\nmodel_output &lt;- capture.output(sarima(lx, 1,1,2))\n\n\n\n\n\nCode\ncat(model_output[71:102], model_output[length(model_output)], sep = \"\\n\") \n\n\n\nCoefficients:\n         ar1      ma1     ma2  constant\n      0.8922  -1.6761  0.6761     1e-04\ns.e.  0.0242   0.0409  0.0408     1e-04\n\nsigma^2 estimated as 0.05564:  log likelihood = 27.85,  aic = -45.69\n\n$degrees_of_freedom\n[1] 1191\n\n$ttable\n         Estimate     SE  t.value p.value\nar1        0.8922 0.0242  36.9201  0.0000\nma1       -1.6761 0.0409 -40.9654  0.0000\nma2        0.6761 0.0408  16.5827  0.0000\nconstant   0.0001 0.0001   1.4324  0.1523\n\n$AIC\n[1] -0.03823631\n\n$AICc\n[1] -0.03820818\n\n$BIC\n[1] -0.01695639\n\nNA\nNA\nNA\nNA\nNA\n\n\nStandardized Residuals: Essentially stating that if the errors are white noise. The model does look stationary as it captures all the signals and essentially captures the raw white noise.\nACF Of Residuals: Auto-correlation of the residuals. The only q value to inspect is 1.\nQ-Q Plot: The series follows a normal distribution pretty closely as even the tails seem to be on the normal line.\np values of the Ljung-Box statistic: Ideally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model. Since the lag value less than 10 have a p-value greater than 0.05, the residuals have no autocorrelations.\nLet’s see what model is outputted by auto.arima().\n\n\nModel Output of Log Monthly Inflation with auto.arima()\n\n\nCode\nfit = auto.arima(lx, seasonal = FALSE)\nsummary(fit)\n\n\nSeries: lx \nARIMA(2,1,1) \n\nCoefficients:\n         ar1     ar2      ma1\n      0.1676  0.0859  -0.9026\ns.e.  0.0442  0.0408   0.0317\n\nsigma^2 = 0.05723:  log likelihood = 14.5\nAIC=-21   AICc=-20.97   BIC=-0.66\n\nTraining set error measures:\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.0003750843 0.2388288 0.1160352 1.793008 15.99104 0.8565603\n                     ACF1\nTraining set -0.004293837\n\n\nFrom the above output, auto.arima() too outputted an ARIMA(2,1,1) model.Some points to keep in mind when using these functions is as follows:\nThe auto.arima() function in R uses a stepwise algorithm to search through the space of possible ARIMA models and select the one with the lowest AIC value. While this approach can be computationally efficient and provide a good starting point for model selection, it does not necessarily always find the best possible model for a given time series.\nOn the other hand, the Arima() function in R allows us to specify the exact order of the ARIMA model and can be used to fit more complex models, such as those with seasonality, exogenous variables, or other constraints. By specifying the exact order of the model, we have more control over the modeling process and can potentially obtain a better fit to the data.\nIn summary, the auto.arima() function can be a useful tool for quickly identifying a potentially good model, but it is not a substitute for careful model selection and customization seen when using the Arima() function.\n\nThe ARIMA(1,1,2) model outperforms the ARIMA(2,1,1) model for the lx series, as evidenced by its lower AIC, BIC, and log likelihood values, indicating a better balance between model fit and complexity. The ARIMA(1,1,2) model’s significant autoregressive term suggests the data has inherent sequential dependencies, which this model captures more effectively than the solely moving average-based ARIMA(2,1,1). While both models have similar error metrics, the simpler ARIMA(1,1,2) provides a more parsimonious and better-fitting representation of the underlying data dynamics.\n\n\n\nForecasting ARIMA(1,1,2) and ARIMA(2,1,1)\n\n\nCode\narimaModel_1 &lt;- arima(lx, order = c(1,1,2))\narimaModel_2 &lt;- arima(lx, order = c(2,1,1))\n\nforecast1=predict(arimaModel_1, length(test_series))\nforecast2=predict(arimaModel_2, length(test_series))\n\n# Convert the time series and forecast objects to data frames\nts_df &lt;- data.frame(date = time(inflation_monthly), value = as.numeric(inflation_monthly))\ntrain_df &lt;- data.frame(date = time(inflation_monthly)[1:train_length], value = as.numeric(lx))\nforecast1_df &lt;- data.frame(date = time(inflation_monthly)[(train_length + 1):length(inflation_monthly)], value = forecast1$pred)\nforecast2_df &lt;- data.frame(date = time(inflation_monthly)[(train_length + 1):length(inflation_monthly)], value = forecast2$pred)\n\n# Plot the time series and forecasts\nggplotly(ggplot() +\n    geom_line(data = train_df, aes(x = date, y = value, \n              color = \"Actual Train Values\"), linetype = \"solid\", alpha=0.6, show.legend = TRUE) +\n    geom_line(data = forecast1_df, aes(x = date, y = value, \n                                       color = \"ARIMA(1,1,2) Forecast\"), linetype = \"solid\", show.legend = TRUE) +\n    geom_line(data = forecast2_df, aes(x = date, y = value, \n                                       color = \"ARIMA(2,1,1) Forecast\"), linetype = \"solid\", show.legend = TRUE) +\n    geom_line(data = ts_df[(train_length + 1):length(inflation_monthly),], aes(x = date, y = log(value), \n                                       color = \"Actual Forecast Values\"), linetype = \"solid\", show.legend = TRUE) +\n    labs(x = \"Date\", y = \"Log of Number of Monthly Inflation\", title = \"Forecasting ARIMA(1,1,2) and ARIMA(2,1,1)\") +\n    theme_minimal() +\n    scale_color_manual(name = \"Forecast\", \n                       values = c(\"ARIMA(1,1,2) Forecast\" = \"blue\", \n                                  \"ARIMA(2,1,1) Forecast\" = \"green\",\n                                   \"Actual Forecast Values\" = \"orange\",\n                                   \"Actual Train Values\" = \"black\"),\n                       labels = c(\"ARIMA(1,1,2) Forecast\", \n                                  \"ARIMA(2,1,1) Forecast\",\n                                  \"Actual Forecast Values\",\n                                  \"Actual Train Values\")))\n\n\n\n\n\n\nFrom the above graph, we can note that the forecasted monthly inflations remains constant at around 1 for both models on the test set. This performance is not what was expected and, hence, it is possible that the models are not able to capture the underlying patterns in the data. This can be due to a variety of reasons, such as insufficient data and the models not being complex enough to capture the variation in the data. It is, however, pragmatic to check whether the sarima.for() function’s predictions may forecast differently. Let us find out below.\n\n\nForecasting Arima (1,1,2) using Sarima.for()\n\n\nCode\nlog_monthly_inflation &lt;- ts(lx, start = c(1913, 1), frequency = 12) \nsarima.for(ts(train_df$value, start = c(1913, 1), frequency = 12), 24, p = 1, d = 1, q = 2, main = \"Forecasting ARIMA(1,1,2) using sarima.for()\") \n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2012                                                                        \n2013 1.266403 1.268101 1.269626 1.270997 1.272230 1.273340 1.274341 1.275243\n2014 1.278622 1.279123 1.279581 1.279999 1.280382 1.280734 1.281058 1.281358\n          Sep      Oct      Nov      Dec\n2012 1.257422 1.260048 1.262401 1.264511\n2013 1.276059 1.276796 1.277464 1.278071\n2014                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2012                                                                      \n2013 0.2517885 0.2538952 0.2555667 0.2568957 0.2579543 0.2587988 0.2594735\n2014 0.2614751 0.2616205 0.2617380 0.2618330 0.2619101 0.2619726 0.2620235\n           Aug       Sep       Oct       Nov       Dec\n2012           0.2359764 0.2414628 0.2457539 0.2491269\n2013 0.2600132 0.2604456 0.2607923 0.2610708 0.2612947\n2014 0.2620650                                        \n\n\n\n\nComparing Arima(1,1,2) with Benchmarks\n\n\nCode\nautoplot(log_monthly_inflation) +\n  autolayer(meanf(log_monthly_inflation, h=24),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(log_monthly_inflation, h=24),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(log_monthly_inflation, h=24),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(log_monthly_inflation, h=24, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(Arima(log_monthly_inflation, order=c(1,1,2)), 24), \n            series=\"ARIMA(1,1,2)\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\")) +\n  ylab(\"Log of Monthly inflation\") + ggtitle(\"Forecasting ARIMA(1,1,2) and Benchmarks\") + theme_minimal()\n\n\n\n\n\nArima(1,1,2) model metrics:\n\n\nCode\nfit &lt;- Arima(log_monthly_inflation, order=c(1,1,2))\nsummary(fit)\n\n\nSeries: log_monthly_inflation \nARIMA(1,1,2) \n\nCoefficients:\n         ar1      ma1     ma2\n      0.8797  -1.6559  0.6576\ns.e.  0.0462   0.0687  0.0664\n\nsigma^2 = 0.05595:  log likelihood = 27.2\nAIC=-46.41   AICc=-46.37   BIC=-26.06\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.002588435 0.2361338 0.1155882 0.6293244 14.72852 0.7438047\n                   ACF1\nTraining set 0.01477834\n\n\nMean metrics:\n\n\nCode\nf1 &lt;- meanf(log_monthly_inflation, h=24) \n\ncheckresiduals(f1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 815.6, df = 24, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 24\n\n\nCode\naccuracy(f1)\n\n\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 2.290299e-16 0.2628595 0.1357713 3.834076 21.53009 0.8736818\n                 ACF1\nTraining set 0.361126\n\n\nSnaive metrics:\n\n\nCode\nf2 &lt;- snaive(log_monthly_inflation, h=24) \n\ncheckresiduals(f2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 579.05, df = 24, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 24\n\n\nCode\naccuracy(f2)\n\n\n                       ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set 0.0001674226 0.3444004 0.1554013 5.803652 24.34328    1 0.2192816\n\n\nRandom Walk metrics:\n\n\nCode\nf3 &lt;- rwf(log_monthly_inflation, h=24) \n\ncheckresiduals(f3)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Random walk\nQ* = 335.5, df = 24, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 24\n\n\nCode\naccuracy(f3)\n\n\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.0001084129 0.2972421 0.1354665 3.373002 17.91581 0.8717203\n                   ACF1\nTraining set -0.4551488\n\n\nThe ARIMA(1,1,2) model seems to be the best performing among the four models considered. It has a MASE value less than 1, suggesting better performance than a naive model, and its residuals show low autocorrelation, indicating that the model has effectively captured the underlying patterns in the data. In contrast, the other models, especially the Mean and Seasonal Naive models, have not performed as effectively, with their residuals showing higher autocorrelation. The Random Walk model’s negative ACF1 suggests possible over-differencing. Therefore, for future forecasts and analyses on this dataset, the ARIMA(1,1,2) model would be the recommended choice. The performance of the ARIMA(1,1,2) model versus the other models can be attributed to the inherent nature of the models and the underlying patterns in the data:\n\nNature of ARIMA:\n\nARIMA (AutoRegressive Integrated Moving Average) models combine autoregressive (AR) and moving average (MA) elements, allowing them to capture both autocorrelations and moving average patterns in the data. The (1,1,2) order suggests that the model uses one lag of the autoregressive term, one order of differencing, and two lags of the moving average term.\n\nSimplicity of Other Models:\n\nThe Mean model uses a simple average of all values, which won’t capture any intricate patterns in a time series beyond its central tendency.\nThe Seasonal Naive model assumes a repetitive pattern after a fixed number of periods. If the data doesn’t have a consistent seasonality or has other complex trends, this model won’t capture them effectively.\nThe Random Walk model is based on the premise that future values are the same as the last observed value. This model is too simplistic for series with trends or seasonality, as it doesn’t anticipate any change in direction.\n\nDynamics of the Dataset:\n\nThe errors (residuals) of a good model should be random without any discernable pattern. The ARIMA(1,1,2) model had the lowest autocorrelation in residuals among all the models, indicating that it has effectively captured most of the structure from the data.\n\nModel Diagnostics:\n\nThe metrics provided (AIC, BIC, MASE, ACF1) offer different perspectives on model performance. AIC and BIC values give information about the goodness of fit of the model, penalizing for complexity. MASE is a scale-free error metric that compares the given model to a naive benchmark, and ACF1 checks for any remaining autocorrelation in the residuals."
  },
  {
    "objectID": "arimax-sarimax-var.html#summary",
    "href": "arimax-sarimax-var.html#summary",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "In the previous modeling sections, we analyzed a univariate time series of monthly inflation rates in the United States, calculated from the Consumer Price Index (CPI) data spanning several decades. While ARIMA and SARIMA models provided insights into the inflation trends and seasonal patterns, we aim to enhance our understanding by incorporating endogenous variables into our analysis. Endogenous variables in the context of inflation might include factors like Disposable Income, unemployment rates, Personal Consumption, and major economic policies or events. These variables are determined within the economic system and are influenced by other variables in the system, often demonstrating interdependence and reacting to changes in other economic indicators."
  },
  {
    "objectID": "arimax-sarimax-var.html#literature-review",
    "href": "arimax-sarimax-var.html#literature-review",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Literature Review",
    "text": "Literature Review\nNumerous studies have explored the relationship between macroeconomic indicators and inflation. A study by Sims (1980) introduced the Vector Autoregression (VAR) approach, which has been widely used in economic forecasting and policy analysis. This approach allows for the modeling of the dynamic interaction among multiple macroeconomic variables. Bernanke et al. (1998) further refined the VAR approach by incorporating structural analysis, providing deeper insights into how monetary policy affects the economy, including inflation.\nResearch by Stock and Watson (1999) demonstrated the predictive power of incorporating multiple macroeconomic variables in forecasting inflation, showing that models with more variables tend to outperform simpler univariate models. This highlights the importance of considering a broad range of economic indicators to understand inflation dynamics more comprehensively."
  },
  {
    "objectID": "arimax-sarimax-var.html#var-model-justification",
    "href": "arimax-sarimax-var.html#var-model-justification",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "VAR Model Justification",
    "text": "VAR Model Justification\nA VAR model is preferred over simpler ARIMAX models in this context for several reasons:\nSimultaneous Modeling of Multiple Time Series: VAR models can simultaneously account for multiple interdependent economic indicators, capturing the complex relationships between them.\nHandling Lags Efficiently: VAR models can effectively handle multiple lags in economic data, which is crucial in understanding the delayed effects of economic policies and events on inflation.\nRobustness to Missing Data: Given the historical nature of economic data, VAR models can handle instances of missing data more efficiently than ARIMAX models.\nDynamic Relationships: VAR models are well-suited to capture the evolving relationships between economic variables over time, which is essential in understanding how various factors influence inflation."
  },
  {
    "objectID": "arimax-sarimax-var.html#methodology",
    "href": "arimax-sarimax-var.html#methodology",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Methodology",
    "text": "Methodology\nOur analysis will be divided into two parts, using both VAR and ARIMAX models. The first part will forecast monthly inflation rates using historical CPI data, considering various macroeconomic variables as potential predictors. The second part will focus on a more recent period to capture the effects of contemporary economic policies and global events on inflation.\nWe will also differentiate between VAR and ARIMAX models based on their capacity to handle multiple variables. While the VAR models will consider a broad range of economic indicators, the ARIMAX models will focus on specific variables identified as key influencers of inflation, such as interest rates and unemployment rates.\nThis approach will enable us to determine the relative impact of different economic factors on inflation and improve our ability to forecast future inflation trends based on current economic conditions."
  },
  {
    "objectID": "arimax-sarimax-var.html#time-series-plot",
    "href": "arimax-sarimax-var.html#time-series-plot",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Time Series Plot",
    "text": "Time Series Plot\n\nplot.ts(var_ts1 , main = \"\", xlab = \"\")"
  },
  {
    "objectID": "arimax-sarimax-var.html#pair-plot",
    "href": "arimax-sarimax-var.html#pair-plot",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Pair Plot",
    "text": "Pair Plot\n\n# create scatterplot matrix using plotly\nfig &lt;- plot_ly(\n  data = as.data.frame(var_ts1),\n  type = \"splom\",\n  diagonal = list(visible = FALSE),\n  dimensions = list(\n    list(label = \"CPI\", values = ~CPIAUCSL),\n    list(label = \"Unemployment\", values = ~UNRATE),\n    list(label = \"DisposableIncome\", values = ~DSPIC96),\n    list(label = \"PersonalConsumption\", values = ~PCE),\n    list(label = \"TreasuryYield\", values = ~GS10),\n    list(label = \"FederalFundsRate\", values = ~FEDFUNDS)\n  )\n) %&gt;%\n  layout(hovermode = \"x\")\n\n\n\n# customize layout\nfig &lt;- fig %&gt;% \n  layout(\n    title = \"Scatterplot Matrix of VAR Model Variables\",\n    xaxis = list(title = \"\"),\n    yaxis = list(title = \"\")\n  )\n\n# display plot\nfig\n\n\n\n\n\n\n# convert the matrix to a time series object with a yearly frequency\nvar_ts1 &lt;- ts(var_ts1, frequency = 1,\n                 start = 1970)\n\n# split into train and test sets\n\nset.seed(29830)\ntrain_idx &lt;- sample(nrow(var_ts1), 0.9 * nrow(var_ts1))\ntrain &lt;- var_ts1[train_idx, ]\ntest &lt;- var_ts1[-train_idx, ]"
  },
  {
    "objectID": "arimax-sarimax-var.html#fitting-the-var-model",
    "href": "arimax-sarimax-var.html#fitting-the-var-model",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting the VAR Model",
    "text": "Fitting the VAR Model\nHere we use the VARselect() function to find the best p to fit VAR(p). We will choose a maximum lag of 10 and check which p value returns lowest AIC.\n\n(var_result &lt;- VARselect(var_ts1, lag.max = 10, type = \"both\"))\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     5      3      2      5 \n\n$criteria\n                1          2          3          4          5          6\nAIC(n)   5.750435   5.112869   5.006714   4.957243   4.953009   4.981328\nHQ(n)    5.889067   5.355475   5.353294   5.407796   5.507537   5.639829\nSC(n)    6.106319   5.735665   5.896423   6.113864   6.376543   6.671775\nFPE(n) 314.330622 166.155229 149.436154 142.249649 141.690540 145.823030\n                7          8          9         10\nAIC(n)   4.976799   5.018745   5.021673   5.058962\nHQ(n)    5.739275   5.885194   5.992096   6.133359\nSC(n)    6.934159   7.243017   7.512858   7.817059\nFPE(n) 145.249853 151.589710 152.184594 158.160557\n\n\nNow, we will fit VAR(1), VAR(2), and VAR(3):\nVAR(1) output:\n\nsummary(fit &lt;- VAR(var_ts1, p=1, type=\"both\"))\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: CPIAUCSL, UNRATE, DSPIC96, PCE, GS10, FEDFUNDS \nDeterministic variables: both \nSample size: 600 \nLog Likelihood: -6769.181 \nRoots of the characteristic polynomial:\n0.9993 0.9868 0.9823 0.9823 0.9081 0.9081\nCall:\nVAR(y = var_ts1, p = 1, type = \"both\")\n\n\nEstimation results for equation CPIAUCSL: \n========================================= \nCPIAUCSL = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  9.808e-01  6.108e-03 160.576  &lt; 2e-16 ***\nUNRATE.l1    2.348e-02  1.366e-02   1.719  0.08621 .  \nDSPIC96.l1   1.807e-04  1.080e-04   1.673  0.09480 .  \nPCE.l1      -1.330e-04  5.939e-05  -2.240  0.02546 *  \nGS10.l1     -2.084e-02  2.026e-02  -1.028  0.30417    \nFEDFUNDS.l1  5.960e-02  1.268e-02   4.700 3.24e-06 ***\nconst       -2.246e-01  4.949e-01  -0.454  0.65011    \ntrend        7.612e-03  2.933e-03   2.595  0.00968 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.4126 on 592 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 2.193e+06 on 7 and 592 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation UNRATE: \n======================================= \nUNRATE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  1.187e-02  2.509e-03   4.730 2.81e-06 ***\nUNRATE.l1    1.002e+00  5.612e-03 178.637  &lt; 2e-16 ***\nDSPIC96.l1   1.674e-04  4.435e-05   3.775 0.000176 ***\nPCE.l1      -6.604e-05  2.440e-05  -2.707 0.006986 ** \nGS10.l1     -2.182e-02  8.322e-03  -2.621 0.008983 ** \nFEDFUNDS.l1  2.023e-02  5.209e-03   3.884 0.000114 ***\nconst       -9.091e-01  2.033e-01  -4.472 9.29e-06 ***\ntrend       -6.327e-03  1.205e-03  -5.252 2.11e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1695 on 592 degrees of freedom\nMultiple R-Squared: 0.9888, Adjusted R-squared: 0.9887 \nF-statistic:  7487 on 7 and 592 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation DSPIC96: \n======================================== \nDSPIC96 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  -1.863527   1.011445  -1.842  0.06591 .  \nUNRATE.l1    -7.201908   2.262177  -3.184  0.00153 ** \nDSPIC96.l1    0.895838   0.017879  50.106  &lt; 2e-16 ***\nPCE.l1        0.054053   0.009834   5.496 5.77e-08 ***\nGS10.l1       3.917953   3.354882   1.168  0.24334    \nFEDFUNDS.l1  -3.606037   2.099888  -1.717  0.08646 .  \nconst       474.101366  81.949981   5.785 1.17e-08 ***\ntrend         1.517586   0.485628   3.125  0.00187 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 68.33 on 592 degrees of freedom\nMultiple R-Squared: 0.9996, Adjusted R-squared: 0.9996 \nF-statistic: 2.212e+05 on 7 and 592 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation PCE: \n==================================== \nPCE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n             Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -1.369810   0.398034  -3.441 0.000619 ***\nUNRATE.l1   -0.904493   0.890235  -1.016 0.310037    \nDSPIC96.l1   0.004132   0.007036   0.587 0.557198    \nPCE.l1       0.996026   0.003870 257.369  &lt; 2e-16 ***\nGS10.l1      1.687626   1.320248   1.278 0.201657    \nFEDFUNDS.l1 -0.214929   0.826369  -0.260 0.794886    \nconst       24.072882  32.249800   0.746 0.455692    \ntrend        0.612548   0.191109   3.205 0.001422 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 26.89 on 592 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 2.049e+06 on 7 and 592 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation GS10: \n===================================== \nGS10 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -1.170e-02  4.351e-03  -2.690  0.00734 ** \nUNRATE.l1    9.329e-03  9.731e-03   0.959  0.33810    \nDSPIC96.l1  -1.479e-04  7.691e-05  -1.923  0.05498 .  \nPCE.l1       3.879e-05  4.230e-05   0.917  0.35958    \nGS10.l1      9.608e-01  1.443e-02  66.582  &lt; 2e-16 ***\nFEDFUNDS.l1  2.072e-02  9.033e-03   2.293  0.02217 *  \nconst        9.874e-01  3.525e-01   2.801  0.00526 ** \ntrend        6.247e-03  2.089e-03   2.990  0.00290 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2939 on 592 degrees of freedom\nMultiple R-Squared: 0.9909, Adjusted R-squared: 0.9908 \nF-statistic:  9169 on 7 and 592 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation FEDFUNDS: \n========================================= \nFEDFUNDS = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -3.060e-02  7.902e-03  -3.873 0.000120 ***\nUNRATE.l1   -4.754e-02  1.767e-02  -2.690 0.007351 ** \nDSPIC96.l1  -1.187e-04  1.397e-04  -0.850 0.395625    \nPCE.l1       5.642e-05  7.683e-05   0.734 0.463016    \nGS10.l1      1.016e-01  2.621e-02   3.875 0.000119 ***\nFEDFUNDS.l1  9.292e-01  1.640e-02  56.643  &lt; 2e-16 ***\nconst        1.372e+00  6.402e-01   2.143 0.032557 *  \ntrend        1.280e-02  3.794e-03   3.375 0.000787 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.5338 on 592 degrees of freedom\nMultiple R-Squared: 0.9816, Adjusted R-squared: 0.9814 \nF-statistic:  4524 on 7 and 592 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n          CPIAUCSL    UNRATE    DSPIC96      PCE      GS10 FEDFUNDS\nCPIAUCSL  0.170259 -0.005113   -5.13816   3.9249  0.021823  0.01893\nUNRATE   -0.005113  0.028730    0.03106  -0.4434 -0.008505 -0.01666\nDSPIC96  -5.138156  0.031063 4668.55214 -37.0247 -0.429415 -0.71821\nPCE       3.924950 -0.443416  -37.02471 723.0004  0.555891  0.57875\nGS10      0.021823 -0.008505   -0.42941   0.5559  0.086381  0.05497\nFEDFUNDS  0.018928 -0.016662   -0.71821   0.5787  0.054966  0.28493\n\nCorrelation matrix of residuals:\n         CPIAUCSL    UNRATE   DSPIC96      PCE     GS10 FEDFUNDS\nCPIAUCSL  1.00000 -0.073101 -0.182247  0.35376  0.17995  0.08594\nUNRATE   -0.07310  1.000000  0.002682 -0.09729 -0.17072 -0.18416\nDSPIC96  -0.18225  0.002682  1.000000 -0.02015 -0.02138 -0.01969\nPCE       0.35376 -0.097292 -0.020153  1.00000  0.07034  0.04032\nGS10      0.17995 -0.170723 -0.021383  0.07034  1.00000  0.35036\nFEDFUNDS  0.08594 -0.184156 -0.019692  0.04032  0.35036  1.00000\n\n\nVAR(2) output:\n\nsummary(fit &lt;- VAR(var_ts1, p=2, type=\"both\"))\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: CPIAUCSL, UNRATE, DSPIC96, PCE, GS10, FEDFUNDS \nDeterministic variables: both \nSample size: 599 \nLog Likelihood: -6533.712 \nRoots of the characteristic polynomial:\n    1 0.9846 0.9683 0.9683 0.9149 0.876 0.4178 0.3643 0.3643 0.2837 0.2198 0.1095\nCall:\nVAR(y = var_ts1, p = 2, type = \"both\")\n\n\nEstimation results for equation CPIAUCSL: \n========================================= \nCPIAUCSL = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  1.332e+00  4.079e-02  32.656  &lt; 2e-16 ***\nUNRATE.l1    7.847e-02  9.250e-02   0.848 0.396587    \nDSPIC96.l1   1.502e-04  2.279e-04   0.659 0.509913    \nPCE.l1       2.146e-03  6.093e-04   3.522 0.000462 ***\nGS10.l1      6.798e-02  5.645e-02   1.204 0.228938    \nFEDFUNDS.l1  6.398e-02  3.078e-02   2.079 0.038096 *  \nCPIAUCSL.l2 -3.472e-01  4.074e-02  -8.523  &lt; 2e-16 ***\nUNRATE.l2   -4.980e-02  9.339e-02  -0.533 0.594023    \nDSPIC96.l2   6.233e-05  2.281e-04   0.273 0.784786    \nPCE.l2      -2.300e-03  6.101e-04  -3.770 0.000180 ***\nGS10.l2     -8.981e-02  5.630e-02  -1.595 0.111173    \nFEDFUNDS.l2 -2.116e-02  3.080e-02  -0.687 0.492337    \nconst       -4.592e-01  4.647e-01  -0.988 0.323472    \ntrend        5.600e-03  2.759e-03   2.029 0.042868 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3709 on 585 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.455e+06 on 13 and 585 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation UNRATE: \n======================================= \nUNRATE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  1.129e-02  1.822e-02   0.620 0.535767    \nUNRATE.l1    1.049e+00  4.133e-02  25.382  &lt; 2e-16 ***\nDSPIC96.l1   7.986e-05  1.018e-04   0.784 0.433185    \nPCE.l1      -1.136e-03  2.722e-04  -4.174 3.44e-05 ***\nGS10.l1     -4.383e-02  2.522e-02  -1.738 0.082768 .  \nFEDFUNDS.l1 -1.212e-02  1.375e-02  -0.881 0.378787    \nCPIAUCSL.l2 -2.694e-03  1.820e-02  -0.148 0.882412    \nUNRATE.l2   -4.968e-02  4.173e-02  -1.191 0.234303    \nDSPIC96.l2   6.881e-05  1.019e-04   0.675 0.499942    \nPCE.l2       1.078e-03  2.726e-04   3.952 8.68e-05 ***\nGS10.l2      2.662e-02  2.516e-02   1.058 0.290402    \nFEDFUNDS.l2  3.132e-02  1.376e-02   2.276 0.023214 *  \nconst       -7.552e-01  2.077e-01  -3.637 0.000300 ***\ntrend       -4.764e-03  1.233e-03  -3.864 0.000124 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1658 on 585 degrees of freedom\nMultiple R-Squared: 0.9894, Adjusted R-squared: 0.9892 \nF-statistic:  4207 on 13 and 585 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation DSPIC96: \n======================================== \nDSPIC96 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n             Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -21.71041    7.38894  -2.938  0.00343 ** \nUNRATE.l1   -28.75670   16.75780  -1.716  0.08669 .  \nDSPIC96.l1    0.72208    0.04128  17.492  &lt; 2e-16 ***\nPCE.l1        0.11559    0.11038   1.047  0.29544    \nGS10.l1       9.94580   10.22646   0.973  0.33118    \nFEDFUNDS.l1  -4.26814    5.57660  -0.765  0.44436    \nCPIAUCSL.l2  20.27753    7.38086   2.747  0.00619 ** \nUNRATE.l2    21.98608   16.91809   1.300  0.19426    \nDSPIC96.l2    0.19711    0.04133   4.769 2.34e-06 ***\nPCE.l2       -0.07270    0.11053  -0.658  0.51100    \nGS10.l2      -5.92789   10.19874  -0.581  0.56130    \nFEDFUNDS.l2   1.70186    5.58020   0.305  0.76049    \nconst       375.76035   84.18901   4.463 9.68e-06 ***\ntrend         1.17311    0.49986   2.347  0.01926 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 67.2 on 585 degrees of freedom\nMultiple R-Squared: 0.9996, Adjusted R-squared: 0.9996 \nF-statistic: 1.227e+05 on 13 and 585 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation PCE: \n==================================== \nPCE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  13.895207   2.877179   4.829 1.75e-06 ***\nUNRATE.l1   -17.149924   6.525321  -2.628 0.008809 ** \nDSPIC96.l1    0.006826   0.016075   0.425 0.671268    \nPCE.l1        0.831833   0.042980  19.354  &lt; 2e-16 ***\nGS10.l1      -1.455403   3.982084  -0.365 0.714879    \nFEDFUNDS.l1  -2.117331   2.171472  -0.975 0.329930    \nCPIAUCSL.l2 -15.358472   2.874035  -5.344 1.31e-07 ***\nUNRATE.l2    16.235457   6.587737   2.464 0.014007 *  \nDSPIC96.l2   -0.001770   0.016093  -0.110 0.912473    \nPCE.l2        0.163794   0.043040   3.806 0.000156 ***\nGS10.l2       3.266601   3.971290   0.823 0.411097    \nFEDFUNDS.l2   1.485204   2.172876   0.684 0.494549    \nconst        22.615719  32.782368   0.690 0.490547    \ntrend         0.642373   0.194642   3.300 0.001025 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 26.17 on 585 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.162e+06 on 13 and 585 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation GS10: \n===================================== \nGS10 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  4.145e-02  3.020e-02   1.373   0.1704    \nUNRATE.l1   -1.361e-01  6.849e-02  -1.987   0.0473 *  \nDSPIC96.l1   2.064e-05  1.687e-04   0.122   0.9027    \nPCE.l1       1.127e-03  4.511e-04   2.499   0.0127 *  \nGS10.l1      1.251e+00  4.180e-02  29.923  &lt; 2e-16 ***\nFEDFUNDS.l1 -1.862e-02  2.279e-02  -0.817   0.4142    \nCPIAUCSL.l2 -4.634e-02  3.017e-02  -1.536   0.1251    \nUNRATE.l2    1.554e-01  6.915e-02   2.247   0.0250 *  \nDSPIC96.l2  -1.173e-04  1.689e-04  -0.695   0.4875    \nPCE.l2      -1.120e-03  4.518e-04  -2.478   0.0135 *  \nGS10.l2     -3.150e-01  4.168e-02  -7.557  1.6e-13 ***\nFEDFUNDS.l2  4.984e-02  2.281e-02   2.185   0.0293 *  \nconst        6.527e-01  3.441e-01   1.897   0.0583 .  \ntrend        3.135e-03  2.043e-03   1.535   0.1254    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2747 on 585 degrees of freedom\nMultiple R-Squared: 0.9921, Adjusted R-squared: 0.9919 \nF-statistic:  5659 on 13 and 585 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation FEDFUNDS: \n========================================= \nFEDFUNDS = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -0.0456235  0.0510978  -0.893 0.372296    \nUNRATE.l1   -0.4759622  0.1158875  -4.107 4.58e-05 ***\nDSPIC96.l1  -0.0001574  0.0002855  -0.551 0.581543    \nPCE.l1       0.0002825  0.0007633   0.370 0.711414    \nGS10.l1      0.5268645  0.0707205   7.450 3.37e-13 ***\nFEDFUNDS.l1  1.2355455  0.0385646  32.038  &lt; 2e-16 ***\nCPIAUCSL.l2  0.0359225  0.0510419   0.704 0.481847    \nUNRATE.l2    0.4448581  0.1169960   3.802 0.000158 ***\nDSPIC96.l2   0.0002183  0.0002858   0.764 0.445329    \nPCE.l2      -0.0003050  0.0007644  -0.399 0.690043    \nGS10.l2     -0.4494898  0.0705288  -6.373 3.76e-10 ***\nFEDFUNDS.l2 -0.3013560  0.0385896  -7.809 2.67e-14 ***\nconst        0.2013182  0.5822041   0.346 0.729628    \ntrend        0.0029051  0.0034568   0.840 0.401020    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.4647 on 585 degrees of freedom\nMultiple R-Squared: 0.9862, Adjusted R-squared: 0.9859 \nF-statistic:  3223 on 13 and 585 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n          CPIAUCSL    UNRATE    DSPIC96      PCE      GS10  FEDFUNDS\nCPIAUCSL  0.137601 -0.001106 -4.549e+00   3.4120  0.010362  0.007159\nUNRATE   -0.001106  0.027474 -1.712e-03  -0.5035 -0.005972 -0.009845\nDSPIC96  -4.549058 -0.001712  4.516e+03 -14.9202 -0.370402 -1.288496\nPCE       3.412025 -0.503523 -1.492e+01 684.7491  0.457156  0.524037\nGS10      0.010362 -0.005972 -3.704e-01   0.4572  0.075443  0.038994\nFEDFUNDS  0.007159 -0.009845 -1.288e+00   0.5240  0.038994  0.215974\n\nCorrelation matrix of residuals:\n         CPIAUCSL     UNRATE    DSPIC96       PCE     GS10 FEDFUNDS\nCPIAUCSL  1.00000 -0.0179823 -0.1824864  0.351509  0.10170  0.04153\nUNRATE   -0.01798  1.0000000 -0.0001537 -0.116089 -0.13118 -0.12781\nDSPIC96  -0.18249 -0.0001537  1.0000000 -0.008485 -0.02007 -0.04126\nPCE       0.35151 -0.1160891 -0.0084845  1.000000  0.06360  0.04309\nGS10      0.10170 -0.1311763 -0.0200670  0.063605  1.00000  0.30548\nFEDFUNDS  0.04153 -0.1278084 -0.0412574  0.043092  0.30548  1.00000\n\n\nVAR(3) output:\n\nsummary(fit &lt;- VAR(var_ts1, p=3, type=\"both\"))\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: CPIAUCSL, UNRATE, DSPIC96, PCE, GS10, FEDFUNDS \nDeterministic variables: both \nSample size: 598 \nLog Likelihood: -6455.747 \nRoots of the characteristic polynomial:\n    1 0.9822 0.9737 0.9737 0.9449 0.9028 0.5495 0.4541 0.4541 0.4277 0.4277 0.4055 0.3394 0.3394 0.3173 0.3173 0.223 0.223\nCall:\nVAR(y = var_ts1, p = 3, type = \"both\")\n\n\nEstimation results for equation CPIAUCSL: \n========================================= \nCPIAUCSL = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  1.3999529  0.0442577  31.632  &lt; 2e-16 ***\nUNRATE.l1    0.0972676  0.0924184   1.052 0.293024    \nDSPIC96.l1   0.0002741  0.0002283   1.201 0.230377    \nPCE.l1       0.0023667  0.0006194   3.821 0.000147 ***\nGS10.l1      0.0882201  0.0579665   1.522 0.128577    \nFEDFUNDS.l1  0.0627856  0.0341177   1.840 0.066242 .  \nCPIAUCSL.l2 -0.5854177  0.0684994  -8.546  &lt; 2e-16 ***\nUNRATE.l2   -0.0386682  0.1317426  -0.294 0.769235    \nDSPIC96.l2   0.0005502  0.0002792   1.971 0.049206 *  \nPCE.l2      -0.0018320  0.0007718  -2.373 0.017947 *  \nGS10.l2     -0.1320624  0.0883875  -1.494 0.135687    \nFEDFUNDS.l2 -0.0014276  0.0521380  -0.027 0.978165    \nCPIAUCSL.l3  0.1717438  0.0426604   4.026 6.43e-05 ***\nUNRATE.l3   -0.0290749  0.0934086  -0.311 0.755710    \nDSPIC96.l3  -0.0006750  0.0002289  -2.950 0.003310 ** \nPCE.l3      -0.0006551  0.0006330  -1.035 0.301204    \nGS10.l3      0.0169111  0.0588071   0.288 0.773780    \nFEDFUNDS.l3 -0.0133484  0.0323942  -0.412 0.680448    \nconst       -0.2750833  0.4670350  -0.589 0.556092    \ntrend        0.0053832  0.0027764   1.939 0.053000 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3633 on 578 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.033e+06 on 19 and 578 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation UNRATE: \n======================================= \nUNRATE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  2.605e-02  1.973e-02   1.320  0.18722    \nUNRATE.l1    1.016e+00  4.120e-02  24.659  &lt; 2e-16 ***\nDSPIC96.l1   9.791e-05  1.018e-04   0.962  0.33655    \nPCE.l1      -1.204e-03  2.761e-04  -4.361 1.53e-05 ***\nGS10.l1     -4.285e-02  2.584e-02  -1.658  0.09781 .  \nFEDFUNDS.l1  1.047e-02  1.521e-02   0.688  0.49144    \nCPIAUCSL.l2 -2.436e-02  3.054e-02  -0.798  0.42533    \nUNRATE.l2    1.524e-01  5.873e-02   2.596  0.00968 ** \nDSPIC96.l2  -5.697e-05  1.244e-04  -0.458  0.64728    \nPCE.l2       5.165e-04  3.441e-04   1.501  0.13393    \nGS10.l2      4.085e-02  3.941e-02   1.037  0.30034    \nFEDFUNDS.l2 -2.391e-02  2.324e-02  -1.029  0.30406    \nCPIAUCSL.l3  4.860e-03  1.902e-02   0.256  0.79840    \nUNRATE.l3   -1.724e-01  4.164e-02  -4.140 3.99e-05 ***\nDSPIC96.l3   8.292e-05  1.020e-04   0.813  0.41673    \nPCE.l3       6.388e-04  2.822e-04   2.264  0.02397 *  \nGS10.l3     -1.217e-02  2.622e-02  -0.464  0.64274    \nFEDFUNDS.l3  3.125e-02  1.444e-02   2.164  0.03088 *  \nconst       -6.055e-01  2.082e-01  -2.908  0.00378 ** \ntrend       -3.654e-03  1.238e-03  -2.952  0.00329 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.162 on 578 degrees of freedom\nMultiple R-Squared:  0.99,  Adjusted R-squared: 0.9897 \nF-statistic:  3010 on 19 and 578 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation DSPIC96: \n======================================== \nDSPIC96 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n             Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -29.12109    8.10177  -3.594 0.000353 ***\nUNRATE.l1   -30.02520   16.91801  -1.775 0.076466 .  \nDSPIC96.l1    0.69196    0.04180  16.555  &lt; 2e-16 ***\nPCE.l1        0.11451    0.11338   1.010 0.312922    \nGS10.l1      11.79430   10.61128   1.111 0.266821    \nFEDFUNDS.l1  -6.19385    6.24555  -0.992 0.321749    \nCPIAUCSL.l2  37.95807   12.53942   3.027 0.002579 ** \nUNRATE.l2    24.42613   24.11665   1.013 0.311564    \nDSPIC96.l2    0.08208    0.05110   1.606 0.108756    \nPCE.l2       -0.09091    0.14129  -0.643 0.520201    \nGS10.l2      -8.52430   16.18011  -0.527 0.598508    \nFEDFUNDS.l2   6.96975    9.54433   0.730 0.465533    \nCPIAUCSL.l3 -10.35743    7.80936  -1.326 0.185269    \nUNRATE.l3    -1.64514   17.09927  -0.096 0.923386    \nDSPIC96.l3    0.15822    0.04189   3.777 0.000175 ***\nPCE.l3        0.01317    0.11588   0.114 0.909542    \nGS10.l3       2.80167   10.76517   0.260 0.794761    \nFEDFUNDS.l3  -4.57569    5.93006  -0.772 0.440660    \nconst       329.61650   85.49492   3.855 0.000129 ***\ntrend         1.10550    0.50825   2.175 0.030025 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 66.5 on 578 degrees of freedom\nMultiple R-Squared: 0.9996, Adjusted R-squared: 0.9996 \nF-statistic: 8.545e+04 on 19 and 578 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation PCE: \n==================================== \nPCE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n             Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  16.05085    3.17036   5.063 5.56e-07 ***\nUNRATE.l1   -16.36366    6.62032  -2.472 0.013733 *  \nDSPIC96.l1    0.01171    0.01636   0.716 0.474330    \nPCE.l1        0.82297    0.04437  18.549  &lt; 2e-16 ***\nGS10.l1      -0.32174    4.15238  -0.077 0.938265    \nFEDFUNDS.l1  -1.06643    2.44400  -0.436 0.662749    \nCPIAUCSL.l2 -20.50444    4.90690  -4.179 3.39e-05 ***\nUNRATE.l2     2.97363    9.43727   0.315 0.752804    \nDSPIC96.l2    0.01885    0.02000   0.943 0.346262    \nPCE.l2        0.19367    0.05529   3.503 0.000496 ***\nGS10.l2      -6.49579    6.33156  -1.026 0.305350    \nFEDFUNDS.l2  -0.49691    3.73486  -0.133 0.894203    \nCPIAUCSL.l3   3.02337    3.05594   0.989 0.322909    \nUNRATE.l3    12.70421    6.69125   1.899 0.058111 .  \nDSPIC96.l3   -0.02594    0.01639  -1.582 0.114087    \nPCE.l3       -0.02060    0.04535  -0.454 0.649819    \nGS10.l3       8.28570    4.21260   1.967 0.049674 *  \nFEDFUNDS.l3   1.34115    2.32053   0.578 0.563524    \nconst        21.37961   33.45567   0.639 0.523048    \ntrend         0.62832    0.19889   3.159 0.001665 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 26.02 on 578 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 8.017e+05 on 19 and 578 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation GS10: \n===================================== \nGS10 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  7.595e-02  3.280e-02   2.316  0.02091 *  \nUNRATE.l1   -1.345e-01  6.848e-02  -1.965  0.04994 *  \nDSPIC96.l1   8.116e-05  1.692e-04   0.480  0.63165    \nPCE.l1       1.088e-03  4.590e-04   2.370  0.01811 *  \nGS10.l1      1.300e+00  4.295e-02  30.265  &lt; 2e-16 ***\nFEDFUNDS.l1  1.310e-02  2.528e-02   0.518  0.60442    \nCPIAUCSL.l2 -1.344e-01  5.076e-02  -2.647  0.00834 ** \nUNRATE.l2    2.030e-01  9.762e-02   2.079  0.03806 *  \nDSPIC96.l2  -7.918e-05  2.069e-04  -0.383  0.70201    \nPCE.l2      -8.839e-04  5.719e-04  -1.545  0.12281    \nGS10.l2     -5.622e-01  6.550e-02  -8.584  &lt; 2e-16 ***\nFEDFUNDS.l2  2.509e-02  3.863e-02   0.649  0.51628    \nCPIAUCSL.l3  5.247e-02  3.161e-02   1.660  0.09747 .  \nUNRATE.l3   -5.275e-02  6.922e-02  -0.762  0.44634    \nDSPIC96.l3  -1.253e-04  1.696e-04  -0.739  0.46036    \nPCE.l3      -1.757e-04  4.691e-04  -0.375  0.70805    \nGS10.l3      2.073e-01  4.358e-02   4.757 2.49e-06 ***\nFEDFUNDS.l3 -9.962e-03  2.400e-02  -0.415  0.67830    \nconst        7.471e-01  3.461e-01   2.159  0.03127 *  \ntrend        3.641e-03  2.057e-03   1.770  0.07733 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2692 on 578 degrees of freedom\nMultiple R-Squared: 0.9925, Adjusted R-squared: 0.9923 \nF-statistic:  4032 on 19 and 578 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation FEDFUNDS: \n========================================= \nFEDFUNDS = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -1.087e-02  5.599e-02  -0.194  0.84616    \nUNRATE.l1   -4.983e-01  1.169e-01  -4.262 2.37e-05 ***\nDSPIC96.l1  -6.070e-05  2.889e-04  -0.210  0.83365    \nPCE.l1       1.475e-04  7.836e-04   0.188  0.85077    \nGS10.l1      5.137e-01  7.334e-02   7.005 6.88e-12 ***\nFEDFUNDS.l1  1.318e+00  4.316e-02  30.538  &lt; 2e-16 ***\nCPIAUCSL.l2 -2.474e-02  8.666e-02  -0.285  0.77543    \nUNRATE.l2    5.383e-01  1.667e-01   3.230  0.00131 ** \nDSPIC96.l2   3.376e-04  3.532e-04   0.956  0.33946    \nPCE.l2      -9.879e-05  9.765e-04  -0.101  0.91945    \nGS10.l2     -5.653e-01  1.118e-01  -5.056 5.77e-07 ***\nFEDFUNDS.l2 -4.970e-01  6.596e-02  -7.534 1.90e-13 ***\nCPIAUCSL.l3  2.421e-02  5.397e-02   0.449  0.65386    \nUNRATE.l3   -7.198e-02  1.182e-01  -0.609  0.54271    \nDSPIC96.l3  -2.633e-04  2.895e-04  -0.909  0.36357    \nPCE.l3      -4.632e-05  8.009e-04  -0.058  0.95390    \nGS10.l3      1.205e-01  7.440e-02   1.619  0.10597    \nFEDFUNDS.l3  1.221e-01  4.098e-02   2.978  0.00302 ** \nconst        4.190e-01  5.909e-01   0.709  0.47857    \ntrend        3.908e-03  3.513e-03   1.113  0.26630    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.4596 on 578 degrees of freedom\nMultiple R-Squared: 0.9867, Adjusted R-squared: 0.9862 \nF-statistic:  2254 on 19 and 578 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n           CPIAUCSL     UNRATE    DSPIC96      PCE      GS10  FEDFUNDS\nCPIAUCSL  0.1319789 -0.0009514   -3.73603   3.2543  0.007906  0.005619\nUNRATE   -0.0009514  0.0262322   -0.03135  -0.4439 -0.006329 -0.011396\nDSPIC96  -3.7360288 -0.0313497 4422.68797  13.7281 -0.167113 -0.932497\nPCE       3.2543067 -0.4439276   13.72809 677.2438  0.300133  0.388906\nGS10      0.0079055 -0.0063292   -0.16711   0.3001  0.072468  0.036450\nFEDFUNDS  0.0056189 -0.0113963   -0.93250   0.3889  0.036450  0.211253\n\nCorrelation matrix of residuals:\n         CPIAUCSL    UNRATE   DSPIC96       PCE      GS10 FEDFUNDS\nCPIAUCSL  1.00000 -0.016170 -0.154637  0.344218  0.080836  0.03365\nUNRATE   -0.01617  1.000000 -0.002911 -0.105323 -0.145164 -0.15309\nDSPIC96  -0.15464 -0.002911  1.000000  0.007932 -0.009335 -0.03051\nPCE       0.34422 -0.105323  0.007932  1.000000  0.042842  0.03251\nGS10      0.08084 -0.145164 -0.009335  0.042842  1.000000  0.29459\nFEDFUNDS  0.03365 -0.153089 -0.030507  0.032514  0.294595  1.00000"
  },
  {
    "objectID": "arimax-sarimax-var.html#k-fold-cross-validation-and-model-diagnostics",
    "href": "arimax-sarimax-var.html#k-fold-cross-validation-and-model-diagnostics",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "K-Fold Cross Validation and Model Diagnostics",
    "text": "K-Fold Cross Validation and Model Diagnostics\n\n# Define the number of folds for cross-validation\nk &lt;- 5\n\n# Define the p values to test\np_values &lt;- c(1, 2, 3)\n\n# Split the data into k folds\ncv_folds &lt;- cut(seq(1, nrow(var_ts1)), breaks = k, labels = FALSE)\n\n# Initialize vectors to store RMSE and AIC values for each p value\nrmse_vec &lt;- numeric(length(p_values))\naic_vec &lt;- numeric(length(p_values))\n\n# Loop over p values and perform cross-validation\nfor (i in seq_along(p_values)) {\n  p &lt;- p_values[i]\n  rmse_cv &lt;- numeric(k)\n  aic_cv &lt;- numeric(k)\n  for (j in 1:k) {\n    # Split the data into training and testing sets\n    train &lt;- var_ts1[cv_folds != j, ]\n    test &lt;- var_ts1[cv_folds == j, ]\n    \n    # Fit the VAR model with the current p value\n    var_fit &lt;- VAR(train, p = p)\n    \n    # Make predictions for the testing set\n    pred &lt;- predict(var_fit, n.ahead = nrow(test))$fcst\n    \n    # Calculate RMSE and AIC for the current fold\n    rmse_cv[j] &lt;- sqrt(mean((pred$CPIAUCSL - test[,1])^2))\n    aic_cv[j] &lt;- AIC(var_fit)\n  }\n  # Calculate the mean RMSE and AIC across all folds for the current p value\n  rmse_vec[i] &lt;- mean(rmse_cv)\n  aic_vec[i] &lt;- mean(aic_cv)\n}\n\n# Create a table of RMSE and AIC values for each p value\nresults_table &lt;- tibble(p_values, rmse_vec, aic_vec)\n\n# Print the results table\nkable(results_table, format = \"markdown\", \n        col.names = c(\"P Values\", \"Mean RMSE (5 Folds)\", \"Mean AIC (5 Folds)\"), align = \"c\", digits = 2\n        )\n\n\n\n\nP Values\nMean RMSE (5 Folds)\nMean AIC (5 Folds)\n\n\n\n\n1\n151.16\n11942.99\n\n\n2\n145.37\n11658.50\n\n\n3\n144.87\n11587.17\n\n\n\n\n\nThe VAR(3) model outputs the lowest Mean RMSE of 144.87 inflation from the 5-fold cross validation. However, it has the highest AIC score. Because test set performance is best and it is the simplest model, we shall choose the VAR(3) model as the best option."
  },
  {
    "objectID": "arimax-sarimax-var.html#forecasting-the-chosen-model-p3",
    "href": "arimax-sarimax-var.html#forecasting-the-chosen-model-p3",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasting the chosen model (P=3)",
    "text": "Forecasting the chosen model (P=3)\n\nfinal_var &lt;- VAR(var_ts1, p = 3)\n\n(fit.pr = predict(final_var, n.ahead = 5, ci = 0.95)) # 5 years ahead \n\n$CPIAUCSL\n         fcst    lower    upper       CI\n[1,] 259.3881 258.6743 260.1018 0.713728\n[2,] 259.8701 258.5939 261.1463 1.276158\n[3,] 260.3400 258.6425 262.0376 1.697582\n[4,] 260.7761 258.7572 262.7949 2.018880\n[5,] 261.2104 258.9234 263.4973 2.286950\n\n$UNRATE\n         fcst    lower    upper        CI\n[1,] 3.482483 3.162932 3.802033 0.3195505\n[2,] 3.462199 2.991376 3.933021 0.4708229\n[3,] 3.476632 2.842868 4.110396 0.6337641\n[4,] 3.489748 2.715837 4.263658 0.7739103\n[5,] 3.513099 2.612477 4.413721 0.9006220\n\n$DSPIC96\n         fcst    lower    upper       CI\n[1,] 15887.36 15756.60 16018.13 130.7634\n[2,] 15906.22 15742.93 16069.51 163.2903\n[3,] 15948.27 15766.61 16129.92 181.6532\n[4,] 15986.92 15786.95 16186.88 199.9637\n[5,] 16022.93 15806.49 16239.36 216.4349\n\n$PCE\n         fcst    lower    upper        CI\n[1,] 14855.74 14804.34 14907.14  51.39997\n[2,] 14908.94 14837.64 14980.25  71.30850\n[3,] 14956.63 14865.87 15047.38  90.75522\n[4,] 15003.92 14897.13 15110.71 106.78949\n[5,] 15050.73 14929.64 15171.82 121.09257\n\n$GS10\n         fcst     lower    upper        CI\n[1,] 1.715155 1.1865638 2.243747 0.5285914\n[2,] 1.689191 0.7992903 2.579092 0.8899010\n[3,] 1.667817 0.5549866 2.780648 1.1128308\n[4,] 1.630803 0.3630515 2.898554 1.2677513\n[5,] 1.588605 0.1911054 2.986104 1.3974992\n\n$FEDFUNDS\n         fcst       lower    upper        CI\n[1,] 1.517328  0.61629815 2.418358 0.9010301\n[2,] 1.552455 -0.05806836 3.162979 1.6105236\n[3,] 1.591517 -0.53307984 3.716113 2.1245965\n[4,] 1.593517 -0.90452339 4.091558 2.4980406\n[5,] 1.579673 -1.20701980 4.366366 2.7866927\n\nfanchart(fit.pr) # plot prediction + error\n\n\n\n\nThe above plot showcases the forecasts for each variable present in the VAR(3) model, Yearly Inflation, Unemployment, Personal Consumption, Treasury Yield and Federal Funds Rate. The predicted forecast, from the years 2021 to 2025, for CPI is a good sign for the US due to the rising trend.\nLet us visualize more closely the forecasts for the CPI from 2021 to 2025, corresponding to the VAR(3) model fitted on all years (1970-2020):\n\ndf_fvar_attack &lt;- as.data.frame(fit.pr$fcst$CPIAUCSL)\n# add year column\ndf_fvar_attack$Year &lt;- c(\"2021\", \"2022\", \"2023\", \"2024\", \"2025\")\n(var_plot &lt;- ggplot(data=df_fvar_attack, aes(x=Year, y=fcst, group = 1)) +\n    geom_line(aes(color=\"Forecast\"), linewidth=1) +\n    geom_ribbon(aes(ymin=lower, ymax=upper, fill=\"Confidence Interval\"), alpha=0.1) +\n    labs(title=\"VAR(3) Forecasts for CPI from 2021 to 2025\",\n         y=\"CPI\",\n         color=\"\", fill=\"\",\n         caption=\"Data Sources: FRED\") +\n    scale_color_manual(values = c(\"Forecast\"=\"red\")) +\n    scale_fill_manual(values = c(\"95% Confidence Interval\"=\"steelblue\")) +\n    theme_minimal() + \n  theme(plot.caption.position = \"plot\"))"
  },
  {
    "objectID": "arimax-sarimax-var.html#building-the-var-model-post-covid-19",
    "href": "arimax-sarimax-var.html#building-the-var-model-post-covid-19",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Building the VAR Model (Post COVID-19)",
    "text": "Building the VAR Model (Post COVID-19)\n\nTime Series Plot\n\nplot.ts(var_ts2 , main = \"\", xlab = \"\")\n\n\n\n\n\n\nPair Plots\n\n# create scatterplot matrix using plotly\nfig &lt;- plot_ly(\n  data = as.data.frame(var_ts2), \n  type = \"splom\",\n  diagonal = list(visible = FALSE),\n  dimensions = list(\n    list(label = \"CPI\", values = ~CPIAUCSL),\n    list(label = \"Unemployment\", values = ~UNRATE),\n    list(label = \"DisposableIncome\", values = ~DSPIC96),\n    list(label = \"PersonalConsumption\", values = ~PCE),\n    list(label = \"TreasuryYield\", values = ~GS10),\n    list(label = \"FederalFundsRate\", values = ~FEDFUNDS)\n  )\n) %&gt;%\n  layout(hovermode = \"x\")\n\nfig &lt;- fig %&gt;% \n  layout(\n    title = \"Scatterplot Matrix of VAR Model Variables (Post COVID-19)\",\n    xaxis = list(title = \"\"),\n    yaxis = list(title = \"\")\n  )\n\n# display plot\nfig\n\n\n\n\n\n\n\nFitting the VAR Model\nHere we use the VARselect() function to find the best p to fit VAR(p). We will choose a maximum lag of 10 and check which p value returns lowest AIC.\n\n(var_result &lt;- VARselect(var_ts2, lag.max = 10, type = \"both\"))\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10      4      2     10 \n\n$criteria\n                  1           2           3           4           5           6\nAIC(n)     12.10596    10.67409    10.54164    10.43213    10.43372    10.42279\nHQ(n)      12.23718    10.90373    10.86970    10.85861    10.95863    11.04611\nSC(n)      12.44385    11.26540    11.38636    11.53027    11.78529    12.02776\nFPE(n) 180947.94882 43223.11299 37864.33401 33941.84237 34004.38817 33646.23719\n                 7           8           9          10\nAIC(n)    10.40910    10.41932    10.37218    10.33357\nHQ(n)     11.13084    11.23949    11.29077    11.35058\nSC(n)     12.26750    12.53113    12.73741    12.95222\nFPE(n) 33204.88543 33567.30729 32047.59811 30864.68386\n\n\nNow, we will fit VAR(1), VAR(2), and VAR(3):\nVAR(1) output:\n\nsummary(fit &lt;- VAR(var_ts2, p=1, type=\"both\"))\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: CPIAUCSL, UNRATE, DSPIC96, PCE, GS10, FEDFUNDS \nDeterministic variables: both \nSample size: 641 \nLog Likelihood: -9274.084 \nRoots of the characteristic polynomial:\n0.999 0.9974 0.9974 0.9575 0.9152 0.6585\nCall:\nVAR(y = var_ts2, p = 1, type = \"both\")\n\n\nEstimation results for equation CPIAUCSL: \n========================================= \nCPIAUCSL = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  1.004e+00  5.422e-03 185.098   &lt;2e-16 ***\nUNRATE.l1   -4.642e-03  1.376e-02  -0.337   0.7360    \nDSPIC96.l1   1.140e-04  6.431e-05   1.773   0.0768 .  \nPCE.l1       6.650e-05  3.760e-05   1.769   0.0775 .  \nGS10.l1      2.405e-02  2.320e-02   1.037   0.3002    \nFEDFUNDS.l1  2.948e-02  1.441e-02   2.045   0.0413 *  \nconst       -5.464e-01  3.362e-01  -1.625   0.1046    \ntrend       -4.218e-03  2.366e-03  -1.783   0.0751 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.49 on 633 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.952e+06 on 7 and 633 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation UNRATE: \n======================================= \nUNRATE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -7.237e-04  5.114e-03  -0.142   0.8875    \nUNRATE.l1    9.644e-01  1.298e-02  74.289   &lt;2e-16 ***\nDSPIC96.l1  -9.547e-05  6.066e-05  -1.574   0.1160    \nPCE.l1       3.030e-05  3.546e-05   0.854   0.3932    \nGS10.l1     -1.413e-02  2.188e-02  -0.646   0.5188    \nFEDFUNDS.l1  1.259e-02  1.360e-02   0.926   0.3547    \nconst        6.086e-01  3.171e-01   1.919   0.0554 .  \ntrend        1.350e-03  2.232e-03   0.605   0.5453    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.4622 on 633 degrees of freedom\nMultiple R-Squared: 0.9264, Adjusted R-squared: 0.9256 \nF-statistic:  1139 on 7 and 633 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation DSPIC96: \n======================================== \nDSPIC96 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  -12.55136    2.56069  -4.902 1.21e-06 ***\nUNRATE.l1      4.17093    6.50028   0.642    0.521    \nDSPIC96.l1     0.68905    0.03037  22.687  &lt; 2e-16 ***\nPCE.l1         0.13983    0.01776   7.875 1.49e-14 ***\nGS10.l1      -17.75539   10.95601  -1.621    0.106    \nFEDFUNDS.l1   10.10453    6.80730   1.484    0.138    \nconst       1503.78315  158.78407   9.471  &lt; 2e-16 ***\ntrend          7.70122    1.11749   6.892 1.34e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 231.4 on 633 degrees of freedom\nMultiple R-Squared: 0.9965, Adjusted R-squared: 0.9964 \nF-statistic: 2.559e+04 on 7 and 633 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation PCE: \n==================================== \nPCE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  2.903e+00  1.071e+00   2.710 0.006911 ** \nUNRATE.l1    9.673e+00  2.719e+00   3.558 0.000402 ***\nDSPIC96.l1   7.236e-02  1.270e-02   5.696 1.88e-08 ***\nPCE.l1       9.703e-01  7.427e-03 130.635  &lt; 2e-16 ***\nGS10.l1     -2.477e+00  4.583e+00  -0.541 0.589006    \nFEDFUNDS.l1  2.484e+00  2.847e+00   0.872 0.383275    \nconst       -3.949e+02  6.642e+01  -5.946 4.53e-09 ***\ntrend       -1.752e+00  4.674e-01  -3.748 0.000195 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 96.8 on 633 degrees of freedom\nMultiple R-Squared: 0.9996, Adjusted R-squared: 0.9996 \nF-statistic: 2.233e+05 on 7 and 633 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation GS10: \n===================================== \nGS10 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -2.657e-03  3.239e-03  -0.820   0.4123    \nUNRATE.l1    7.473e-03  8.222e-03   0.909   0.3637    \nDSPIC96.l1  -6.607e-05  3.841e-05  -1.720   0.0859 .  \nPCE.l1       3.731e-05  2.246e-05   1.661   0.0972 .  \nGS10.l1      9.671e-01  1.386e-02  69.788   &lt;2e-16 ***\nFEDFUNDS.l1  1.724e-02  8.610e-03   2.003   0.0456 *  \nconst        4.340e-01  2.008e-01   2.161   0.0311 *  \ntrend        1.281e-03  1.413e-03   0.907   0.3650    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2927 on 633 degrees of freedom\nMultiple R-Squared: 0.9915, Adjusted R-squared: 0.9914 \nF-statistic: 1.05e+04 on 7 and 633 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation FEDFUNDS: \n========================================= \nFEDFUNDS = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -1.242e-02  5.813e-03  -2.137  0.03296 *  \nUNRATE.l1   -4.563e-02  1.476e-02  -3.092  0.00207 ** \nDSPIC96.l1  -3.499e-05  6.895e-05  -0.507  0.61201    \nPCE.l1       8.068e-05  4.031e-05   2.001  0.04577 *  \nGS10.l1      1.006e-01  2.487e-02   4.044  5.9e-05 ***\nFEDFUNDS.l1  9.305e-01  1.545e-02  60.214  &lt; 2e-16 ***\nconst        5.428e-01  3.604e-01   1.506  0.13259    \ntrend        3.728e-03  2.537e-03   1.469  0.14219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.5253 on 633 degrees of freedom\nMultiple R-Squared: 0.9823, Adjusted R-squared: 0.9821 \nF-statistic:  5030 on 7 and 633 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n          CPIAUCSL    UNRATE   DSPIC96      PCE     GS10 FEDFUNDS\nCPIAUCSL   0.24013  -0.05535   -15.686   16.695  0.03101  0.02665\nUNRATE    -0.05535   0.21361    36.255  -32.261 -0.01421 -0.03240\nDSPIC96  -15.68635  36.25500 53555.024 -571.886 -1.06595 -3.35790\nPCE       16.69544 -32.26071  -571.886 9369.730  3.31255  5.74350\nGS10       0.03101  -0.01421    -1.066    3.313  0.08567  0.05487\nFEDFUNDS   0.02665  -0.03240    -3.358    5.744  0.05487  0.27597\n\nCorrelation matrix of residuals:\n         CPIAUCSL  UNRATE  DSPIC96      PCE     GS10 FEDFUNDS\nCPIAUCSL   1.0000 -0.2444 -0.13833  0.35198  0.21621  0.10354\nUNRATE    -0.2444  1.0000  0.33897 -0.72110 -0.10500 -0.13344\nDSPIC96   -0.1383  0.3390  1.00000 -0.02553 -0.01574 -0.02762\nPCE        0.3520 -0.7211 -0.02553  1.00000  0.11692  0.11295\nGS10       0.2162 -0.1050 -0.01574  0.11692  1.00000  0.35682\nFEDFUNDS   0.1035 -0.1334 -0.02762  0.11295  0.35682  1.00000\n\n\nVAR(2) output:\n\nsummary(fit &lt;- VAR(var_ts2, p=2, type=\"both\"))\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: CPIAUCSL, UNRATE, DSPIC96, PCE, GS10, FEDFUNDS \nDeterministic variables: both \nSample size: 640 \nLog Likelihood: -8765.803 \nRoots of the characteristic polynomial:\n1.002 0.9938 0.976 0.9665 0.8687 0.8687 0.5569 0.4755 0.439 0.439 0.3769 0.3769\nCall:\nVAR(y = var_ts2, p = 2, type = \"both\")\n\n\nEstimation results for equation CPIAUCSL: \n========================================= \nCPIAUCSL = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  1.4215516  0.0379593  37.449  &lt; 2e-16 ***\nUNRATE.l1    0.1853689  0.0597027   3.105  0.00199 ** \nDSPIC96.l1  -0.0000137  0.0000832  -0.165  0.86928    \nPCE.l1       0.0011975  0.0002791   4.290 2.07e-05 ***\nGS10.l1      0.1185237  0.0627596   1.889  0.05942 .  \nFEDFUNDS.l1  0.0646079  0.0344637   1.875  0.06130 .  \nCPIAUCSL.l2 -0.4220773  0.0380908 -11.081  &lt; 2e-16 ***\nUNRATE.l2   -0.1829256  0.0605829  -3.019  0.00264 ** \nDSPIC96.l2   0.0001004  0.0000787   1.275  0.20264    \nPCE.l2      -0.0011885  0.0002694  -4.411 1.21e-05 ***\nGS10.l2     -0.1108661  0.0627963  -1.765  0.07797 .  \nFEDFUNDS.l2 -0.0468105  0.0341387  -1.371  0.17081    \nconst       -0.3101875  0.3341981  -0.928  0.35368    \ntrend       -0.0012712  0.0022396  -0.568  0.57051    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.4232 on 626 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 1.403e+06 on 13 and 626 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation UNRATE: \n======================================= \nUNRATE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  3.780e-02  3.322e-02   1.138 0.255642    \nUNRATE.l1    3.118e-01  5.225e-02   5.967 4.05e-09 ***\nDSPIC96.l1   2.553e-04  7.281e-05   3.507 0.000486 ***\nPCE.l1      -4.257e-03  2.443e-04 -17.428  &lt; 2e-16 ***\nGS10.l1     -1.138e-01  5.492e-02  -2.072 0.038711 *  \nFEDFUNDS.l1 -7.030e-02  3.016e-02  -2.331 0.020081 *  \nCPIAUCSL.l2 -2.805e-02  3.333e-02  -0.841 0.400461    \nUNRATE.l2    6.658e-01  5.302e-02  12.558  &lt; 2e-16 ***\nDSPIC96.l2  -8.537e-05  6.888e-05  -1.239 0.215657    \nPCE.l2       4.201e-03  2.358e-04  17.816  &lt; 2e-16 ***\nGS10.l2      9.876e-02  5.495e-02   1.797 0.072785 .  \nFEDFUNDS.l2  9.394e-02  2.988e-02   3.144 0.001744 ** \nconst       -7.496e-01  2.925e-01  -2.563 0.010604 *  \ntrend       -5.475e-03  1.960e-03  -2.793 0.005374 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3704 on 626 degrees of freedom\nMultiple R-Squared: 0.9532, Adjusted R-squared: 0.9522 \nF-statistic: 980.7 on 13 and 626 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation DSPIC96: \n======================================== \nDSPIC96 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  -19.45821   16.87132  -1.153   0.2492    \nUNRATE.l1   -134.95087   26.53532  -5.086 4.85e-07 ***\nDSPIC96.l1     0.44313    0.03698  11.984  &lt; 2e-16 ***\nPCE.l1        -1.13145    0.12407  -9.120  &lt; 2e-16 ***\nGS10.l1        3.50497   27.89400   0.126   0.9000    \nFEDFUNDS.l1   -7.43573   15.31766  -0.485   0.6275    \nCPIAUCSL.l2   16.36520   16.92976   0.967   0.3341    \nUNRATE.l2    134.87420   26.92654   5.009 7.13e-07 ***\nDSPIC96.l2     0.48497    0.03498  13.864  &lt; 2e-16 ***\nPCE.l2         1.16933    0.11974   9.765  &lt; 2e-16 ***\nGS10.l2       -9.15256   27.91032  -0.328   0.7431    \nFEDFUNDS.l2   13.54166   15.17323   0.892   0.3725    \nconst        368.11628  148.53700   2.478   0.0135 *  \ntrend          1.88999    0.99539   1.899   0.0581 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 188.1 on 626 degrees of freedom\nMultiple R-Squared: 0.9977, Adjusted R-squared: 0.9976 \nF-statistic: 2.082e+04 on 13 and 626 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation PCE: \n==================================== \nPCE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1    4.83697    7.72100   0.626 0.531236    \nUNRATE.l1    152.54985   12.14365  12.562  &lt; 2e-16 ***\nDSPIC96.l1    -0.02646    0.01692  -1.563 0.118449    \nPCE.l1         1.61059    0.05678  28.366  &lt; 2e-16 ***\nGS10.l1       21.97184   12.76543   1.721 0.085709 .  \nFEDFUNDS.l1   10.72636    7.00999   1.530 0.126483    \nCPIAUCSL.l2   -3.98752    7.74775  -0.515 0.606967    \nUNRATE.l2   -146.96614   12.32268 -11.926  &lt; 2e-16 ***\nDSPIC96.l2     0.05725    0.01601   3.576 0.000376 ***\nPCE.l2        -0.62567    0.05480 -11.418  &lt; 2e-16 ***\nGS10.l2      -21.16124   12.77290  -1.657 0.098075 .  \nFEDFUNDS.l2  -12.19515    6.94389  -1.756 0.079536 .  \nconst       -167.34735   67.97660  -2.462 0.014091 *  \ntrend         -0.54154    0.45553  -1.189 0.234964    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 86.08 on 626 degrees of freedom\nMultiple R-Squared: 0.9997, Adjusted R-squared: 0.9997 \nF-statistic: 1.517e+05 on 13 and 626 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation GS10: \n===================================== \nGS10 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  6.550e-02  2.465e-02   2.657  0.00809 ** \nUNRATE.l1    7.839e-03  3.877e-02   0.202  0.83985    \nDSPIC96.l1  -2.594e-05  5.403e-05  -0.480  0.63134    \nPCE.l1       4.549e-05  1.813e-04   0.251  0.80194    \nGS10.l1      1.270e+00  4.076e-02  31.161  &lt; 2e-16 ***\nFEDFUNDS.l1 -8.618e-03  2.238e-02  -0.385  0.70032    \nCPIAUCSL.l2 -6.637e-02  2.474e-02  -2.683  0.00749 ** \nUNRATE.l2    6.638e-03  3.934e-02   0.169  0.86607    \nDSPIC96.l2  -5.303e-05  5.111e-05  -1.038  0.29989    \nPCE.l2      -2.424e-05  1.750e-04  -0.139  0.88985    \nGS10.l2     -3.264e-01  4.078e-02  -8.004 5.85e-15 ***\nFEDFUNDS.l2  3.381e-02  2.217e-02   1.525  0.12780    \nconst        4.882e-01  2.170e-01   2.249  0.02484 *  \ntrend        1.047e-03  1.454e-03   0.720  0.47177    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2748 on 626 degrees of freedom\nMultiple R-Squared: 0.9926, Adjusted R-squared: 0.9924 \nF-statistic:  6417 on 13 and 626 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation FEDFUNDS: \n========================================= \nFEDFUNDS = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1 -2.041e-02  4.121e-02  -0.495   0.6207    \nUNRATE.l1   -1.106e-01  6.482e-02  -1.706   0.0884 .  \nDSPIC96.l1   6.588e-05  9.033e-05   0.729   0.4661    \nPCE.l1      -3.047e-04  3.031e-04  -1.005   0.3152    \nGS10.l1      5.524e-01  6.814e-02   8.107 2.74e-15 ***\nFEDFUNDS.l1  1.266e+00  3.742e-02  33.828  &lt; 2e-16 ***\nCPIAUCSL.l2  1.813e-02  4.136e-02   0.438   0.6613    \nUNRATE.l2    7.872e-02  6.578e-02   1.197   0.2319    \nDSPIC96.l2  -2.405e-05  8.545e-05  -0.281   0.7785    \nPCE.l2       3.212e-04  2.925e-04   1.098   0.2726    \nGS10.l2     -4.702e-01  6.818e-02  -6.897 1.30e-11 ***\nFEDFUNDS.l2 -3.376e-01  3.707e-02  -9.108  &lt; 2e-16 ***\nconst        5.425e-02  3.629e-01   0.150   0.8812    \ntrend       -3.988e-04  2.432e-03  -0.164   0.8698    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.4595 on 626 degrees of freedom\nMultiple R-Squared: 0.9866, Adjusted R-squared: 0.9863 \nF-statistic:  3550 on 13 and 626 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         CPIAUCSL     UNRATE   DSPIC96      PCE      GS10 FEDFUNDS\nCPIAUCSL  0.17910  -0.016725    -7.566   10.897  0.016536  0.01143\nUNRATE   -0.01672   0.137164    13.924  -21.288 -0.006801 -0.02031\nDSPIC96  -7.56559  13.924086 35380.471 1741.521  1.640781 -1.27537\nPCE      10.89689 -21.288001  1741.521 7409.914  2.247045  4.56767\nGS10      0.01654  -0.006801     1.641    2.247  0.075540  0.03990\nFEDFUNDS  0.01143  -0.020314    -1.275    4.568  0.039898  0.21114\n\nCorrelation matrix of residuals:\n         CPIAUCSL   UNRATE  DSPIC96      PCE     GS10 FEDFUNDS\nCPIAUCSL  1.00000 -0.10671 -0.09504  0.29912  0.14217  0.05877\nUNRATE   -0.10671  1.00000  0.19988 -0.66774 -0.06682 -0.11937\nDSPIC96  -0.09504  0.19988  1.00000  0.10756  0.03174 -0.01476\nPCE       0.29912 -0.66774  0.10756  1.00000  0.09498  0.11548\nGS10      0.14217 -0.06682  0.03174  0.09498  1.00000  0.31592\nFEDFUNDS  0.05877 -0.11937 -0.01476  0.11548  0.31592  1.00000\n\n\nVAR(3) output:\n\nsummary(fit &lt;- VAR(var_ts2, p=3, type=\"both\"))\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: CPIAUCSL, UNRATE, DSPIC96, PCE, GS10, FEDFUNDS \nDeterministic variables: both \nSample size: 639 \nLog Likelihood: -8674.334 \nRoots of the characteristic polynomial:\n1.002 0.9966 0.9822 0.9673 0.8943 0.8943 0.5423 0.5423 0.5056 0.5056 0.4748 0.4748 0.4181 0.4181 0.244 0.244 0.07907 0.06819\nCall:\nVAR(y = var_ts2, p = 3, type = \"both\")\n\n\nEstimation results for equation CPIAUCSL: \n========================================= \nCPIAUCSL = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  1.455e+00  4.345e-02  33.483  &lt; 2e-16 ***\nUNRATE.l1    2.048e-01  6.736e-02   3.041  0.00246 ** \nDSPIC96.l1  -3.795e-05  9.929e-05  -0.382  0.70245    \nPCE.l1       1.214e-03  2.990e-04   4.059 5.56e-05 ***\nGS10.l1      1.449e-01  6.554e-02   2.211  0.02743 *  \nFEDFUNDS.l1  5.266e-02  3.901e-02   1.350  0.17754    \nCPIAUCSL.l2 -5.571e-01  7.115e-02  -7.830 2.13e-14 ***\nUNRATE.l2   -2.003e-01  8.964e-02  -2.234  0.02581 *  \nDSPIC96.l2   6.915e-05  9.327e-05   0.741  0.45877    \nPCE.l2      -1.008e-03  4.604e-04  -2.189  0.02900 *  \nGS10.l2     -1.549e-01  1.006e-01  -1.540  0.12406    \nFEDFUNDS.l2 -4.306e-03  6.025e-02  -0.071  0.94304    \nCPIAUCSL.l3  1.032e-01  4.211e-02   2.451  0.01452 *  \nUNRATE.l3   -5.081e-03  6.993e-02  -0.073  0.94210    \nDSPIC96.l3   6.730e-05  9.161e-05   0.735  0.46285    \nPCE.l3      -1.957e-04  3.403e-04  -0.575  0.56531    \nGS10.l3      2.370e-02  6.698e-02   0.354  0.72355    \nFEDFUNDS.l3 -3.266e-02  3.706e-02  -0.882  0.37838    \nconst       -3.892e-01  3.517e-01  -1.107  0.26884    \ntrend       -2.102e-03  2.339e-03  -0.899  0.36922    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.423 on 619 degrees of freedom\nMultiple R-Squared:     1,  Adjusted R-squared:     1 \nF-statistic: 9.571e+05 on 19 and 619 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation UNRATE: \n======================================= \nUNRATE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  7.876e-02  3.753e-02   2.099  0.03624 *  \nUNRATE.l1    2.777e-01  5.818e-02   4.774 2.26e-06 ***\nDSPIC96.l1   3.058e-04  8.575e-05   3.566  0.00039 ***\nPCE.l1      -4.667e-03  2.583e-04 -18.068  &lt; 2e-16 ***\nGS10.l1     -1.161e-01  5.660e-02  -2.051  0.04072 *  \nFEDFUNDS.l1 -4.357e-02  3.370e-02  -1.293  0.19643    \nCPIAUCSL.l2 -1.153e-01  6.145e-02  -1.876  0.06113 .  \nUNRATE.l2    9.307e-01  7.742e-02  12.022  &lt; 2e-16 ***\nDSPIC96.l2  -2.363e-04  8.056e-05  -2.933  0.00348 ** \nPCE.l2       5.164e-03  3.976e-04  12.987  &lt; 2e-16 ***\nGS10.l2      1.165e-01  8.685e-02   1.342  0.18024    \nFEDFUNDS.l2  3.070e-02  5.204e-02   0.590  0.55544    \nCPIAUCSL.l3  4.385e-02  3.637e-02   1.206  0.22838    \nUNRATE.l3   -2.365e-01  6.039e-02  -3.916  0.00010 ***\nDSPIC96.l3   5.552e-05  7.912e-05   0.702  0.48314    \nPCE.l3      -5.290e-04  2.939e-04  -1.800  0.07235 .  \nGS10.l3     -9.171e-03  5.785e-02  -0.159  0.87408    \nFEDFUNDS.l3  3.308e-02  3.200e-02   1.034  0.30166    \nconst       -5.142e-01  3.037e-01  -1.693  0.09093 .  \ntrend       -4.222e-03  2.020e-03  -2.090  0.03705 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.3653 on 619 degrees of freedom\nMultiple R-Squared: 0.9549, Adjusted R-squared: 0.9535 \nF-statistic: 689.6 on 19 and 619 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation DSPIC96: \n======================================== \nDSPIC96 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1   -2.84105   19.15148  -0.148 0.882118    \nUNRATE.l1   -175.64362   29.69048  -5.916 5.47e-09 ***\nDSPIC96.l1     0.38830    0.04376   8.873  &lt; 2e-16 ***\nPCE.l1        -1.19554    0.13181  -9.070  &lt; 2e-16 ***\nGS10.l1       11.43483   28.88638   0.396 0.692348    \nFEDFUNDS.l1  -12.30414   17.19537  -0.716 0.474539    \nCPIAUCSL.l2  -15.35811   31.35946  -0.490 0.624489    \nUNRATE.l2    134.52057   39.50940   3.405 0.000705 ***\nDSPIC96.l2     0.49485    0.04111  12.037  &lt; 2e-16 ***\nPCE.l2         0.75594    0.20292   3.725 0.000213 ***\nGS10.l2      -16.45059   44.32124  -0.371 0.710640    \nFEDFUNDS.l2   17.17192   26.55585   0.647 0.518108    \nCPIAUCSL.l3   16.39397   18.56128   0.883 0.377452    \nUNRATE.l3     39.16077   30.82043   1.271 0.204344    \nDSPIC96.l3     0.07422    0.04038   1.838 0.066492 .  \nPCE.l3         0.46848    0.14997   3.124 0.001869 ** \nGS10.l3        2.12261   29.52092   0.072 0.942703    \nFEDFUNDS.l3    0.61079   16.33225   0.037 0.970180    \nconst        229.41415  154.99244   1.480 0.139338    \ntrend          1.08535    1.03104   1.053 0.292900    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 186.4 on 619 degrees of freedom\nMultiple R-Squared: 0.9978, Adjusted R-squared: 0.9977 \nF-statistic: 1.445e+04 on 19 and 619 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation PCE: \n==================================== \nPCE = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1   10.31227    8.80046   1.172 0.241734    \nUNRATE.l1    134.95588   13.64333   9.892  &lt; 2e-16 ***\nDSPIC96.l1    -0.04224    0.02011  -2.101 0.036073 *  \nPCE.l1         1.57997    0.06057  26.086  &lt; 2e-16 ***\nGS10.l1       25.32738   13.27383   1.908 0.056845 .  \nFEDFUNDS.l1    8.18212    7.90159   1.036 0.300839    \nCPIAUCSL.l2  -10.24779   14.41026  -0.711 0.477262    \nUNRATE.l2   -143.31706   18.15531  -7.894 1.34e-14 ***\nDSPIC96.l2     0.06398    0.01889   3.387 0.000751 ***\nPCE.l2        -0.78580    0.09324  -8.427 2.48e-16 ***\nGS10.l2      -29.36288   20.36644  -1.442 0.149885    \nFEDFUNDS.l2   -6.73688   12.20291  -0.552 0.581099    \nCPIAUCSL.l3    1.17986    8.52926   0.138 0.890024    \nUNRATE.l3     13.17155   14.16257   0.930 0.352720    \nDSPIC96.l3     0.01923    0.01855   1.036 0.300392    \nPCE.l3         0.18779    0.06892   2.725 0.006613 ** \nGS10.l3        6.21115   13.56542   0.458 0.647209    \nFEDFUNDS.l3   -3.55427    7.50498  -0.474 0.635961    \nconst       -213.76325   71.22194  -3.001 0.002796 ** \ntrend         -0.80677    0.47378  -1.703 0.089100 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 85.67 on 619 degrees of freedom\nMultiple R-Squared: 0.9997, Adjusted R-squared: 0.9997 \nF-statistic: 1.045e+05 on 19 and 619 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation GS10: \n===================================== \nGS10 = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  1.057e-01  2.760e-02   3.831 0.000141 ***\nUNRATE.l1   -3.024e-02  4.278e-02  -0.707 0.479956    \nDSPIC96.l1   3.271e-05  6.306e-05   0.519 0.604131    \nPCE.l1      -6.582e-05  1.899e-04  -0.347 0.729053    \nGS10.l1      1.318e+00  4.162e-02  31.658  &lt; 2e-16 ***\nFEDFUNDS.l1  1.692e-02  2.478e-02   0.683 0.494916    \nCPIAUCSL.l2 -1.467e-01  4.519e-02  -3.246 0.001232 ** \nUNRATE.l2    2.460e-02  5.693e-02   0.432 0.665769    \nDSPIC96.l2  -2.008e-05  5.924e-05  -0.339 0.734755    \nPCE.l2       1.131e-05  2.924e-04   0.039 0.969154    \nGS10.l2     -5.874e-01  6.386e-02  -9.197  &lt; 2e-16 ***\nFEDFUNDS.l2  1.828e-02  3.827e-02   0.478 0.632935    \nCPIAUCSL.l3  3.899e-02  2.675e-02   1.458 0.145383    \nUNRATE.l3    1.759e-02  4.441e-02   0.396 0.692251    \nDSPIC96.l3  -9.884e-05  5.818e-05  -1.699 0.089843 .  \nPCE.l3       8.885e-05  2.161e-04   0.411 0.681090    \nGS10.l3      2.230e-01  4.254e-02   5.242 2.18e-07 ***\nFEDFUNDS.l3 -1.294e-02  2.353e-02  -0.550 0.582740    \nconst        5.077e-01  2.233e-01   2.273 0.023345 *  \ntrend        1.378e-03  1.486e-03   0.928 0.353938    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2686 on 619 degrees of freedom\nMultiple R-Squared: 0.993,  Adjusted R-squared: 0.9927 \nF-statistic:  4597 on 19 and 619 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation FEDFUNDS: \n========================================= \nFEDFUNDS = CPIAUCSL.l1 + UNRATE.l1 + DSPIC96.l1 + PCE.l1 + GS10.l1 + FEDFUNDS.l1 + CPIAUCSL.l2 + UNRATE.l2 + DSPIC96.l2 + PCE.l2 + GS10.l2 + FEDFUNDS.l2 + CPIAUCSL.l3 + UNRATE.l3 + DSPIC96.l3 + PCE.l3 + GS10.l3 + FEDFUNDS.l3 + const + trend \n\n              Estimate Std. Error t value Pr(&gt;|t|)    \nCPIAUCSL.l1  2.062e-02  4.670e-02   0.442  0.65889    \nUNRATE.l1   -1.674e-01  7.239e-02  -2.312  0.02110 *  \nDSPIC96.l1   1.188e-04  1.067e-04   1.114  0.26582    \nPCE.l1      -5.239e-04  3.214e-04  -1.630  0.10360    \nGS10.l1      5.419e-01  7.043e-02   7.693 5.68e-14 ***\nFEDFUNDS.l1  1.339e+00  4.193e-02  31.941  &lt; 2e-16 ***\nCPIAUCSL.l2 -4.525e-02  7.646e-02  -0.592  0.55423    \nUNRATE.l2    1.624e-01  9.633e-02   1.686  0.09225 .  \nDSPIC96.l2  -3.545e-05  1.002e-04  -0.354  0.72373    \nPCE.l2       4.968e-04  4.948e-04   1.004  0.31575    \nGS10.l2     -5.942e-01  1.081e-01  -5.499 5.60e-08 ***\nFEDFUNDS.l2 -5.150e-01  6.475e-02  -7.953 8.68e-15 ***\nCPIAUCSL.l3  2.110e-02  4.526e-02   0.466  0.64114    \nUNRATE.l3   -2.912e-02  7.515e-02  -0.388  0.69850    \nDSPIC96.l3  -6.282e-05  9.845e-05  -0.638  0.52365    \nPCE.l3       6.049e-05  3.657e-04   0.165  0.86866    \nGS10.l3      1.297e-01  7.198e-02   1.802  0.07208 .  \nFEDFUNDS.l3  1.110e-01  3.982e-02   2.786  0.00549 ** \nconst        1.694e-01  3.779e-01   0.448  0.65413    \ntrend        1.320e-04  2.514e-03   0.053  0.95814    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.4546 on 619 degrees of freedom\nMultiple R-Squared: 0.987,  Adjusted R-squared: 0.9866 \nF-statistic:  2481 on 19 and 619 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         CPIAUCSL     UNRATE   DSPIC96      PCE      GS10 FEDFUNDS\nCPIAUCSL  0.17893  -0.016960    -7.857   11.013  0.015729  0.01174\nUNRATE   -0.01696   0.133468    13.591  -21.714 -0.007292 -0.02283\nDSPIC96  -7.85739  13.590859 34759.020 1384.812  1.438151 -1.60584\nPCE      11.01265 -21.714480  1384.812 7339.628  2.122663  4.45476\nGS10      0.01573  -0.007292     1.438    2.123  0.072170  0.03731\nFEDFUNDS  0.01174  -0.022825    -1.606    4.455  0.037308  0.20665\n\nCorrelation matrix of residuals:\n         CPIAUCSL  UNRATE  DSPIC96      PCE     GS10 FEDFUNDS\nCPIAUCSL  1.00000 -0.1098 -0.09963  0.30389  0.13841  0.06107\nUNRATE   -0.10975  1.0000  0.19954 -0.69378 -0.07430 -0.13744\nDSPIC96  -0.09963  0.1995  1.00000  0.08670  0.02871 -0.01895\nPCE       0.30389 -0.6938  0.08670  1.00000  0.09223  0.11439\nGS10      0.13841 -0.0743  0.02871  0.09223  1.00000  0.30550\nFEDFUNDS  0.06107 -0.1374 -0.01895  0.11439  0.30550  1.00000\n\n\n\n\nK-Fold Cross Validation and Model Diagnostics\n\n# Define the number of folds for cross-validation\nk &lt;- 5\n\n# Define the p values to test\np_values &lt;- c(1, 2, 3)\n\n# Split the data into k folds\ncv_folds &lt;- cut(seq(1, nrow(var_ts2)), breaks = k, labels = FALSE)\n\n# Initialize vectors to store RMSE and AIC values for each p value\nrmse_vec &lt;- numeric(length(p_values))\naic_vec &lt;- numeric(length(p_values))\n\n# Loop over p values and perform cross-validation\nfor (i in seq_along(p_values)) {\n  p &lt;- p_values[i]\n  rmse_cv &lt;- numeric(k)\n  aic_cv &lt;- numeric(k)\n  for (j in 1:k) {\n    # Split the data into training and testing sets\n    train &lt;- var_ts2[cv_folds != j, ]\n    test &lt;- var_ts2[cv_folds == j, ]\n    \n    # Fit the VAR model with the current p value\n    var_fit &lt;- VAR(train, p = p)\n    \n    # Make predictions for the testing set\n    pred &lt;- predict(var_fit, n.ahead = nrow(test))$fcst\n    \n    # Calculate RMSE and AIC for the current fold\n    rmse_cv[j] &lt;- sqrt(mean((pred$CPIAUCSL - test[,1])^2))\n    aic_cv[j] &lt;- AIC(var_fit)\n  }\n  # Calculate the mean RMSE and AIC across all folds for the current p value\n  rmse_vec[i] &lt;- mean(rmse_cv)\n  aic_vec[i] &lt;- mean(aic_cv)\n}\n\n# Create a table of RMSE and AIC values for each p value\nresults_table &lt;- tibble(p_values, rmse_vec, aic_vec)\n\n# Print the results table\nkable(results_table, format = \"markdown\", \n        col.names = c(\"P Values\", \"Mean RMSE (5 Folds)\", \"Mean AIC (5 Folds)\"), align = \"c\", digits = 2\n        )\n\n\n\n\nP Values\nMean RMSE (5 Folds)\nMean AIC (5 Folds)\n\n\n\n\n1\n214.07\n15472.03\n\n\n2\n197.77\n14821.86\n\n\n3\n202.50\n14714.38\n\n\n\n\n\nThe VAR(2) model outputs the lowest Mean RMSE of 197.84 from the 5-fold cross validation. However, it has the highest AIC score. Because predictive performance is best and it is the simplest model, we shall choose the VAR(2) model as the best option.\n\n\nForecasting the chosen model (P=2)\n\nfinal_var &lt;- VAR(var_ts2, p = 2)\n\n(fit.pr = predict(final_var, n.ahead = 5, ci = 0.95)) # 5 years ahead \n\n$CPIAUCSL\n         fcst    lower    upper        CI\n[1,] 304.5980 303.7690 305.4270 0.8290186\n[2,] 305.3897 303.8877 306.8916 1.5019366\n[3,] 306.2069 304.1124 308.3014 2.0945028\n[4,] 307.0482 304.4439 309.6525 2.6043236\n[5,] 307.9016 304.8500 310.9531 3.0515667\n\n$UNRATE\n         fcst    lower    upper       CI\n[1,] 3.668710 2.938897 4.398523 0.729813\n[2,] 3.896853 2.746868 5.046838 1.149985\n[3,] 4.091349 2.702882 5.479815 1.388466\n[4,] 4.241375 2.694821 5.787928 1.546553\n[5,] 4.385280 2.700831 6.069729 1.684449\n\n$DSPIC96\n         fcst    lower    upper       CI\n[1,] 16925.05 16555.62 17294.47 369.4284\n[2,] 17011.23 16589.17 17433.30 422.0644\n[3,] 17104.42 16609.96 17598.88 494.4611\n[4,] 17175.07 16644.95 17705.19 530.1203\n[5,] 17252.02 16681.60 17822.45 570.4264\n\n$PCE\n         fcst    lower    upper       CI\n[1,] 18508.30 18339.53 18677.07 168.7710\n[2,] 18515.66 18239.26 18792.06 276.4016\n[3,] 18540.34 18202.74 18877.95 337.6041\n[4,] 18572.68 18194.11 18951.25 378.5740\n[5,] 18606.08 18190.87 19021.29 415.2084\n\n$GS10\n         fcst    lower    upper        CI\n[1,] 3.806431 3.267952 4.344910 0.5384791\n[2,] 3.832180 2.955379 4.708980 0.8768005\n[3,] 3.844942 2.710557 4.979328 1.1343854\n[4,] 3.852638 2.512646 5.192629 1.3399920\n[5,] 3.856427 2.344649 5.368205 1.5117781\n\n$FEDFUNDS\n         fcst    lower    upper        CI\n[1,] 5.076313 4.176405 5.976221 0.8999079\n[2,] 5.015310 3.460506 6.570113 1.5548037\n[3,] 4.911707 2.823874 6.999540 2.0878330\n[4,] 4.787958 2.272402 7.303515 2.5155566\n[5,] 4.660179 1.798935 7.521422 2.8612435\n\nfanchart(fit.pr) # plot prediction + error\n\n\n\n\nThe above plot showcases the forecasts for each variable present in the VAR(2) model, Yearly Inflation, Unemployment, Personal Consumption, Treasury Yield and Federal Funds Rate. The above plot showcases the forecasts for each variable present in the VAR(3) model, Yearly Inflation, Unemployment, Personal Consumption, Treasury Yield and Federal Funds Rate. The predicted forecast, from the years 2021 to 2025, for CPI is a good sign for the US due to the rising trend.\nLet us visualize more closely the forecasts for the CPI from 2021 to 2025, corresponding to the VAR(2) model fitted on Pre COVID-19 and post-COVID-19 years (2001-2020):\n\ndf_fvar_attack &lt;- as.data.frame(fit.pr$fcst$CPIAUCSL)\n# add year column\ndf_fvar_attack$Year &lt;- c(\"2021\", \"2022\", \"2023\", \"2024\", \"2025\")\n(var_plot &lt;- ggplot(data=df_fvar_attack, aes(x=Year, y=fcst, group = 1)) +\n    geom_line(aes(color=\"Forecast\"), linewidth=1) +\n    geom_ribbon(aes(ymin=lower, ymax=upper, fill=\"Confidence Interval\"), alpha=0.1) +\n    labs(title=\"VAR(3) Forecasts for CPI from 2021 to 2025\",\n         y=\"CPI\",\n         color=\"\", fill=\"\",\n         caption=\"Data Sources: FRED\") +\n    scale_color_manual(values = c(\"Forecast\"=\"red\")) +\n    scale_fill_manual(values = c(\"95% Confidence Interval\"=\"steelblue\")) +\n    theme_minimal() + \n  theme(plot.caption.position = \"plot\"))"
  },
  {
    "objectID": "arimax-sarimax-var.html#manual-arimax-modelling-1970-2020-pre-covid",
    "href": "arimax-sarimax-var.html#manual-arimax-modelling-1970-2020-pre-covid",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual ARIMAX Modelling (1970-2020: Pre COVID)",
    "text": "Manual ARIMAX Modelling (1970-2020: Pre COVID)\nThe ARIMAX model being analyzed in this section is:\nCPI ~ Unemployment Rate + Disposable Income + Personal Consumption + Treasury Yield + Federal Funds Rate"
  },
  {
    "objectID": "arimax-sarimax-var.html#regression-summary-and-fitting-arima-to-residuals",
    "href": "arimax-sarimax-var.html#regression-summary-and-fitting-arima-to-residuals",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Regression Summary and Fitting ARIMA to Residuals",
    "text": "Regression Summary and Fitting ARIMA to Residuals\n\nfit.reg &lt;- lm(CPIAUCSL ~ ., data =var_ts1)\nsummary(fit.reg)\n\n\nCall:\nlm(formula = CPIAUCSL ~ ., data = var_ts1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.2129  -6.5241  -0.9476   5.3942  22.4885 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.175e+02  5.669e+00 -20.732  &lt; 2e-16 ***\nUNRATE       1.303e+00  2.764e-01   4.713 3.05e-06 ***\nDSPIC96      3.523e-02  1.272e-03  27.691  &lt; 2e-16 ***\nPCE         -1.216e-02  1.071e-03 -11.350  &lt; 2e-16 ***\nGS10         3.938e+00  3.860e-01  10.200  &lt; 2e-16 ***\nFEDFUNDS    -1.170e+00  2.574e-01  -4.545 6.65e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.533 on 595 degrees of freedom\nMultiple R-squared:  0.9835,    Adjusted R-squared:  0.9834 \nF-statistic:  7096 on 5 and 595 DF,  p-value: &lt; 2.2e-16\n\n\n\nres.fit&lt;-ts(residuals(fit.reg),star=decimal_date(as.Date(\"1970-01-01\",format = \"%Y-%m-%d\")),frequency = 1)\n\n# Then look at the residuals \nres.fit %&gt;% ggtsdisplay() # no need to difference\n\n\n\n\n\n#q=1,3 Q=1 , p=1,2, P=1,2\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){\n  \n  temp=c()\n  d=0\n  D=0\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*44),nrow=44)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q&lt;=8)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n##q=1,3 Q=0 p=1,2 P=0 d=0\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=0,data=res.fit)\noutput\n\n   p d q P D Q      AIC      BIC     AICc\n1  0 0 0 0 0 0 4280.507 4289.305 4280.527\n2  0 0 0 0 0 1 4280.507 4289.305 4280.527\n3  0 0 0 1 0 0 4280.507 4289.305 4280.527\n4  0 0 0 1 0 1 4280.507 4289.305 4280.527\n5  0 0 0 2 0 0 4280.507 4289.305 4280.527\n6  0 0 0 2 0 1 4280.507 4289.305 4280.527\n7  0 0 1 0 0 0 3716.284 3729.480 3716.324\n8  0 0 1 0 0 1 3716.284 3729.480 3716.324\n9  0 0 1 1 0 0 3716.284 3729.480 3716.324\n10 0 0 1 1 0 1 3716.284 3729.480 3716.324\n11 0 0 1 2 0 0 3716.284 3729.480 3716.324\n12 0 0 1 2 0 1 3716.284 3729.480 3716.324\n13 0 0 2 0 0 0 3435.150 3452.744 3435.217\n14 0 0 2 0 0 1 3435.150 3452.744 3435.217\n15 0 0 2 1 0 0 3435.150 3452.744 3435.217\n16 0 0 2 1 0 1 3435.150 3452.744 3435.217\n17 0 0 2 2 0 0 3435.150 3452.744 3435.217\n18 0 0 3 0 0 0 3276.820 3298.813 3276.921\n19 0 0 3 0 0 1 3276.820 3298.813 3276.921\n20 0 0 3 1 0 0 3276.820 3298.813 3276.921\n21 1 0 0 0 0 0 2942.581 2955.776 2942.621\n22 1 0 0 0 0 1 2942.581 2955.776 2942.621\n23 1 0 0 1 0 0 2942.581 2955.776 2942.621\n24 1 0 0 1 0 1 2942.581 2955.776 2942.621\n25 1 0 0 2 0 0 2942.581 2955.776 2942.621\n26 1 0 0 2 0 1 2942.581 2955.776 2942.621\n27 1 0 1 0 0 0 2939.053 2956.648 2939.121\n28 1 0 1 0 0 1 2939.053 2956.648 2939.121\n29 1 0 1 1 0 0 2939.053 2956.648 2939.121\n30 1 0 1 1 0 1 2939.053 2956.648 2939.121\n31 1 0 1 2 0 0 2939.053 2956.648 2939.121\n32 1 0 2 0 0 0 2924.167 2946.160 2924.268\n33 1 0 2 0 0 1 2924.167 2946.160 2924.268\n34 1 0 2 1 0 0 2924.167 2946.160 2924.268\n35 1 0 3 0 0 0 2923.576 2949.968 2923.718\n36 2 0 0 0 0 0 2940.801 2958.395 2940.868\n37 2 0 0 0 0 1 2940.801 2958.395 2940.868\n38 2 0 0 1 0 0 2940.801 2958.395 2940.868\n39 2 0 0 1 0 1 2940.801 2958.395 2940.868\n40 2 0 0 2 0 0 2940.801 2958.395 2940.868\n41 2 0 1 0 0 0 2926.113 2948.106 2926.214\n42 2 0 1 0 0 1 2926.113 2948.106 2926.214\n43 2 0 1 1 0 0 2926.113 2948.106 2926.214\n44 2 0 2 0 0 0 2923.837 2950.229 2923.979\n\n\n\nARIMA(1,0,3)\n\noutput[which.min(output$AIC),] \n\n   p d q P D Q      AIC      BIC     AICc\n35 1 0 3 0 0 0 2923.576 2949.968 2923.718\n\noutput[which.min(output$BIC),]\n\n   p d q P D Q      AIC     BIC     AICc\n32 1 0 2 0 0 0 2924.167 2946.16 2924.268\n\noutput[which.min(output$AICc),]\n\n   p d q P D Q      AIC      BIC     AICc\n35 1 0 3 0 0 0 2923.576 2949.968 2923.718\n\n\n\n\nModel Diagnostics\n\nmodel_outputar2 &lt;- capture.output(sarima(res.fit, 1,0,3, 0,0,0))\n\n\n\ncat(model_outputar2[57:89], model_outputar2[length(model_outputar2)], sep = \"\\n\")\n\niter  27 value 1.003339\niter  28 value 1.003338\niter  29 value 1.003338\niter  30 value 1.003338\niter  30 value 1.003338\niter  30 value 1.003338\nfinal  value 1.003338 \nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1      ma2      ma3   xmean\n      0.9784  -0.1445  -0.1657  -0.0644  -0.557\ns.e.  0.0091   0.0418   0.0428   0.0400   3.008\n\nsigma^2 estimated as 7.409:  log likelihood = -1455.79,  aic = 2923.58\n\n$degrees_of_freedom\n[1] 596\n\n$ttable\n      Estimate     SE  t.value p.value\nar1     0.9784 0.0091 107.3409  0.0000\nma1    -0.1445 0.0418  -3.4592  0.0006\nma2    -0.1657 0.0428  -3.8684  0.0001\nma3    -0.0644 0.0400  -1.6100  0.1079\nxmean  -0.5570 3.0080  -0.1852  0.8532\n\n\n\narimaModel_1 &lt;- arima(res.fit, order = c(1,0,3))\nforecast1=predict(arimaModel_1, 5)\n# create df with fcast preds and +-1.96 SE for 95% CI Bands\nfarimax_df &lt;- data.frame(\n  Year = 2021:2025,\n  fcst = as.numeric(forecast1$pred),\n  lower = as.numeric(forecast1$pred - 1.96 * forecast1$se),\n  upper = as.numeric(forecast1$pred + 1.96 * forecast1$se)\n)\n\n\n(arimax_plot &lt;- ggplot(data=farimax_df, aes(x=Year, y=fcst, group = 1)) +\n    geom_line(aes(color=\"Forecast\"), linewidth=1) +\n    geom_ribbon(aes(ymin=lower, ymax=upper, fill=\"Confidence Interval\"), alpha=0.1) +\n    labs(title=\"ARIMA(1,0,3) Forecasts for CPI from 2021 to 2025\",\n         y=\"CPI\",\n         color=\"\", fill=\"\",\n         caption=\"Data Sources: FRED\") +\n    scale_color_manual(values = c(\"Forecast\"=\"red\")) +\n    scale_fill_manual(values = c(\"95% Confidence Interval\"=\"steelblue\")) +\n    theme_minimal() + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(size=8)))"
  },
  {
    "objectID": "arimax-sarimax-var.html#regression-summary-and-fitting-arima-to-residuals-1",
    "href": "arimax-sarimax-var.html#regression-summary-and-fitting-arima-to-residuals-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Regression Summary and Fitting ARIMA to Residuals",
    "text": "Regression Summary and Fitting ARIMA to Residuals\n\nfit.reg &lt;- lm(CPIAUCSL ~ ., data =var_ts2)\nsummary(fit.reg)\n\n\nCall:\nlm(formula = CPIAUCSL ~ ., data = var_ts2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.110  -8.310   1.465   8.365  23.493 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.943e+01  4.893e+00  -8.057 3.86e-15 ***\nUNRATE      -1.008e+00  3.170e-01  -3.179  0.00155 ** \nDSPIC96      1.763e-02  8.892e-04  19.828  &lt; 2e-16 ***\nPCE          2.064e-03  7.002e-04   2.948  0.00331 ** \nGS10         6.466e+00  4.687e-01  13.797  &lt; 2e-16 ***\nFEDFUNDS    -2.986e+00  3.140e-01  -9.510  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.43 on 636 degrees of freedom\nMultiple R-squared:  0.9748,    Adjusted R-squared:  0.9746 \nF-statistic:  4919 on 5 and 636 DF,  p-value: &lt; 2.2e-16\n\n\n\nres.fit&lt;-ts(residuals(fit.reg),star=decimal_date(as.Date(\"1970-01-01\",format = \"%Y-%m-%d\")),frequency = 1)\n\n# Then look at the residuals \nres.fit %&gt;% ggtsdisplay() # no need to difference\n\n\n\n\n\n#q=1,3 Q=1 , p=1,2, P=1,2\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){\n  \n  temp=c()\n  d=0\n  D=0\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*44),nrow=44)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q&lt;=8)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n##q=1,2 Q=0 , p=1,2 P=0 d=0\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=0,data=res.fit)\noutput\n\n   p d q P D Q      AIC      BIC     AICc\n1  0 0 0 0 0 0 4947.866 4956.796 4947.885\n2  0 0 0 0 0 1 4947.866 4956.796 4947.885\n3  0 0 0 1 0 0 4947.866 4956.796 4947.885\n4  0 0 0 1 0 1 4947.866 4956.796 4947.885\n5  0 0 0 2 0 0 4947.866 4956.796 4947.885\n6  0 0 0 2 0 1 4947.866 4956.796 4947.885\n7  0 0 1 0 0 0 4496.989 4510.383 4497.027\n8  0 0 1 0 0 1 4496.989 4510.383 4497.027\n9  0 0 1 1 0 0 4496.989 4510.383 4497.027\n10 0 0 1 1 0 1 4496.989 4510.383 4497.027\n11 0 0 1 2 0 0 4496.989 4510.383 4497.027\n12 0 0 1 2 0 1 4496.989 4510.383 4497.027\n13 0 0 2 0 0 0 4203.872 4221.731 4203.935\n14 0 0 2 0 0 1 4203.872 4221.731 4203.935\n15 0 0 2 1 0 0 4203.872 4221.731 4203.935\n16 0 0 2 1 0 1 4203.872 4221.731 4203.935\n17 0 0 2 2 0 0 4203.872 4221.731 4203.935\n18 0 0 3 0 0 0 4039.712 4062.035 4039.807\n19 0 0 3 0 0 1 4039.712 4062.035 4039.807\n20 0 0 3 1 0 0 4039.712 4062.035 4039.807\n21 1 0 0 0 0 0 3812.180 3825.574 3812.218\n22 1 0 0 0 0 1 3812.180 3825.574 3812.218\n23 1 0 0 1 0 0 3812.180 3825.574 3812.218\n24 1 0 0 1 0 1 3812.180 3825.574 3812.218\n25 1 0 0 2 0 0 3812.180 3825.574 3812.218\n26 1 0 0 2 0 1 3812.180 3825.574 3812.218\n27 1 0 1 0 0 0 3714.614 3732.473 3714.677\n28 1 0 1 0 0 1 3714.614 3732.473 3714.677\n29 1 0 1 1 0 0 3714.614 3732.473 3714.677\n30 1 0 1 1 0 1 3714.614 3732.473 3714.677\n31 1 0 1 2 0 0 3714.614 3732.473 3714.677\n32 1 0 2 0 0 0 3712.704 3735.027 3712.799\n33 1 0 2 0 0 1 3712.704 3735.027 3712.799\n34 1 0 2 1 0 0 3712.704 3735.027 3712.799\n35 1 0 3 0 0 0 3692.626 3719.414 3692.758\n36 2 0 0 0 0 0 3703.861 3721.719 3703.924\n37 2 0 0 0 0 1 3703.861 3721.719 3703.924\n38 2 0 0 1 0 0 3703.861 3721.719 3703.924\n39 2 0 0 1 0 1 3703.861 3721.719 3703.924\n40 2 0 0 2 0 0 3703.861 3721.719 3703.924\n41 2 0 1 0 0 0 3705.857 3728.180 3705.952\n42 2 0 1 0 0 1 3705.857 3728.180 3705.952\n43 2 0 1 1 0 0 3705.857 3728.180 3705.952\n44 2 0 2 0 0 0 3707.542 3734.330 3707.675\n\n\n\n\n\noutput[which.min(output$AIC),] \n\n   p d q P D Q      AIC      BIC     AICc\n35 1 0 3 0 0 0 3692.626 3719.414 3692.758\n\noutput[which.min(output$BIC),]\n\n   p d q P D Q      AIC      BIC     AICc\n35 1 0 3 0 0 0 3692.626 3719.414 3692.758\n\noutput[which.min(output$AICc),]\n\n   p d q P D Q      AIC      BIC     AICc\n35 1 0 3 0 0 0 3692.626 3719.414 3692.758\n\nmodel_outputma2 &lt;- capture.output(sarima(res.fit, 1,0,3,0,0,0))\n\n\n\ncat(model_outputma2[57:89], model_outputma2[length(model_outputar2)], sep = \"\\n\")\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     ma2     ma3    xmean\n      0.9771  -0.4537  0.1864  -0.196  -0.2391\ns.e.  0.0091   0.0402  0.0458   0.041   3.6951\n\nsigma^2 estimated as 18.02:  log likelihood = -1840.31,  aic = 3692.63\n\n$degrees_of_freedom\n[1] 637\n\n$ttable\n      Estimate     SE  t.value p.value\nar1     0.9771 0.0091 107.5191  0.0000\nma1    -0.4537 0.0402 -11.2978  0.0000\nma2     0.1864 0.0458   4.0690  0.0001\nma3    -0.1960 0.0410  -4.7836  0.0000\nxmean  -0.2391 3.6951  -0.0647  0.9484\n\n$AIC\n[1] 5.751754\n\n$AICc\n[1] 5.751901\n\n$BIC\n[1] 5.793479\nNA\n\n\n\narimaModel_1 &lt;- arima(res.fit, order = c(1,0,3))\nforecast1=predict(arimaModel_1, 5)\n# create df with fcast preds and +-1.96 SE for 95% CI Bands\nfarimax_df &lt;- data.frame(\n  Year = 2021:2025,\n  fcst = as.numeric(forecast1$pred),\n  lower = as.numeric(forecast1$pred - 1.96 * forecast1$se),\n  upper = as.numeric(forecast1$pred + 1.96 * forecast1$se)\n)\n\n#plot(forecast1$pred, main = \"ARIMA(2,0,0) Forecast For 5 Years\", xlab = \"Time\", ylab = \"Values\", col = \"red\")\n(arimax_plot &lt;- ggplot(data=farimax_df, aes(x=Year, y=fcst, group = 1)) +\n    geom_line(aes(color=\"Forecast\"), linewidth=1) +\n    geom_ribbon(aes(ymin=lower, ymax=upper, fill=\"Confidence Interval\"), alpha=0.1) +\n    labs(title=\"ARIMA(1,0,3) Forecasts for CPI from 2021 to 2025\",\n         y=\"CPI\",\n         color=\"\", fill=\"\",\n         caption=\"Data Sources: FRED\") +\n    scale_color_manual(values = c(\"Forecast\"=\"red\")) +\n    scale_fill_manual(values = c(\"95% Confidence Interval\"=\"steelblue\")) +\n    theme_minimal() + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(size=8)))"
  },
  {
    "objectID": "arimax-sarimax-var.html#final-results-unveiling-the-impact-of-post-covid-data-on-economic-forecasting-insights-from-var2-and-arimax-models",
    "href": "arimax-sarimax-var.html#final-results-unveiling-the-impact-of-post-covid-data-on-economic-forecasting-insights-from-var2-and-arimax-models",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Final Results: Unveiling the Impact of Post-COVID Data on Economic Forecasting: Insights from VAR(2) and ARIMAX Models",
    "text": "Final Results: Unveiling the Impact of Post-COVID Data on Economic Forecasting: Insights from VAR(2) and ARIMAX Models\nThe Emergence of VAR(2) as the Front-Runner in Post-Pandemic Economic Forecasting In the realm of economic forecasting, the COVID-19 pandemic has catalyzed a paradigm shift, challenging conventional models and necessitating a reevaluation of data relevance. Our analysis reveals a compelling narrative: the Vector Autoregression (VAR) model, specifically VAR(2), emerges as a beacon of adaptability and accuracy in this new economic era.\nKey Findings:\nVAR(2) Model Proficiency: The VAR(2) model, utilizing data exclusively from the post-COVID era, significantly outstripped other models in forecasting accuracy. This model adeptly captured the increasing economic trends unique to this period, underscoring its ability to adapt to rapid changes.\nPost-COVID Data’s Preeminence: The analysis underscores the importance of focusing on recent, post-pandemic data. Traditional datasets, encompassing years prior to COVID-19, now offer less predictive value for current and future economic states. This shift is vividly reflected in the performance leap of the VAR(2) model, which thrives on post-COVID data.\nComparing VAR(2) with ARIMAX Models: A Tale of Two Methodologies\nOur journey into economic forecasting doesn’t end with VAR models. A comparison with ARIMAX (Autoregressive Integrated Moving Average with Exogenous Variables) models painted a nuanced picture:\nARIMAX Model Disparities: Models trained on all-inclusive data spanning from 1970 to 2020 paled in comparison to those trained solely on post-COVID data. This was particularly evident in the leap in adjusted R-squared values, indicating a more accurate fit when focusing on recent data.\nVAR’s Superiority: Despite the improvements seen in ARIMAX models using post-COVID data, they fell short of the predictive prowess exhibited by VAR models. This highlighted VAR’s superior ability to interpret and forecast based on the interplay between multiple economic indicators.\nDeciphering the Economic Narrative: The Role of Key Variables\nOur exploration revealed two variables as particularly influential in forecasting the yearly Consumer Price Index (CPI) in the US: Personal Consumption and Treasury Yield. These variables showcased a high correlation with the CPI, hinting at their pivotal roles in the post-pandemic economic landscape.\nPersonal Consumption: Reflecting shifts in consumer behavior during the pandemic, this variable has become a critical indicator of economic health.\nTreasury Yield: Serving as a barometer for broader economic trends and policies, Treasury Yield’s correlation with CPI underscores its significance in our current economic context.\nThe Road Ahead: Embracing Change in Economic\nForecasting In conclusion, our analysis not only highlights the VAR(2) model’s adeptness in navigating the post-COVID economic waters but also underscores the imperative to prioritize recent data for more accurate forecasting. The pandemic has irrevocably altered the economic fabric, mandating an adaptive approach to forecasting models and data selection.\nStay tuned as we continue to explore and unravel the complexities of economic forecasting in this ever-changing world."
  },
  {
    "objectID": "arma-arima-sarima-models.html#monthly-inflation-sarima-modelling",
    "href": "arma-arima-sarima-models.html#monthly-inflation-sarima-modelling",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Monthly Inflation SARIMA Modelling",
    "text": "Monthly Inflation SARIMA Modelling\n\n\nCode\n# Visualize Seasonal Component\ninflation_ts &lt;- ts(inflation_monthly, frequency = 12)  # frequency depends on your data, 12 for monthly\ninf.s=decompose(inflation_ts)$seasonal\nplot(inf.s, axes=FALSE, \n     main='Seasonal Component of Monthly Inflation in the US Over Time', xlab=\"Time\", ylab = \"Inflation\", type='c') \nQuarters = c(\"1\",\"2\",\"3\",\"4\") \npoints(inf.s, pch=Quarters, cex=1, font=4, col=1:4)\naxis(1, 1:4); abline(v=1:4, lty=2, col=gray(.7))\naxis(2); box()\n\n\n\n\n\nFrom the above seasonal component graph of the number of monthly inflation, we notice there does exist some level of seasonality in the original series. The seasonal component graph illustrates the degree of seasonal variation in the monthly inflation. The magnitude of the seasonal variation is shown on the y-axis of the graph, and it indicates how much the monthly inflation deviates from the average value for each season. The graph shows a repeating pattern in the number of monthly inflation over time, with clear peaks in the first and second quarters and troughs in the third quarter. This pattern implies that the monthly inflation in the US might be influenced by the season of the year."
  },
  {
    "objectID": "arma-arima-sarima-models.html#seasonally-differenced-monthly-inflation",
    "href": "arma-arima-sarima-models.html#seasonally-differenced-monthly-inflation",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Seasonally Differenced Monthly Inflation",
    "text": "Seasonally Differenced Monthly Inflation\n\n\nCode\n# Seasonal differenced\nattacks.diff=diff(inflation_ts,12)\nplot(attacks.diff, axes=FALSE, main='Monthly Inflation (S. differenced)',type='c', ylab = \"Inflation\") #with type='c' I get dashed lines\nQuarters = c(\"1\",\"2\",\"3\",\"4\") \npoints(attacks.diff, pch=Quarters, cex=1, font=4, col=1:4)\naxis(1, 1:4); abline(v=1:4, lty=2, col=gray(.7))\naxis(2); box()"
  },
  {
    "objectID": "arma-arima-sarima-models.html#acf-and-pacf-plots-of-seasonally-differenced-monthly-inflation",
    "href": "arma-arima-sarima-models.html#acf-and-pacf-plots-of-seasonally-differenced-monthly-inflation",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "ACF and PACF Plots of Seasonally Differenced Monthly Inflation",
    "text": "ACF and PACF Plots of Seasonally Differenced Monthly Inflation\n\n\nCode\ninflation_ts %&gt;% \n  diff(lag=12) %&gt;% \n  ggtsdisplay(main=\"ACF and PACF Plots of First Seasonal Differenced Monthly Inflation\")\n\n\n\n\n\nAfter first ordinary differencing the original series (ACF?), we saw a lot of seasonal correlation left, suggesting that first order differencing did not help in transforming the raw data into a stationary series. This differenced series cannot be used for building a robust SARIMA model. Therefore, a seasonal differencing on the original monthly inflation was performed above and we can still notice some correlation left, but lesser compared to when the raw series was differenced with first order. Therefore, it could be that D=1 and d=0. Let’s keep this as one option and let’s proceed with performing both seasonal differencing and first-order differencing the raw monthly inflation series."
  },
  {
    "objectID": "arma-arima-sarima-models.html#acf-and-pacf-plots-of-seasonally-and-first-ordered-monthly-inflation",
    "href": "arma-arima-sarima-models.html#acf-and-pacf-plots-of-seasonally-and-first-ordered-monthly-inflation",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "ACF and PACF Plots of Seasonally and First Ordered Monthly Inflation",
    "text": "ACF and PACF Plots of Seasonally and First Ordered Monthly Inflation\n\n\nCode\ninflation_ts %&gt;% \n  diff(lag=12) %&gt;% \n  diff() %&gt;%\n  ggtsdisplay(main=\"ACF and PACF Plots of Seasonally and First Order Differenced Monthly Inflation\")\n\n\n\n\n\nAfter both seasonal differencing and ordinary differencing together the raw data, the ACF and PACF plots seem to portray the least correlation than the individual differencing methods. Next, we shall difference and select the relevant p,d,q,P,D,Q values from the original monthly inflation series for our SARIMA model.\nFrom the seasonal differencing and ordinary differencing (together) ACF and PACF plots, the following combinations for p,d,q,P,D,Q are:\nq values obtained from ACF = 0,1,2,3,4 Q values obtained from ACF = 1 p values obtained from PACF = 0,1,2,3,4 P values obtained from PACF = 1,2 d (Difference) = 1 D (Seasonal Difference) = 1\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*29),nrow=29)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9) # parsimonious principle\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n            }\n          }\n        }\n      }\n    }\n  \n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n# q=0,1,2,3,4; Q=1 and PACF plot: p=0,1,2,3,4; P=1,2; D=1 and d=1\n\noutput=SARIMA.c(p1=1,p2=5,q1=1,q2=5,P1=1,P2=3,Q1=1,Q2=2,data=inflation_ts)\n#output\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n3480.647\n3485.829\n3480.650\n\n\n0\n1\n0\n0\n1\n1\n2639.797\n2650.161\n2639.806\n\n\n0\n1\n0\n1\n1\n0\n3076.969\n3087.334\n3076.978\n\n\n0\n1\n0\n1\n1\n1\n2641.209\n2656.757\n2641.228\n\n\n0\n1\n0\n2\n1\n0\n2973.552\n2989.099\n2973.570\n\n\n0\n1\n0\n2\n1\n1\n2643.163\n2663.893\n2643.194\n\n\n0\n1\n1\n0\n1\n0\n2988.168\n2998.533\n2988.178\n\n\n0\n1\n1\n0\n1\n1\n2135.257\n2150.804\n2135.275\n\n\n0\n1\n1\n1\n1\n0\n2584.681\n2600.228\n2584.699\n\n\n0\n1\n1\n1\n1\n1\n2135.320\n2156.049\n2135.350\n\n\n0\n1\n1\n2\n1\n0\n2454.577\n2475.306\n2454.608\n\n\n0\n1\n2\n0\n1\n0\n2978.301\n2993.849\n2978.320\n\n\n0\n1\n2\n0\n1\n1\n2124.433\n2145.163\n2124.464\n\n\n0\n1\n2\n1\n1\n0\n2580.931\n2601.660\n2580.961\n\n\n0\n1\n3\n0\n1\n0\n2939.600\n2960.329\n2939.630\n\n\n1\n1\n0\n0\n1\n0\n3212.549\n3222.913\n3212.558\n\n\n1\n1\n0\n0\n1\n1\n2388.030\n2403.577\n2388.048\n\n\n1\n1\n0\n1\n1\n0\n2789.768\n2805.315\n2789.786\n\n\n1\n1\n0\n1\n1\n1\n2389.776\n2410.505\n2389.806\n\n\n1\n1\n0\n2\n1\n0\n2690.159\n2710.888\n2690.189\n\n\n1\n1\n1\n0\n1\n0\n2941.689\n2957.236\n2941.708\n\n\n1\n1\n1\n0\n1\n1\n2124.106\n2144.835\n2124.136\n\n\n1\n1\n1\n1\n1\n0\n2580.136\n2600.866\n2580.167\n\n\n1\n1\n2\n0\n1\n0\n2901.388\n2922.118\n2901.419\n\n\n2\n1\n0\n0\n1\n0\n3125.210\n3140.757\n3125.228\n\n\n2\n1\n0\n0\n1\n1\n2286.999\n2307.728\n2287.029\n\n\n2\n1\n0\n1\n1\n0\n2718.776\n2739.505\n2718.806\n\n\n2\n1\n1\n0\n1\n0\n2918.961\n2939.691\n2918.992\n\n\n3\n1\n0\n0\n1\n0\n3013.639\n3034.368\n3013.669\n\n\n\n\n\n\n\nCode\ncat(\"\\n Best Model in terms of AIC: \\n\")\n\n\n\n Best Model in terms of AIC: \n\n\nCode\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n22 1 1 1 0 1 1 2124.106 2144.835 2124.136\n\n\nCode\ncat(\"\\n Best Model in terms of AICc: \\n\")\n\n\n\n Best Model in terms of AICc: \n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n22 1 1 1 0 1 1 2124.106 2144.835 2124.136\n\n\nCode\ncat(\"\\n Best Model in terms of BIC: \\n\")\n\n\n\n Best Model in terms of BIC: \n\n\nCode\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n22 1 1 1 0 1 1 2124.106 2144.835 2124.136\n\n\nThe best model with the lowest AIC, AICc and BIC metric is the SARIMA(1,1,1,0,1,1) model. The equation of the SARIMA(1,1,1,0,1,1) model is given by:\n\\[\n\\begin{align*}\n(1 - \\Phi B^s)(1 - B^s)Y_t &= (1 - \\theta B)(1 - B)X_t \\\\\n\\text{Where:} \\\\\nY_t &\\text{ is the differenced series (after applying non-seasonal and seasonal differencing).} \\\\\nX_t &\\text{ is the original time series.} \\\\\nB &\\text{ is the backshift operator. For example, } B^k X_t = X_{t-k}. \\\\\n\\theta &\\text{ is the parameter for the non-seasonal moving average component.} \\\\\n\\Phi &\\text{ is the parameter for the seasonal autoregressive component.} \\\\\ns &\\text{ is the seasonal period.}\n\\end{align*}\n\\] ## Model Diagnostics of ARIMA(1,1,1)(0,1,1)\n\n\nCode\nmodel_output &lt;- capture.output(sarima(inflation_ts, 1,1,1,0,1,1,12))\n\n\n\n\n\nStandardized Residuals: Essentially stating if the errors are white noise. The model does look stationary as it captures all the signals and essentially captures the raw white noise.\nACF Of Residuals: However, looking at the ACF of the Residuals gives us a definitive answer to whether the model is stationary. Because some spikes are not within the significance limits, the model is not being able to capture all the signal in the data.\nQ-Q Plot: The series weakly follows a normal distribution as the tails waver away significantly from the normal line.\np values of the Ljung-Box statistic: Ideally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model. Since all lag values greater than 5 have a p-value less than 0.05, residuals have remaining autocorrelations."
  },
  {
    "objectID": "arma-arima-sarima-models.html#forecast-for-the-next-3-years-using-arima111011",
    "href": "arma-arima-sarima-models.html#forecast-for-the-next-3-years-using-arima111011",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Forecast for the next 3 years using ARIMA(1,1,1)(0,1,1)",
    "text": "Forecast for the next 3 years using ARIMA(1,1,1)(0,1,1)\n\n\nCode\nfit &lt;- Arima(inflation_ts, order=c(0,1,1), seasonal=c(0,1,1))\nsummary(fit)\n\n\nSeries: inflation_ts \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.7628  -0.9416\ns.e.   0.0210   0.0093\n\nsigma^2 = 0.2897:  log likelihood = -1064.63\nAIC=2135.26   AICc=2135.27   BIC=2150.8\n\nTraining set error measures:\n                        ME      RMSE       MAE MPE MAPE      MASE       ACF1\nTraining set -0.0001692118 0.5351804 0.3408826 NaN  Inf 0.7206196 0.07360051\n\n\nCode\nfit %&gt;% forecast(h=36) %&gt;% autoplot() #next 3 years"
  },
  {
    "objectID": "arma-arima-sarima-models.html#comparing-111011-with-benchmarks",
    "href": "arma-arima-sarima-models.html#comparing-111011-with-benchmarks",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Comparing (1,1,1)(0,1,1) with Benchmarks",
    "text": "Comparing (1,1,1)(0,1,1) with Benchmarks\n\n\nCode\ncat(\"Best model metrics: \\n\")\n\n\nBest model metrics: \n\n\nCode\nfit &lt;- Arima(inflation_ts, order=c(1,1,1), seasonal=c(0,1,1))\n\nautoplot(inflation_ts) +\n  autolayer(meanf(inflation_ts, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(inflation_ts, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(inflation_ts, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(inflation_ts, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,36), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nCode\ncat(\"Best model metrics: \\n\")\n\n\nBest model metrics: \n\n\nCode\nsummary(fit)\n\n\nSeries: inflation_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.1367  -0.8271  -0.9450\ns.e.  0.0375   0.0232   0.0092\n\nsigma^2 = 0.2868:  log likelihood = -1058.05\nAIC=2124.11   AICc=2124.14   BIC=2144.84\n\nTraining set error measures:\n                       ME      RMSE       MAE MPE MAPE      MASE        ACF1\nTraining set -0.000610487 0.5323361 0.3387781 NaN  Inf 0.7161705 0.001247957\n\n\nCode\ncat(\"Snaive metrics: \\n\")\n\n\nSnaive metrics: \n\n\nCode\nf2 &lt;- snaive(inflation_ts, h=36) \n\naccuracy(f2)\n\n\n                       ME      RMSE      MAE MPE MAPE MASE      ACF1\nTraining set 0.0008879703 0.7759247 0.473041 NaN  Inf    1 0.3160833"
  },
  {
    "objectID": "arma-arima-sarima-models.html#seasonal-cross-validation-of-arima111011-and-211-using-1-step-ahead-forecasts",
    "href": "arma-arima-sarima-models.html#seasonal-cross-validation-of-arima111011-and-211-using-1-step-ahead-forecasts",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Seasonal cross validation of ARIMA(1,1,1)(0,1,1) and (2,1,1) using 1 Step Ahead Forecasts",
    "text": "Seasonal cross validation of ARIMA(1,1,1)(0,1,1) and (2,1,1) using 1 Step Ahead Forecasts\n\n\nCode\nk &lt;- 75 # minimum data length for fitting a model \nn &lt;- length(inflation_ts)\n#n-k # rest of the observations\n\nset.seed(123)\n\nfarima1 &lt;- function(x, h){forecast(Arima(inflation_ts, order=c(1,1,1),seasonal=c(0,1,1)), h=h)}\ne &lt;- tsCV(inflation_ts, farima1, h=1)\n\nMAE1 &lt;-abs(mean(e,na.rm=TRUE))\ncat(\"MAE for ARIMA(1,1,1)(0,1,1) is: \", MAE1)\n\n\nMAE for ARIMA(1,1,1)(0,1,1) is:  0.326548\n\n\nCode\nRMSE1=sqrt(mean(e^2, na.rm=TRUE)) #one-step time series cross-validation\ncat(\"\\nRMSE for ARIMA(1,1,1)(0,1,1) is: \", RMSE1)\n\n\n\nRMSE for ARIMA(1,1,1)(0,1,1) is:  0.716712\n\n\nCode\nfarima2 &lt;- function(x, h){forecast(Arima(inflation_ts, order=c(2,1,1),seasonal=c(0,1,1)), h=h)}\ne &lt;- tsCV(inflation_ts, farima2, h=1)\n\nMAE2 &lt;-abs(mean(e,na.rm=TRUE))\ncat(\"\\nMAE for ARIMA(2,1,1)(0,1,1) is: \", MAE2)\n\n\n\nMAE for ARIMA(2,1,1)(0,1,1) is:  0.32673\n\n\nCode\nRMSE2=sqrt(mean(e^2, na.rm=TRUE)) #one-step time series cross-validation\ncat(\"\\nRMSE for ARIMA(2,1,1)(0,1,1) is: \", RMSE2)\n\n\n\nRMSE for ARIMA(2,1,1)(0,1,1) is:  0.716795\n\n\nBoth MAE and RMSE metrics agree that ARIMA(1,1,1)(0,1,1) is the best model by a slight margin. However, the BIC metric does not agree with this result as it outputted ARIMA(0,1,1)(0,1,1) as the model with lowest BIC. AIC and AICc metrics, however, do agree with the MAE and RMSE metrics generated from Seasonal Cross Validation using 1 step ahead forecasts. Let’s see whether this is the case when forecasting 12 steps ahead."
  },
  {
    "objectID": "arma-arima-sarima-models.html#seasonal-cross-validation-of-arima111011-and-arima211011-using-12-steps-seasonal-period-ahead-forecasts",
    "href": "arma-arima-sarima-models.html#seasonal-cross-validation-of-arima111011-and-arima211011-using-12-steps-seasonal-period-ahead-forecasts",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Seasonal Cross Validation of ARIMA(1,1,1)(0,1,1) and ARIMA(2,1,1)(0,1,1) Using 12 steps (Seasonal Period) Ahead Forecasts",
    "text": "Seasonal Cross Validation of ARIMA(1,1,1)(0,1,1) and ARIMA(2,1,1)(0,1,1) Using 12 steps (Seasonal Period) Ahead Forecasts\n\n\nCode\nk &lt;- 75 # minimum data length for fitting a model \nn &lt;- length(inflation_ts)\nn-k # rest of the observations\n\n\n[1] 1254\n\n\nCode\nset.seed(123)\n\nfarima1 &lt;- function(x, h){forecast(Arima(inflation_ts, order=c(1,1,1),seasonal=c(0,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne &lt;- tsCV(inflation_ts, forecastfunction = farima1, h = 12)\n\nmse1 &lt;- colMeans(e^2, na.rm = TRUE)\n\nfarima2 &lt;- function(x, h){forecast(Arima(inflation_ts, order=c(2,1,1),seasonal=c(0,1,1)), h=h)}\n# Compute cross-validated errors for up to 12 steps ahead\ne &lt;- tsCV(inflation_ts, forecastfunction = farima2, h = 12)\n\n# Compute the MSE values and remove missing values\nmse2 &lt;- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE1 = mse1, MSE2 = mse2) %&gt;%\n  ggplot() + geom_point(aes(y=MSE1,x= h)) + geom_point(aes(y=MSE2,x= h)) +\n           geom_line(aes(y=MSE1,x= h,colour=\"MSE for ARIMA(1,1,1)(0,1,1)\")) + \n           geom_line(aes(y=MSE2,x= h,colour=\"MSE for ARIMA(2,1,1)(0,1,1)\"))+\n  theme_minimal()\n\n\n\n\n\nThis plot gives cross-validation statistics up to horizon 12. The procedure for seasonal cross validation using 12 steps ahead is very similar to seasonal cross validation using 1 step ahead. We need to change the “h” parameter to the desired the number of time horizons we want to forecast for. The farima() function manually written by us helps us call our desired SARIMA model with the number of horizons. Then, farima() function is called inside the tsCV() function, which helps us store the cross-validated errors for up to 12 steps ahead. Then, because we get forecasts for each time horizon, we need to take the mean of the squared column using colMeans to obtain MSE.\nAlthough we observed that the MSE and RMSE of ARIMA(1,1,1)(0,1,1) when forecasting 1 step ahead was lower than that of ARIMA(2,1,1)(0,1,1), from the above plot it can be seen that the cross-validated MSEs get lower or better as the number of forecasting steps increases. Both models’ MSE performance follow a very similar pattern, with ARIMA(1,1,1)(0,1,1), picked by lowest BIC, having a lower MSE across all forecasting steps, except for step 1. Therefore, ARIMA(2,1,1)(0,1,1) is the better SARIMA model!"
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#visualizing-returns-of-financial-time-series",
    "href": "financial-time-series-models(arch-garch).html#visualizing-returns-of-financial-time-series",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Visualizing Returns of Financial Time Series",
    "text": "Visualizing Returns of Financial Time Series\n\nSCHPSPYTIP\n\n\n\n\nCode\ngetSymbols(\"SCHP\", from = \"1995-03-17\",\n            to = \"2023-01-30\", src=\"yahoo\")\nschp.close&lt;- Ad(SCHP)\nreturns_SCHP = diff(log(schp.close))\nchartSeries(returns_SCHP)\n\n\n\n\n\n\n\n\n\nCode\ngetSymbols(\"SPY\", from = \"1983-03-04\",\n           to = \"2023-01-30\", src=\"yahoo\")\nspy.close&lt;- Ad(SPY)\nreturns_SPY = diff(log(spy.close))\nchartSeries(returns_SPY)\n\n\n\n\n\n\n\n\n\nCode\ngetSymbols(\"TIP\", from = \"2008-12-01\",\n           to = \"2023-01-30\", src=\"yahoo\")\ntip.close&lt;- Ad(TIP)\nreturns_TIP = diff(log(tip.close))\nchartSeries(returns_TIP)\n\n\n\n\n\n\n\n\nSCHP: The candlestick chart for SCHP shows a more mixed trend, with periods of both bullish and bearish behavior over the time period. There are several instances of long shadows, indicating significant price volatility, particularly in the early part of the time period. The candlesticks also show several instances of short-term reversals, with a few examples of bearish engulfing patterns, which may be a cause for concern for investors. Although SCHP also shows evidence of volatility clustering during three distinct periods - January 1998, December 2000, and January 2019. The candlestick chart for SCHP reveals that during these periods, there were some large bullish and bearish candlesticks.\nThe low volatility of SCHP may suggest that it could be a more stable investment option, but it is important to note that low volatility can also lead to lower returns. Additionally, the volatility clustering observed in SCHP may still pose a risk for investors who are not adequately prepared to manage the potential impact of unexpected market events.\nThe use of ARCH/GARCH models may help us understand the underlying dynamics of SCHP’s returns, despite its low volatility. By modeling the time-varying volatility of SCHP returns, we can potentially identify periods of heightened risk and better manage our investment strategy accordingly.\nThe candlestick chart can also provide additional insights into these periods of volatility clustering. For instance, during Jan 1998 and December 2000, we can see that there were several long shadows and a few bearish engulfing patterns, indicating a shift in sentiment towards selling. Similarly, in January 2019, we can see some long bullish candlesticks, which suggest a bullish sentiment among investors. These patterns can be useful for developing trading strategies and for understanding market sentiment.\nSPY: The candlestick chart for SPY shows a predominantly bullish trend, with a series of higher highs and higher lows over the course of the time period. There are a few instances of short-term bearish reversals, but overall the stock appears to be in an upward trend. The shadows of the candlesticks are generally small, indicating relatively little price volatility, but there are a few instances of longer shadows, which may be a sign of increased uncertainty or volatility.\nSPY’s returns does support the findings from the candlestick chart. We see evidence of volatility clustering during three distinct periods: around 1998-2000, the beginning of 2009-2010, and the beginning of 2020 to the end of 2021. This clustering of volatility is consistent with the findings of many financial studies, and can have important implications for trading strategies and risk management. One approach to modeling volatility clustering is to use ARCH/GARCH models, which are specifically designed to capture the time-varying volatility of financial time series data.\nIn the case of SPY, we can use the candlestick chart to gain additional insights into these periods of volatility clustering. For example, during the period from 1998-2000, we can see that there are several long shadows and a few bearish engulfing patterns, which may have contributed to the increased volatility during that time period. Similarly, during the beginning of 2009-2010 and the beginning of 2020 to the end of 2021, we can see that the candlestick chart shows increased uncertainty and volatility, with larger and more frequent bullish and bearish candlesticks.\nBy using ARCH/GARCH models to model the time-varying volatility of SPY returns, we can gain a more accurate understanding of the underlying dynamics of the stock’s behavior, and potentially identify trading opportunities or develop more effective risk management strategies.\nTIP:\nThe candlestick chart for the TIP shows a predominantly bullish trend, with a steady increase in price over the course of the time period. There are a few instances of short-term bearish reversals, but overall the trend is upward. The shadows of the candlesticks are generally small, indicating relatively little price volatility, but there are a few instances of longer shadows, which may be a sign of increased uncertainty or volatility. It’s also worth noting that the trend appears to be accelerating in the latter part of the time period, with more frequent and larger bullish candlesticks.\nThe candlestick analysis of TIP reveals a consistent pattern of volatility clustering, which is also evident in its returns. The ARCH/GARCH models can be used to capture the clustering of volatility in TIP’s returns. The clusters of high volatility are reflected in the candlestick chart as long candlesticks, indicating large price movements. The GARCH model can help identify periods of high volatility and provide insight into the potential future volatility of TIP The presence of volatility clustering in TIP’s returns suggests that market participants may be reacting to significant news or events, leading to heightened uncertainty and increased volatility. The ARCH/GARCH models can be useful tools for understanding the sources and potential impacts of such market developments, helping investors and analysts to make more informed decisions."
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#acf-and-pacf-of-returns",
    "href": "financial-time-series-models(arch-garch).html#acf-and-pacf-of-returns",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF and PACF of Returns",
    "text": "ACF and PACF of Returns\n\nSCHPSPYTIP\n\n\n\n\nCode\nreturns_SCHP %&gt;% ggtsdisplay() \n\n\n\n\n\nBoth ACF and PACF plots of SCHP’s returns (raw) since its incorporation show high correlation, which means the series is not stationary. This may also suggest that there is strong persistence or momentum in the returns. Such patterns can be captured by ARMA models and can help to inform the selection of appropriate lag lengths for ARCH/GARCH models.\n\n\n\n\nCode\nreturns_SPY %&gt;% ggtsdisplay() \n\n\n\n\n\nLike SCHP’s returns, SPY’s ACF and PACF plots show high correlation, which means the series is not stationary. An AR or ARMA model might be required to fit the series before fitting the ARCH/GARCH model.\n\n\n\n\nCode\nreturns_TIP %&gt;% ggtsdisplay() \n\n\n\n\n\nThe ACF and PACF plots of TIP returns suggest that there may be some autocorrelation present in the data. Specifically, lags 1 to 5 do not appear to be significantly correlated, but there is significant autocorrelation from lag 6 onwards. This pattern of autocorrelation may be indicative of a GARCH effect, where the volatility of the series is changing over time. The presence of significant autocorrelation at higher lags may suggest that the returns exhibit a degree of persistence or momentum, where positive (or negative) returns tend to be followed by further positive (or negative) returns. Such patterns can be captured by ARMA models and can help to inform the selection of appropriate lag lengths for ARCH/GARCH models."
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#acf-and-pacf-of-squared-returns",
    "href": "financial-time-series-models(arch-garch).html#acf-and-pacf-of-squared-returns",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF and PACF of Squared Returns",
    "text": "ACF and PACF of Squared Returns\n\nSCHPSPYTIP\n\n\n\n\nCode\nreturns_SCHP^2 %&gt;% ggtsdisplay() \n\n\n\n\n\n\n\n\n\nCode\nreturns_SPY^2 %&gt;% ggtsdisplay() \n\n\n\n\n\n\n\n\n\nCode\nreturns_TIP^2 %&gt;% ggtsdisplay()"
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#acf-and-pacf-of-absolute-returns",
    "href": "financial-time-series-models(arch-garch).html#acf-and-pacf-of-absolute-returns",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ACF and PACF of Absolute Returns",
    "text": "ACF and PACF of Absolute Returns\n\nSCHPSPYTIP\n\n\n\n\nCode\nabs(returns_SCHP) %&gt;% ggtsdisplay() \n\n\n\n\n\n\n\n\n\nCode\nabs(returns_SPY) %&gt;% ggtsdisplay() \n\n\n\n\n\n\n\n\n\nCode\nabs(returns_TIP) %&gt;% ggtsdisplay() \n\n\n\n\n\n\n\n\nAfter transforming the returns, clear correlation is discerned in both ACF and PACF plots for all three financial series when we examine their absolute and squared returns. It is likely that there is some non-linear dependence present in the data, which could be attributed to volatility clustering, signifying periods of high volatility tend to be followed by periods of high volatility, and periods of low volatility tend to be followed by periods of low volatility. This pattern can be captured by ARCH/GARCH models, which allow for the conditional variance of the series to depend on past squared returns. The decreasing significance of the ACF and PACF at higher lags may indicate that the effects of past volatility on current volatility decay over time, which can be modeled by including lagged terms of the conditional variance in the ARCH/GARCH models. Overall, the ACF and PACF plots of squared returns can provide useful information about the structure of the data and can guide the development of appropriate time series models for capturing the volatility clustering.\nTherefore, just fitting an ARCH model to each series is not enough! An ARCH model, which is designed to capture volatility clustering or autocorrelation in the squared returns, could be a good starting point. However, an ARCH model only models the conditional variance of the data and assumes that the conditional mean is constant over time. Since there is also autocorrelation in the returns themselves, then an ARMA or ARIMA model may be necessary to model the conditional mean. Therefore, we should fit an ARMA or ARIMA model first to the stocks and then fit an ARCH to the residuals of the ARMA or ARIMA model."
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#fitting-arima-model",
    "href": "financial-time-series-models(arch-garch).html#fitting-arima-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Fitting ARIMA Model",
    "text": "Fitting ARIMA Model\n\nACF and PACF of SCHP Stock’s Transformations (Log, Difference, Differenced Log)\n\nLog TransformationDifferenced TransformationDifferenced Log Transformation\n\n\n\n\nCode\nlog.schp=log(schp.close)\n\nlog.schp %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\nCode\ndiff.schp=diff(schp.close)\n\ndiff.schp %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\nCode\nlogdiff.schp=diff(log(schp.close))\n\nlogdiff.schp %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\n\nACF and PACF of SPY Stock’s Transformations (Log, Difference, Differenced Log)\n\nLog TransformationDifferenced TransformationDifferenced Log Transformation\n\n\n\n\nCode\nlog.spy=log(spy.close)\n\nlog.spy %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\nCode\ndiff.spy=diff(spy.close)\n\ndiff.spy %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\nCode\ndifflog.spy=diff(log(spy.close))\n\ndifflog.spy %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\n\nACF and PACF of TIP Index’s Transformations (Log, Difference, Differenced Log)\n\nLog TransformationDifferenced TransformationDifferenced Log Transformation\n\n\n\n\nCode\nlog.tip=log(tip.close)\n\nlog.tip %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\nCode\ndiff.tip=diff(tip.close)\n\ndiff.tip %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\nCode\nlogdiff.tip=log(diff(tip.close))\n\nlogdiff.tip %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\n\nChecking for different ARIMA(p,q,d) Combinations: SPY\n\n\nCode\n######################## Check for different combinations ########\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=1,2,3\n{\n  for(q in 1:4)# q=1,2,3\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8)\n      {\n        \n        model&lt;- Arima(log.schp,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\nCode\ncat(\"Lowest AIC model: \\n\")\ntemp[which.min(temp$AIC),] # 0,1,0\ncat(\"\\nLowest BIC model: \\n\")\ntemp[which.min(temp$BIC),] # 0,1,1\ncat(\"\\nLowest AICc model: \\n\")\ntemp[which.min(temp$AICc),] # 0,1,0\n\n\nWe shall choose ARIMA(2,1,3) as the best model for SCHP, given it has the lowest BIC, its AIC is fairly close to other, more complex ARIMA model and we shall be abiding by the principle of parsimony. Moreover, an ARMA model only captures the conditional mean, whereas an ARIMA model captures both the conditional mean and conditional variance.\n\n\nChecking for different ARIMA(p,q,d) Combinations: SPY\n\n\nCode\n######################## Check for different combinations ########\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=1,2,3\n{\n  for(q in 1:4)# q=1,2,3\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8)\n      {\n        \n        model&lt;- Arima(log.spy,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\nCode\ncat(\"Lowest AIC model: \\n\")\ntemp[which.min(temp$AIC),] # 3,1,3\ncat(\"\\nLowest BIC model: \\n\")\ntemp[which.min(temp$BIC),] # 0,1,0\ncat(\"\\nLowest AICc model: \\n\")\ntemp[which.min(temp$AICc),] # 3,1,3\n\n\nWe shall choose ARIMA(3,1,2), a random walk, as the best model for SPY, given it has the lowest BIC, its AIC is fairly close to other, more complex ARIMA model, ARIMA(3,1,3), and we shall be abiding by the principle of parsimony.\n\n\nChecking for different ARIMA(p,q,d) Combinations: TIP\n\n\nCode\n######################## Check for different combinations ########\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=1,2,3\n{\n  for(q in 1:4)# q=1,2,3\n  {\n    for(d in 0:1)\n    {\n      \n      if(p-1+d+q-1&lt;=8)\n      {\n        \n        model&lt;- Arima(tip.close,order=c(p-1,d,q-1),include.drift=TRUE) # taking log gives NA error (dont want to use CSS Method that is based conditional likelihood and does not produce same likelihood value which unconditional likelihood function produces when it is optimized for the same parameters.)\n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\nCode\ncat(\"Lowest AIC model: \\n\")\ntemp[which.min(temp$AIC),] # 3,0,3\ncat(\"\\nLowest BIC model: \\n\")\ntemp[which.min(temp$BIC),] # 0,1,0\ncat(\"\\nLowest AICc model: \\n\")\ntemp[which.min(temp$AICc),] # 3,0,3\n\n\nWe shall choose ARIMA(3,1,2), a random walk, as the best model for TIP, given it has the lowest BIC and AIC."
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#model-diagnostics",
    "href": "financial-time-series-models(arch-garch).html#model-diagnostics",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nSCHPSPYTIP\n\n\n\n\nCode\narima.schp=sarima(log.schp,2,1,3)\n\n\n\n\n\nCode\nsummary(arima.schp)\n\n\nThe plot of standardized residuals should have a mean around 0 and a variance of approximately 1. The plot for SCHP generally meets the criterion for the mean but has higher variance with clusters, indicating the need for an ARCH/GARCH model for the errors. The absence of significant lags in the ACF plot of residuals is an encouraging sign. The qq-plot indicates some signs of normality with slight skew at the tails. In addition, the p-values for the Ljung-Box test are above 0.05 for many lags, suggesting a well-fitted model. Ideally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent. Overall, while the variance of the standardized residuals suggests the need for an ARCH/GARCH model, the other diagnostic plots indicate a good fit for the ARIMA model.\n\n\n\n\nCode\narima.spy=sarima(log.spy,3,1,2)\n\n\n\n\n\nCode\nsummary(arima.spy)\n\n\nThe plot of standardized residuals for SPY is also centered around 0 and has significantly lower number of clusters. Moreover, the variance within those few clusters is not as high as those in SCHP’s plot of standardized residuals. In addition, the p-values for the Ljung-Box test are above 0.05 for the first few lags and the absence of significant lags in the ACF plot of residuals is an encouraging sign. The qq-plot indicates some signs of normality with slight skew at the tails.\n\n\n\n\nCode\narima.tip=sarima(tip.close,3,1,2)\n\n\n\n\n\nCode\nsummary(arima.tip)\n\n\nThe plot of standardized residuals for TIP is also centered around 0 and has significantly lower number of clusters. Moreover, the variance within those few clusters is not as high as those in SCHP’s plot of standardized residuals. Although none of the p-values for the Ljung-Box test are above 0.05, signifying that the residuals might not be independently distributed or that they exhibit serial correlation, a random walk model can be expected to show these outputs. Regardless, the absence of significant lags in the ACF plot of residuals is an encouraging sign. The qq-plot indicates some signs of normality with slight skew at the tails."
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#plotting-squared-residuals-of-chosen-models-and-their-acfs-and-pacfs",
    "href": "financial-time-series-models(arch-garch).html#plotting-squared-residuals-of-chosen-models-and-their-acfs-and-pacfs",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Plotting Squared Residuals of Chosen Models and their ACFs and PACFs",
    "text": "Plotting Squared Residuals of Chosen Models and their ACFs and PACFs\n\nSCHPSPYTIP\n\n\n\n\nCode\narima.schp=Arima(log.schp,order=c(2,1,3))\nres.arima.schp=arima.schp$res\nsq.res.arima.schp=res.arima.schp^2\n\nsq.res.arima.schp %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\nCode\narima.spy=Arima(log.spy,order=c(3,1,2))\nres.arima.spy=arima.spy$res\nsq.res.arima.spy=res.arima.spy^2\n\nsq.res.arima.spy %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\n\nCode\narima.tip=Arima(tip.close,order=c(3,1,2))\nres.arima.tip=arima.tip$res\nsq.res.arima.tip=res.arima.tip^2\n\nsq.res.arima.tip %&gt;% ggtsdisplay()\n\n\n\n\n\n\n\n\nAll the squared residuals plots show signs of volatility clustering. After conducting the model diagnostics and perusing the ACF and PACF of the squared residuals, a GARCH(p,q) model is suitable for all financial series."
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#fitting-garchpq-models-to-residuals",
    "href": "financial-time-series-models(arch-garch).html#fitting-garchpq-models-to-residuals",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Fitting GARCH(p,q) Models to Residuals",
    "text": "Fitting GARCH(p,q) Models to Residuals\n\nSCHPSPYTIP\n\n\n\n\nCode\nmodel &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:10) {\n  for (q in 1:10) {\n  \nmodel[[cc]] &lt;- garch(res.arima.schp,order=c(q,p),trace=F)\ncc &lt;- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC &lt;- sapply(model, AIC) \n\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]] ## model with lowest AIC is the best and output model summary\n\n\nSCHP:\nThe outputted model is GARCH(1,1). Therefore, the final model is ARIMA(2,1,3) + GARCH(1,1)\n\n\n\n\nCode\nmodel &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:10) {\n  for (q in 1:10) {\n  \nmodel[[cc]] &lt;- garch(res.arima.spy,order=c(q,p),trace=F)\ncc &lt;- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC &lt;- sapply(model, AIC) \n\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]] ## model with lowest AIC is the best and output model summary\n\n\nSPY:\nThe outputted model is GARCH(2,2). Therefore, the final model is ARIMA(3,1,2) + GARCH(2,2)\n\n\n\n\nCode\nmodel &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:20) {\n  for (q in 1:20) {\n  \nmodel[[cc]] &lt;- garch(res.arima.tip,order=c(q,p),trace=F)\ncc &lt;- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC &lt;- sapply(model, AIC) \n\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]] ## model with lowest AIC is the best and output model summary\n\n\nTIP:\nThe outputted model is GARCH(18,1). Therefore, the final model is ARIMA(3,1,2) + GARCH(18,1)"
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#fitting-arima-garch-final-model-and-conducting-box-ljung-test-on-residuals",
    "href": "financial-time-series-models(arch-garch).html#fitting-arima-garch-final-model-and-conducting-box-ljung-test-on-residuals",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Fitting ARIMA + GARCH (final model) and Conducting Box-Ljung Test on Residuals",
    "text": "Fitting ARIMA + GARCH (final model) and Conducting Box-Ljung Test on Residuals\n\nSCHPSPYTIP\n\n\n\n\nCode\nspec &lt;- ugarchspec(variance.model = list(model = \"sGARCH\", garchOrder = c(1,1)), \n                   mean.model = list(armaOrder = c(2,1, 3)), \n                   distribution.model = \"std\")\n\nfit.schp &lt;- ugarchfit(spec, data = res.arima.schp, solver = \"hybrid\")\n\n\n# Perform Box-Ljung test on residuals\ncat(\"Box-Ljung Test on Residuals based on lag = 1: \\n\")\n\n\nBox-Ljung Test on Residuals based on lag = 1: \n\n\nCode\nBox.test(fit.schp@fit$residuals, type=\"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  fit.schp@fit$residuals\nX-squared = 8.5283, df = 1, p-value = 0.003497\n\n\nCode\ncat(\"\\nBox-Ljung Test on Residuals based on lag = 10: \\n\")\n\n\n\nBox-Ljung Test on Residuals based on lag = 10: \n\n\nCode\nBox.test(fit.schp@fit$residuals, type=\"Ljung-Box\", lag=10) # not signif after lag = 11 \n\n\n\n    Box-Ljung test\n\ndata:  fit.schp@fit$residuals\nX-squared = 27.592, df = 10, p-value = 0.002098\n\n\nThe Box-Ljung Test outputs a p-value &gt; 0.05 for all lags up until 10, which signifies that the ARIMA(2,1,3) + GARCH(1,1), fitted on SCHP, captures the autocorrelation structure in the data until lag 10. This indicates that the model is robust is forecasting the volatility of future returns of SCHP.\n\n\n\n\nCode\nspec &lt;- ugarchspec(variance.model = list(model = \"sGARCH\", garchOrder = c(2,2)), \n                   mean.model = list(armaOrder = c(3,1,2)), \n                   distribution.model = \"std\")\n\nfit.spy &lt;- ugarchfit(spec, data = res.arima.spy, solver = \"hybrid\")\n\n\n# Perform Box-Ljung test (lag=1 and lag=10) on residuals\ncat(\"Box-Ljung Test on Residuals based on lag = 1: \\n\")\n\n\nBox-Ljung Test on Residuals based on lag = 1: \n\n\nCode\nBox.test(fit.spy@fit$residuals, type=\"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  fit.spy@fit$residuals\nX-squared = 8.0428, df = 1, p-value = 0.004568\n\n\nCode\ncat(\"\\nBox-Ljung Test on Residuals based on lag = 5: \\n\")\n\n\n\nBox-Ljung Test on Residuals based on lag = 5: \n\n\nCode\nBox.test(fit.spy@fit$residuals, type=\"Ljung-Box\", lag=5) # not signif after lag = 3\n\n\n\n    Box-Ljung test\n\ndata:  fit.spy@fit$residuals\nX-squared = 18.761, df = 5, p-value = 0.002129\n\n\nThe Box-Ljung Test outputs a p-value &gt; 0.05 for all lags up until 3, which signifies that the ARIMA(3,1,2) + GARCH(2,2), fitted on SPY, captures the autocorrelation structure in the data until lag 3. The model can still be employed to forecast the volatility of future returns of SPY and is a good indication that the model is capturing the important dynamics in the data.\n\n\n\n\nCode\nspec &lt;- ugarchspec(variance.model = list(model = \"sGARCH\", garchOrder = c(18,1)), \n                   mean.model = list(armaOrder = c(3,1,2)), \n                   distribution.model = \"std\")\n\nfit.tip &lt;- ugarchfit(spec, data = res.arima.tip, solver = \"hybrid\")\n\n\n# Perform Box-Ljung test (lag=1 and lag=10) on residuals\ncat(\"Box-Ljung Test on Residuals based on lag = 1: \\n\")\n\n\nBox-Ljung Test on Residuals based on lag = 1: \n\n\nCode\nBox.test(fit.tip@fit$residuals, type=\"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  fit.tip@fit$residuals\nX-squared = 14.022, df = 1, p-value = 0.0001807\n\n\nCode\ncat(\"\\nBox-Ljung Test on Residuals based on lag = 5: \\n\")\n\n\n\nBox-Ljung Test on Residuals based on lag = 5: \n\n\nCode\nBox.test(fit.tip@fit$residuals, type=\"Ljung-Box\", lag=5) # not signif after lag = 5\n\n\n\n    Box-Ljung test\n\ndata:  fit.tip@fit$residuals\nX-squared = 16.437, df = 5, p-value = 0.005702\n\n\nThe Box-Ljung Test outputs a p-value &gt; 0.05 for all lags up until 5, which signifies that the ARIMA(3,1,2) + GARCH(18,1), fitted on TIP, captures the autocorrelation structure in the data until lag 3. The AR component of the GARCH model is significantly more complex than that of the other models because it has a much higher number of GARCH terms (18) relative to the other models."
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#model-equations",
    "href": "financial-time-series-models(arch-garch).html#model-equations",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Model Equations",
    "text": "Model Equations\nSCHP: ARIMA(2,1,3) + GARCH(1,1) \\[\n\\begin{align*}\n\\text{ARIMA(2,1,3):} \\quad (1 - \\phi_1 B - \\phi_2 B^2) \\nabla Y_t &= (1 + \\theta_1 B + \\theta_2 B^2 + \\theta_3 B^3)\\epsilon_t \\\\\n\\text{GARCH(1,1):} \\quad \\sigma_t^2 &= \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2 \\\\\n\\text{Where:} \\\\\nY_t &\\text{ is the time series data.} \\\\\n\\nabla &\\text{ is the difference operator.} \\\\\n\\phi_i &\\text{ are the autoregressive parameters.} \\\\\n\\theta_j &\\text{ are the moving average parameters.} \\\\\n\\epsilon_t &\\text{ is the error term at time } t. \\\\\n\\sigma_t^2 &\\text{ is the conditional variance at time } t. \\\\\n\\omega &\\text{ is the constant term in the variance equation.} \\\\\n\\alpha &\\text{ is the parameter for the lagged squared residual.} \\\\\n\\beta &\\text{ is the parameter for the lagged conditional variance.}\n\\end{align*}\n\\]\nSPY: ARIMA(3,1,2) + GARCH(2,2) \\[\n\\begin{align*}\n\\text{ARIMA(3,1,2):} \\quad (1 - \\phi_1 B - \\phi_2 B^2 - \\phi_3 B^3) \\nabla Y_t &= (1 + \\theta_1 B + \\theta_2 B^2)\\epsilon_t \\\\\n\\text{GARCH(2,2):} \\quad \\sigma_t^2 &= \\omega + \\alpha_1 \\epsilon_{t-1}^2 + \\alpha_2 \\epsilon_{t-2}^2 + \\beta_1 \\sigma_{t-1}^2 + \\beta_2 \\sigma_{t-2}^2 \\\\\n\\text{Where:} \\\\\nY_t &\\text{ is the time series data.} \\\\\n\\nabla &\\text{ is the difference operator.} \\\\\n\\phi_i &\\text{ are the autoregressive parameters.} \\\\\n\\theta_j &\\text{ are the moving average parameters.} \\\\\n\\epsilon_t &\\text{ is the error term at time } t. \\\\\n\\sigma_t^2 &\\text{ is the conditional variance at time } t. \\\\\n\\omega &\\text{ is the constant term in the variance equation.} \\\\\n\\alpha_i &\\text{ are the parameters for the lagged squared residuals.} \\\\\n\\beta_j &\\text{ are the parameters for the lagged conditional variances.}\n\\end{align*}\n\\]\nTIP: ARIMA(3,1,2) + GARCH(18,1) \\[\n\\begin{align*}\n\\text{ARIMA(3,1,2):} \\quad (1 - \\phi_1 B - \\phi_2 B^2 - \\phi_3 B^3) \\nabla Y_t &= (1 + \\theta_1 B + \\theta_2 B^2)\\epsilon_t \\\\\n\\text{GARCH(18,1):} \\quad \\sigma_t^2 &= \\omega + \\sum_{i=1}^{18} \\alpha_i \\epsilon_{t-i}^2 + \\beta \\sigma_{t-1}^2 \\\\\n\\text{Where:} \\\\\nY_t &\\text{ is the time series data.} \\\\\n\\nabla &\\text{ is the difference operator.} \\\\\n\\phi_i &\\text{ are the autoregressive parameters.} \\\\\n\\theta_j &\\text{ are the moving average parameters.} \\\\\n\\epsilon_t &\\text{ is the error term at time } t. \\\\\n\\sigma_t^2 &\\text{ is the conditional variance at time } t. \\\\\n\\omega &\\text{ is the constant term in the variance equation.} \\\\\n\\alpha_i &\\text{ are the parameters for the lagged squared residuals (up to 18 lags in this case).} \\\\\n\\beta &\\text{ is the parameter for the lagged conditional variance.}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "financial-time-series-models(arch-garch).html#volatility-plots-for-arima-garch-models",
    "href": "financial-time-series-models(arch-garch).html#volatility-plots-for-arima-garch-models",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "Volatility Plots for ARIMA + GARCH Models",
    "text": "Volatility Plots for ARIMA + GARCH Models\nThese plots represent the estimated conditional variances of the ARIMA residuals obtained from the fitted sGARCH model. The y-axis shows the values of the estimated variances, while the x-axis represents the time period for which the variances were estimated.\n\nSCHPSPYTIP\n\n\n\n\nCode\nhhat &lt;- (fit.schp@fit$sigma^2)\nplot.ts(hhat, col=\"#005BAD\")\n\n\n\n\n\nThe volatility plot provides evidence that LMT’s stock price experienced relatively higher volatility in the following periods: end of 1998 to beginning of 2003, the beginning of the 2008 financial crisis, and the onset of the COVID-19 pandemic.\n\n\n\n\nCode\nhhat &lt;- (fit.spy@fit$sigma^2)\nplot.ts(hhat, col=\"#E61231\")\n\n\n\n\n\nThe volatility plot provides evidence that SPY’s stock price experienced relatively higher volatility in the following periods: end of 1987 to beginning of 1988 (signifying that the Black Monday stock market crash catalyzed SPY’s lowest stock price until 2019), the beginning of the 2008 financial crisis, and the onset of the COVID-19 pandemic.\n\n\n\n\nCode\nhhat &lt;- (fit.tip@fit$sigma^2)\nplot.ts(hhat)\n\n\n\n\n\nThe volatility plot provides evidence that SPY’s stock price experienced relatively higher volatility in the following periods: the onset of the COVID-19 pandemic. The index is fairly new compared to the other 2 stocks and, hence, its higher volatility is warranted, especially due to the COVID-19 pandemic."
  },
  {
    "objectID": "deep-learning-for-TS.html#summary",
    "href": "deep-learning-for-TS.html#summary",
    "title": "Deep Learning for Time Series",
    "section": "",
    "text": "In this section, we shift our focus to the analysis of monthly inflation rates in the United States using Deep Learning techniques, building upon the principles applied in previous sections where time-series models were used for financial and other types of data. Our objective is to predict the monthly inflation rates using the same univariate time-series data framework previously utilized for ARMA/ARIMA/SARIMA models.\nFor this purpose, we will employ various Recurrent Neural Network (RNN) architectures, including Dense RNN, Gated Recurrent Unit (GRU), and Long Short-Term Memory Network (LSTM). These models will be tested both with and without the implementation of L2 regularization. L2 regularization is a technique used to prevent overfitting in machine learning models by penalizing larger weights in the model’s parameters, encouraging them to move towards zero.\nBy using these advanced deep learning models, we aim to assess their performance in predicting monthly inflation rates and compare their effectiveness against traditional univariate time-series models like ARIMA and SARIMA. This comparative analysis will help us understand the strengths and limitations of both traditional and deep learning approaches in the context of economic data.\nFurthermore, we will explore the impact of regularization on the RNN models’ predictions, evaluate how far into the future these models can reliably forecast inflation rates, and compare the outcomes with those obtained from traditional ARMA/ARIMA models.\nTo implement these models, we will use the Keras library in Python, which serves as an interface for the TensorFlow framework. Our approach will be guided by the methodologies and insights presented in Francois Chollet’s “Deep Learning in Python, Second Edition” (Chollet 2021), adapting and applying these concepts specifically to the domain of monthly inflation rate analysis in the United States."
  },
  {
    "objectID": "deep-learning-for-TS.html#training-parameter",
    "href": "deep-learning-for-TS.html#training-parameter",
    "title": "Deep Learning for Time Series",
    "section": "Training Parameter",
    "text": "Training Parameter\nIn this methodology, we outline the key parameters essential for defining and training our models:\nRecurrent Hidden Units: The count of hidden units in the Simple RNN layer plays a crucial role in the model’s complexity and its ability to learn features. More hidden units enable learning more complex features, but also increase the risk of overfitting due to a higher number of parameters. We typically use a set number of units across all models.\nEpochs: An epoch represents a complete cycle of training the model on the entire dataset, including both forward and backward passes. We standardize the training process by setting the epoch count to 100 for consistency across all models.\nFractional Batch Size (f_batch): This parameter determines the batch size as a fraction of the total training dataset size. Using a fraction for batch sizing enhances parameter updating efficiency. We have fixed this fraction at 20% (0.2) of the training data.\nOptimizer: The choice of optimizer is critical for updating model parameters. We use the RMSprop optimizer, known for its effectiveness in deep learning and recurrent neural networks. It’s preferred over other optimizers like Adam and Nadam for our models.\nValidation Split: This fraction of the training data is set aside for validation purposes, aiding in monitoring model performance and mitigating overfitting. We allocate 20% of our training dataset for validation.\nActivation Function: The activation function is pivotal, especially considering the vanishing gradient issue in RNNs. The tanh function is preferred due to its ability to maintain gradients in a linear state longer than ReLU. However, for the Bidirectional LSTM model, we opt for ReLU due to its unbounded positive output, contrasting tanh’s bounded nature on both positive and negative sides.\nThese parameters form the backbone of our training methodology, ensuring consistency and robustness across different model architectures."
  },
  {
    "objectID": "deep-learning-for-TS.html#normalizing-the-data",
    "href": "deep-learning-for-TS.html#normalizing-the-data",
    "title": "Deep Learning for Time Series",
    "section": "Normalizing the data",
    "text": "Normalizing the data\nWhen handling Deep Learning models for analyzing monthly inflation data, normalizing the input features is a critical step. Even with univariate data like monthly inflation rates, it is essential to scale the values, typically within a 0-1 range. This normalization process helps in managing the input data’s scale, ensuring it is suitable for feeding into a neural network. Large or heterogeneous data values can lead to disproportionately large gradient updates, hindering the network’s ability to converge.\nHere are the key benefits of normalizing the monthly inflation data:\n\nMitigates Vanishing or Exploding Gradients: Differing scales in input features can adversely affect the gradient flow during backpropagation in neural networks. If the features have a wide range of scales, gradients corresponding to smaller-scaled features might become negligible or zero, slowing down or even stalling the network’s learning process.\nFacilitates Efficient Optimization: By normalizing input features, the optimization process becomes more streamlined. Consistent scales across features ensure that gradients do not vary wildly, helping avoid local minima traps and accelerating the network’s convergence.\nEnhances Generalization: Normalizing features can improve the model’s ability to generalize. It minimizes the impact of outliers or extreme values in the input data, leading to a more balanced and less overfit model. This is particularly crucial for data like inflation rates, where outliers can significantly skew the model’s performance.\n\nIn conclusion, normalizing the monthly inflation data into a consistent range like 0-1 is a vital preparatory step for deep learning model training, leading to better performance and more reliable predictions.\n\n\nCode\nimport pandas_datareader as pdr\nimport datetime\n\nstart = datetime.datetime(1960, 1, 1) # starting date\nend = datetime.datetime.now() # current date\n\n# Fetching US Inflation data from FRED\ninflation_data = pdr.get_data_fred('CPIAUCSL', start, end)\n\nimport numpy as np\nimport pandas as pd\n\n# Assuming 'inflation_data' is your DataFrame and it has CPI values in a column named 'CPIAUCSL'\ninflation_data['CPIAUCSL'] = pd.to_numeric(inflation_data['CPIAUCSL'], errors='coerce')\ninflation_monthly = np.diff(np.log(inflation_data['CPIAUCSL'])) * 100\n\n# Convert the result to a DataFrame if needed\ninflation_monthly_df = pd.DataFrame(inflation_monthly, columns=['Monthly Inflation'], index=inflation_data.index[1:])\n\n# Split into test and train\n# Calculate the split index\nsplit_idx = int(len(inflation_monthly_df) * 0.8)\n\n# Split the data into training and testing sets\ntrain = inflation_monthly_df.iloc[:split_idx]\ntest = inflation_monthly_df.iloc[split_idx:]\n\n# normalize values\nscaler = MinMaxScaler(feature_range=(0, 1))\ntrain_data = scaler.fit_transform(train).flatten()\ntest_data = scaler.fit_transform(test).flatten()\n\nprint(\"Train Data Shape: \", train_data.shape)\nprint(\"Test Data Shape: \", test_data.shape)\n\n\nTrain Data Shape:  (612,)\nTest Data Shape:  (153,)\n\n\n\n\nCode\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport plotly.express as px\n\n# UTILITY\ndef plotly_line_plot(t, y, title=\"Plot\", x_label=\"t: time (months)\", y_label=\"y(t): Response variable\"):\n    # GENERATE PLOTLY FIGURE\n    fig = px.line(x=t[0], y=y[0], title=title, render_mode='SVG')  \n    \n    # ADD MORE\n    for i in range(1, len(y)):\n        if len(t[i]) == 1:\n            fig.add_scatter(x=t[i], y=y[i], name='Training' if i==1 else 'Validation')\n        else:\n            fig.add_scatter(x=t[i], y=y[i], mode='lines', name='Training' if i==1 else 'Validation')\n\n    fig.update_layout(\n        xaxis_title=x_label,\n        yaxis_title=y_label,\n        template=\"plotly_white\",\n        showlegend=True\n    )\n    fig.show()\n\n\n\n\nCode\n# PREPARE THE INPUT X AND TARGET Y\ndef get_XY(dat, time_steps,plot_data_partition=False):\n    global X_ind,X,Y_ind,Y #use for plotting later\n\n    # INDICES OF TARGET ARRAY\n    # Y_ind [  12   24   36   48 ..]; print(np.arange(1,12,1)); exit()\n    Y_ind = np.arange(time_steps, len(dat), time_steps); #print(Y_ind); exit()\n    Y = dat[Y_ind]\n\n    # PREPARE X\n    rows_x = len(Y)\n    X_ind=[*range(time_steps*rows_x)]\n    del X_ind[::time_steps] #if time_steps=10 remove every 10th entry\n    X = dat[X_ind]; \n\n    #PLOT\n    if(plot_data_partition):\n        plt.figure(figsize=(15, 6), dpi=80)\n        plt.plot(Y_ind, Y,'o',X_ind, X,'-'); plt.show(); \n\n    #RESHAPE INTO KERAS FORMAT\n    X1 = np.reshape(X, (rows_x, time_steps-1, 1))\n    # print([*X_ind]); print(X1); print(X1.shape,Y.shape); exit()\n\n    return X1, Y\n\n\n#PARTITION DATA\np=10 # simpilar to AR(p) given time_steps data points, predict time_steps+1 point (make prediction one month in future)\n\ntestX, testY = get_XY(test_data, p)\ntrainX, trainY = get_XY(train_data, p)\n\nprint(\"testX shape:\", testX.shape, \"testY shape:\", testY.shape)\nprint(\"trainX shape:\", trainX.shape, \"trainY shape:\", trainY.shape)\nprint(type(trainX))\n\n\ntestX shape: (15, 9, 1) testY shape: (15,)\ntrainX shape: (61, 9, 1) trainY shape: (61,)\n&lt;class 'numpy.ndarray'&gt;"
  },
  {
    "objectID": "deep-learning-for-TS.html#training-a-simple-rnn",
    "href": "deep-learning-for-TS.html#training-a-simple-rnn",
    "title": "Deep Learning for Time Series",
    "section": "Training a Simple RNN",
    "text": "Training a Simple RNN\nA recurrent neural network (RNN) processes sequences by iterating through the sequence elements and maintaining a state that contains information relative to what it has seen so far. In effect, an RNN is a type of neural network that has an internal loop. The state of the RNN is reset between processing two different, independent sequences (such as two samples in a batch), so we still consider one sequence to be a single data point: a single input to the network. What changes is that this data point is no longer processed in a single step; rather, the network internally loops over sequence elements. In summary, an RNN is a for loop that reuses quantities computed during the previous iteration of the loop, nothing more. (Chollet 2021)\n\nModel and Training Parameters\nThe input to the model is a 3D tensor with shape (batch_size, timesteps, input_dim). The output of the RNN layer is fed into a Dense layer with a single output unit, which is used to generate a scalar output.\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=100\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\n\n\n\n\nCode\n#CREATE MODEL\nmodel = Sequential()\nmodel.add(SimpleRNN(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 3)                 15        \n                                                                 \n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19 (76.00 Byte)\nTrainable params: 19 (76.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nTrain Model and Visualze Performance\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, # how many times to go through the entire dataset\nbatch_size=int(f_batch*trainX.shape[0]), # 20% of training data as batch size\nvalidation_split=validation_split,  #use 20% of training data for validation\nverbose=0) #suppress messages\n\n\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)Visualize Predictions (Unnormalized Data)\n\n\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 11ms/step\nTrain MSE = 0.00463 RMSE = 0.06803\nTest MSE = 0.02617 RMSE = 0.16178\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe train RMSE (Root Mean Squared Error) outputted by the Simple RNN model is 0.07 and test RMSE is 0.16\n\n\nThe parity plot provides further evidence of the model’s accuracy and suggests that the model’s performance is better for smaller Y values, as the deviation from the line increases for larger Y values. This is likely due to the fact that the model is trained on a larger number of smaller Y values (the data contains a lot of 0 values), and thus is better at predicting smaller Y values. The Visualize Predictions plot shows that the model is able to predict the general trend of the data, and is able to predict the peaks and valleys of the data. The model does, to an extent, capture the seasonality of the data but not completely, implying overfitting! Let’s see if we can improve the model’s performance by adding L2 regularization."
  },
  {
    "objectID": "deep-learning-for-TS.html#training-a-simple-rnn-with-l2-regularization",
    "href": "deep-learning-for-TS.html#training-a-simple-rnn-with-l2-regularization",
    "title": "Deep Learning for Time Series",
    "section": "Training a Simple RNN with L2 Regularization",
    "text": "Training a Simple RNN with L2 Regularization\n\nModel and Training Parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=100 \nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\n\n\n\n\nCreate Simple RNN Model With L2 Regularization\n\n\nCode\n#CREATE MODEL\nmodel = Sequential()\nmodel.add(SimpleRNN(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \nrecurrent_regularizer=regularizers.L2(1e-2),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_1 (SimpleRNN)    (None, 3)                 15        \n                                                                 \n dense_1 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19 (76.00 Byte)\nTrainable params: 19 (76.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "deep-learning-for-TS.html#train-model-and-visualize-performance",
    "href": "deep-learning-for-TS.html#train-model-and-visualize-performance",
    "title": "Deep Learning for Time Series",
    "section": "Train Model and Visualize Performance",
    "text": "Train Model and Visualize Performance\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, # how many times to go through the entire dataset\nbatch_size=int(f_batch*trainX.shape[0]), # 20% of training data as batch size\nvalidation_split=validation_split,  #use 20% of training data for validation\nverbose=0) #suppress messages\n\n\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)Visualize Predictions (Unnormalized Data)\n\n\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 5ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 13ms/step\nTrain MSE = 0.00421 RMSE = 0.06490\nTest MSE = 0.01550 RMSE = 0.12449\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith L2 Regularization (lambda = 0.01) added to the Simple RNN model, the Train RMSE remained constant at 0.07 but test RMSE decreased from 0.16 to 0.12.\n\n\nI experimented with 50 epochs at first, then worked my way up to 100. Using the same number of epochs as previously, it appears that the regularization penalty was too great in comparison to the model’s capacity, as evidenced by the higher RMSE. The model had less time to overfit to the training set when the number of epochs was decreased, and the regularization penalty was able to improve the model’s ability to generalize."
  },
  {
    "objectID": "deep-learning-for-TS.html#training-and-evaluating-a-gru-with-l2-regularization",
    "href": "deep-learning-for-TS.html#training-and-evaluating-a-gru-with-l2-regularization",
    "title": "Deep Learning for Time Series",
    "section": "Training and Evaluating a GRU with L2 Regularization",
    "text": "Training and Evaluating a GRU with L2 Regularization\nThe GRU is very similar to LSTM—you can think of it as a slightly simpler, streamlined version of the LSTM architecture. It was introduced in 2014 by Cho et al. and have gained popularity in the last few years. GRU’s have become popular alternatives to LSTMs because they combine the forget and input gates into a single “update gate.” Therefore, it uses gating mechanisms to selectively update and reset the hidden state, allowing it to learn long-term dependencies more effectively. [@chollet2021deep]\n\nModel and Training Parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=100\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\n\n\n\n\nCreate GRU Model With L2 Regularization\n\n\nCode\n#CREATE MODEL\nmodel = Sequential()\nmodel.add(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \nrecurrent_regularizer=regularizers.L2(1e-2),\nactivation='tanh')\n          ) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 3)                 54        \n                                                                 \n dense_2 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58 (232.00 Byte)\nTrainable params: 58 (232.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "deep-learning-for-TS.html#train-model-and-visualize-performance-1",
    "href": "deep-learning-for-TS.html#train-model-and-visualize-performance-1",
    "title": "Deep Learning for Time Series",
    "section": "Train Model and Visualize Performance",
    "text": "Train Model and Visualize Performance\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, # how many times to go through the entire dataset\nbatch_size=int(f_batch*trainX.shape[0]), # 20% of training data as batch size\nvalidation_split=validation_split,  #use 20% of training data for validation\nverbose=0) #suppress messages\n\n\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)Visualize Predictions (Unnormalized Data)\n\n\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 12ms/step\nTrain MSE = 0.00379 RMSE = 0.06158\nTest MSE = 0.01504 RMSE = 0.12263\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe GRU with L2 Regularization performs better, as observed by its Train RMSE of 0.06 and test RMSE of 0.12, a slight drop from that of the Simple RNN models.\n\n\nAlthough this is a good sign that a more complex model performs better, the prediction plot remains fairly similar compared to those of the RNN models. Let’s try a Bidirectional GRU model, keeping the same number of epochs (100), to see if we can further improve the model’s performance."
  },
  {
    "objectID": "deep-learning-for-TS.html#training-and-evaluating-a-stacked-bidirectional-gru-with-l2-regularization",
    "href": "deep-learning-for-TS.html#training-and-evaluating-a-stacked-bidirectional-gru-with-l2-regularization",
    "title": "Deep Learning for Time Series",
    "section": "Training and Evaluating a Stacked Bidirectional GRU with L2 Regularization",
    "text": "Training and Evaluating a Stacked Bidirectional GRU with L2 Regularization\n\nModel and Training Parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=100 \nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\ncallback = EarlyStopping(monitor='loss', patience=3) # This callback will stop the training when there is no improvement in the loss for three consecutive epochs\n\n\n\n\nCreate Stacked Bidirectional GRU with L2 Regularization\n\n\nCode\nmodel = Sequential()\nmodel.add(Bidirectional(GRU(\nrecurrent_hidden_units, \nreturn_sequences=True,\ninput_shape=(trainX.shape[1],trainX.shape[2]))\n          )) \nmodel.add(Bidirectional(GRU(\nrecurrent_hidden_units,\nrecurrent_regularizer=regularizers.L2(1e-2),\nactivation='relu')\n          )) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# BUILD THE MODEL \nmodel.build(input_shape=(None, trainX.shape[1], trainX.shape[2]))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bidirectional_1 (Bidirecti  (None, 9, 6)              108       \n onal)                                                           \n                                                                 \n bidirectional_2 (Bidirecti  (None, 6)                 198       \n onal)                                                           \n                                                                 \n dense_4 (Dense)             (None, 1)                 7         \n                                                                 \n=================================================================\nTotal params: 313 (1.22 KB)\nTrainable params: 313 (1.22 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nTrain Model and Visualize Performance\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, # how many times to go through the entire dataset\nbatch_size=int(f_batch*trainX.shape[0]), # 20% of training data as batch size\nvalidation_split=validation_split,  #use 20% of training data for validation\ncallbacks=[callback], #early stopping\nverbose=0) #suppress messages\n\n\n\nVisualize Fitting History\n\n\n\n\nWARNING:tensorflow:5 out of the last 13 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x29c19f0d0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 12ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 18ms/step\nTrain MSE = 0.00407 RMSE = 0.06381\nTest MSE = 0.02427 RMSE = 0.15579\n\n\n\n                                                \n\n\n\nVisualize Parity Plot (Unnormalized Data)\n\n\n\n                                                \n\n\n\n\nVisualize Predictions (Unnormalized Data)\n\n\n\n\n\n\n\n\n\nWhen using the Stacked Bidirectional GRU, it is imperative to add return_sequences=True to stack recurrent layers on top of each other in Keras. All intermediate layers should return their full sequence of outputs (a rank-3 tensor) rather than their output at the last timestep.\n\n\nThe output of this model is the worst out of all models so far, given its train RMSE of 0.06 and test RMSE of 0.16. Therefore, the best GRU model for predicting inflation in the US is the GRU model with L2 Regularization."
  },
  {
    "objectID": "deep-learning-for-TS.html#visualize-parity-plot-unnormalized-data-3",
    "href": "deep-learning-for-TS.html#visualize-parity-plot-unnormalized-data-3",
    "title": "Deep Learning for Time Series",
    "section": "Visualize Parity Plot (Unnormalized Data)",
    "text": "Visualize Parity Plot (Unnormalized Data)"
  },
  {
    "objectID": "deep-learning-for-TS.html#visualize-predictions-unnormalized-data-3",
    "href": "deep-learning-for-TS.html#visualize-predictions-unnormalized-data-3",
    "title": "Deep Learning for Time Series",
    "section": "Visualize Predictions (Unnormalized Data)",
    "text": "Visualize Predictions (Unnormalized Data)"
  },
  {
    "objectID": "deep-learning-for-TS.html#training-and-evaluating-a-bidirectional-lstm",
    "href": "deep-learning-for-TS.html#training-and-evaluating-a-bidirectional-lstm",
    "title": "Deep Learning for Time Series",
    "section": "Training and Evaluating a Bidirectional LSTM",
    "text": "Training and Evaluating a Bidirectional LSTM\n\nModel and Training Parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=100 \nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\ncallback = EarlyStopping(monitor='loss', patience=3) # This callback will stop the training when there is no improvement in the loss for three consecutive epochs\n\n\n\n\nCreate Bidirectional LSTM\n\n\nCode\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]),\nactivation='tanh')\n          )) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# BUILD THE MODEL \nmodel.build(input_shape=(None, trainX.shape[1], trainX.shape[2]))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bidirectional_3 (Bidirecti  (None, 6)                 120       \n onal)                                                           \n                                                                 \n dense_5 (Dense)             (None, 1)                 7         \n                                                                 \n=================================================================\nTotal params: 127 (508.00 Byte)\nTrainable params: 127 (508.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nTrain Model and Visualize Performance\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, # how many times to go through the entire dataset\nbatch_size=int(f_batch*trainX.shape[0]), # 20% of training data as batch size\nvalidation_split=validation_split,  #use 20% of training data for validation\ncallbacks=[callback], #early stopping\nverbose=0) #suppress messages\n\n\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)Visualize Predictions (Unnormalized Data)\n\n\n\n\nWARNING:tensorflow:5 out of the last 13 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x29d270940&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 12ms/step\nTrain MSE = 0.00443 RMSE = 0.06656\nTest MSE = 0.02747 RMSE = 0.16574\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bidirectional LSTM with no regularization performs better than the Stacked GRU with L2 regularization, but worse than both the Bidirectional GRU and the GRU with L2 regularization. The train RMSE of this Bidirectional LSTM is 0.07 and test RMSE is 0.17. The LSTM model, which is more complex than the GRU, is likely overfitting the data. The GRU model with L2 regularization is the best model so far. Let’s see if we can improve the performance of the Bidirectional LSTM with L2 regularization by tuning the hyperparameters."
  },
  {
    "objectID": "deep-learning-for-TS.html#train-model-and-visualize-performance-3",
    "href": "deep-learning-for-TS.html#train-model-and-visualize-performance-3",
    "title": "Deep Learning for Time Series",
    "section": "Train Model and Visualize Performance",
    "text": "Train Model and Visualize Performance\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, # how many times to go through the entire dataset\nbatch_size=int(f_batch*trainX.shape[0]), # 20% of training data as batch size\nvalidation_split=validation_split,  #use 20% of training data for validation\ncallbacks=[callback], #early stopping\nverbose=0) #suppress messages\n\n\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)Visualize Predictions (Unnormalized Data)\n\n\n\n\nWARNING:tensorflow:5 out of the last 13 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x179446790&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 10ms/step\nTrain MSE = 0.00518 RMSE = 0.07198\nTest MSE = 0.03154 RMSE = 0.17759\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bidirectional LSTM with no regularization performs better than the Stacked GRU with L2 regularization, but worse than both the Bidirectional GRU and the GRU with L2 regularization. The train RMSE of this Bidirectional LSTM is 0.07 and test RMSE is 0.18. The LSTM model, which is more complex than the GRU, is likely overfitting the data. The GRU model with L2 regularization is the best model so far. Let’s see if we can improve the performance of the Bidirectional LSTM with L2 regularization by tuning the hyperparameters."
  },
  {
    "objectID": "deep-learning-for-TS.html#training-and-evaluating-a-bidirectional-gru",
    "href": "deep-learning-for-TS.html#training-and-evaluating-a-bidirectional-gru",
    "title": "Deep Learning for Time Series",
    "section": "Training and Evaluating a Bidirectional GRU",
    "text": "Training and Evaluating a Bidirectional GRU\n\nModel and Training Parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=100\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\n\n\n\n\nCreate Bidirectional GRU\n\n\nCode\n#CREATE MODEL\nmodel = Sequential()\nmodel.add(Bidirectional(GRU(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]), \nactivation='tanh')\n          )) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# BUILD THE MODEL \nmodel.build(input_shape=(None, trainX.shape[1], trainX.shape[2]))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bidirectional (Bidirection  (None, 6)                 108       \n al)                                                             \n                                                                 \n dense_3 (Dense)             (None, 1)                 7         \n                                                                 \n=================================================================\nTotal params: 115 (460.00 Byte)\nTrainable params: 115 (460.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nTrain Model and Visualize Performance\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, # how many times to go through the entire dataset\nbatch_size=int(f_batch*trainX.shape[0]), # 20% of training data as batch size\nvalidation_split=validation_split,  #use 20% of training data for validation\nverbose=0) #suppress messages\n\n\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)Visualize Predictions (Unnormalized Data)\n\n\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 12ms/step\nTrain MSE = 0.00554 RMSE = 0.07440\nTest MSE = 0.03403 RMSE = 0.18447\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Bidirectional GRU with no regularization performs worse than not only the GRU with L2 Regularization but also the Simple RNN models! Its Train RMSE of 0.07 and test RMSE of 0.18 are worse than the previous 3 models tested. This is likely due to the fact that the Bidirectional GRU is a more complex model and thus requires more regularization to prevent overfitting. Let’s introduce regularization again, but this time with a Stacked Bidirectional GRU."
  },
  {
    "objectID": "deep-learning-for-TS.html#visualize-parity-plot-unnormalized-data-4",
    "href": "deep-learning-for-TS.html#visualize-parity-plot-unnormalized-data-4",
    "title": "Deep Learning for Time Series",
    "section": "Visualize Parity Plot (Unnormalized Data)",
    "text": "Visualize Parity Plot (Unnormalized Data)"
  },
  {
    "objectID": "deep-learning-for-TS.html#visualize-predictions-unnormalized-data-4",
    "href": "deep-learning-for-TS.html#visualize-predictions-unnormalized-data-4",
    "title": "Deep Learning for Time Series",
    "section": "Visualize Predictions (Unnormalized Data)",
    "text": "Visualize Predictions (Unnormalized Data)"
  },
  {
    "objectID": "deep-learning-for-TS.html#training-and-evaluating-a-bidirectional-lstm-with-l2-regularization",
    "href": "deep-learning-for-TS.html#training-and-evaluating-a-bidirectional-lstm-with-l2-regularization",
    "title": "Deep Learning for Time Series",
    "section": "Training and Evaluating a Bidirectional LSTM with L2 Regularization",
    "text": "Training and Evaluating a Bidirectional LSTM with L2 Regularization\n\nModel and Training Parameters\n\n\nCode\n#USER PARAM\nrecurrent_hidden_units=3\nepochs=100\nf_batch=0.2    #fraction used for batch size\noptimizer=\"RMSprop\"\nvalidation_split=0.2\ncallback = EarlyStopping(monitor='loss', patience=3) # This callback will stop the training when there is no improvement in the loss for three consecutive epochs\n\n\n\n\nCreate Bidirectional LSTM with L2 Regularization\n\n\nCode\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(\nrecurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=(trainX.shape[1],trainX.shape[2]),\nrecurrent_regularizer=regularizers.L2(1e-2),\nactivation='tanh')\n          )) \n     \n#NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# BUILD THE MODEL \nmodel.build(input_shape=(None, trainX.shape[1], trainX.shape[2]))\n\n# COMPILE THE MODEL \nmodel.compile(loss='MeanSquaredError', optimizer=optimizer)\nmodel.summary()\n\n\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bidirectional_4 (Bidirecti  (None, 6)                 120       \n onal)                                                           \n                                                                 \n dense_6 (Dense)             (None, 1)                 7         \n                                                                 \n=================================================================\nTotal params: 127 (508.00 Byte)\nTrainable params: 127 (508.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nTrain Model and Visualize Performance\n\n\nCode\n#TRAIN MODEL\nhistory = model.fit(\ntrainX, trainY, \nepochs=epochs, # how many times to go through the entire dataset\nbatch_size=int(f_batch*trainX.shape[0]), # 20% of training data as batch size\nvalidation_split=validation_split,  #use 20% of training data for validation\ncallbacks=[callback], #early stopping\nverbose=0) #suppress messages\n\n\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)\n\n\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 11ms/step\nTrain MSE = 0.00383 RMSE = 0.06188\nTest MSE = 0.02236 RMSE = 0.14954\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\nVisualize Predictions (Unnormalized Data)\n\n\n\n\n\n\n\n\n\n\n\nThe Bidirectional LSTM with L2 Regularization outputted a train RMSE of 0.06 and test RMSE is 0.15. It performs worse than Bidirectional LSTM without L2 Regularization and significantly worse than the Simple RNN with L2 Regularization and the GRU with L2 Regularization. Now, we can conclude that the GRU with L2 Regularization is the best performing model. But, will it be the best when predicting the Inflation for the next 5 years instead of 2 years as seen above? Let’s find out."
  },
  {
    "objectID": "deep-learning-for-TS.html#simple-rnn-with-l2-regularization",
    "href": "deep-learning-for-TS.html#simple-rnn-with-l2-regularization",
    "title": "Deep Learning for Time Series",
    "section": "Simple RNN with L2 Regularization",
    "text": "Simple RNN with L2 Regularization\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)Visualize Predictions (Unnormalized Data)\n\n\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 11ms/step\nTrain MSE = 0.00759 RMSE = 0.08715\nTest MSE = 0.04037 RMSE = 0.20093"
  },
  {
    "objectID": "deep-learning-for-TS.html#gru-with-l2-regularization",
    "href": "deep-learning-for-TS.html#gru-with-l2-regularization",
    "title": "Deep Learning for Time Series",
    "section": "GRU with L2 Regularization",
    "text": "GRU with L2 Regularization\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)Visualize Predictions (Unnormalized Data)\n\n\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 10ms/step\nTrain MSE = 0.00522 RMSE = 0.07225\nTest MSE = 0.02395 RMSE = 0.15476"
  },
  {
    "objectID": "deep-learning-for-TS.html#bidirectional-lstm-no-regularization",
    "href": "deep-learning-for-TS.html#bidirectional-lstm-no-regularization",
    "title": "Deep Learning for Time Series",
    "section": "Bidirectional LSTM (no regularization)",
    "text": "Bidirectional LSTM (no regularization)\n\nVisualize Fitting HistoryVisualize Parity Plot (Unnormalized Data)Visualize Predictions (Unnormalized Data)\n\n\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 14ms/step\nTrain MSE = 0.00532 RMSE = 0.07296\nTest MSE = 0.02590 RMSE = 0.16094\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\nThe LSTM model demonstrates superior performance, achieving the lowest RMSE, particularly when the size of the test set is expanded. This intriguing outcome suggests that the LSTM model is more adept at recognizing and adapting to the seasonal patterns in the data, especially for predictions of the inflation. The model’s enhanced ability to forecast over extended future periods indicates its strong capacity for generalization to new, unseen data. Consequently, the LSTM model emerges as a highly recommended option for future forecasting tasks."
  },
  {
    "objectID": "introduction.html#the-big-picture",
    "href": "introduction.html#the-big-picture",
    "title": "Introduction",
    "section": "The Big Picture:",
    "text": "The Big Picture:"
  },
  {
    "objectID": "arimax-sarimax-var.html#manual-arimax-modelling-2020-2023-post-covid-19",
    "href": "arimax-sarimax-var.html#manual-arimax-modelling-2020-2023-post-covid-19",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Manual ARIMAX Modelling (2020-2023: Post COVID-19 )",
    "text": "Manual ARIMAX Modelling (2020-2023: Post COVID-19 )\nThe ARIMAX model being analyzed in this section is:\nCPI ~ Unemployment Rate + Disposable Income + Personal Consumption + Treasury Yield + Federal Funds Rate"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Aanchal Dusija",
    "section": "Education",
    "text": "Education\n\nNMIMS University | Mumbai, India\n\nB.Sc. Applied Statistics & Analytics | June 2016 - May 2019\n\nMeghnad Desai Academy of Economics | Mumbai, India\n\nPostgraduation Diploma in Economics and Finance | August 2019 - August 2020\n\nGeorgetown University | Washington, DC\n\nM.S Data Science & Analytics | Aug 2022 - May 2024 (anticipated)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Aanchal Dusija",
    "section": "Experience",
    "text": "Experience\n\nGlobalFoundries | Digital Strategic Planning Intern | June 2023 - Aug 2023\nNational Stock Exchange | Research Associate | Feb 2021 - Feb 2022\nNational Stock Exchange | Research Intern | Jan 2020 - Mar 2020"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Aanchal Dusija",
    "section": "Education",
    "text": "Education\n\nNMIMS University | Mumbai, India\n\nB.Sc. Applied Statistics & Analytics | June 2016 - May 2019\n\nMeghnad Desai Academy of Economics | Mumbai, India\n\nPostgraduation Diploma in Economics and Finance | August 2019 - August 2020\n\nGeorgetown University | Washington, DC\n\nM.S Data Science & Analytics | Aug 2022 - May 2024 (anticipated)"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Aanchal Dusija",
    "section": "Experience",
    "text": "Experience\n\nGlobalFoundries | Digital Strategic Planning Intern | June 2023 - Aug 2023\nNational Stock Exchange | Research Associate | Feb 2021 - Feb 2022\nNational Stock Exchange | Research Intern | Jan 2020 - Mar 2020"
  },
  {
    "objectID": "conclusion.html#revisiting-the-big-picture",
    "href": "conclusion.html#revisiting-the-big-picture",
    "title": "Conclusions",
    "section": "",
    "text": "In my project focused on inflation analysis in the United States, I sought to explore a central question: Can time-series modeling effectively forecast inflation rates? To address this, I utilized both univariate and multivariate time-series models, along with cutting-edge Deep Recurrent Neural Networks. My multivariate models were enriched with auxiliary datasets, including key economic indicators like the Federal Funds Rate, Consumer Price Index (CPI), and Unemployment Rate, alongside significant global events impacting the economy.\nThe initial phase of the project involved thorough exploratory data analysis to discern patterns and trends in inflation rates. I developed various time series models to analyze and predict inflation trends, taking into account different influencing factors.\nThe complexity of economic fluctuations, amplified by events like the COVID-19 pandemic, presented substantial challenges in my analysis. The COVID-19 pandemic, served as a significant outlier impacting economic conditions and inflation rates. This unprecedented event led to unique economic scenarios, such as massive fiscal stimulus, supply chain disruptions, and shifts in consumer behavior, which were critical in shaping the inflation trajectory.\nMy project delved into the impacts of the pandemic by examining inflation rates before and after the onset of COVID-19. I observed that the pandemic’s effects on inflation were profound, with initial deflationary trends due to decreased consumer spending followed by inflationary pressures as economies reopened and demand surged. The deep impact of COVID-19 was evident, disrupting typical economic patterns and presenting unique challenges in forecasting inflation rates using traditional models.\nThrough my analysis, I gained valuable insights into how extraordinary global events like the COVID-19 pandemic can drastically alter economic indicators. I also faced limitations and learned lessons about the dynamic nature of inflation and the factors influencing it. This project highlighted the importance of flexibility and adaptability in economic modeling, especially in the face of unexpected global crises."
  },
  {
    "objectID": "conclusion.html#data-and-its-complexity",
    "href": "conclusion.html#data-and-its-complexity",
    "title": "Conclusions",
    "section": "Data and its Complexity",
    "text": "Data and its Complexity\nFor the inflation analysis project, we heavily relied on data from the Federal Reserve Economic Data (FRED), Bureau of Labor Statistics (BLS), and The World Bank, encompassing a broad spectrum of economic indicators. The decision to focus on the United States for our analysis was arbitrary, given that inflation trends can vary significantly across different global economies. It’s important to note that inflation dynamics in the U.S. might not mirror those in other regions with different economic structures and policies.\nUtilizing these sources, we created time-series datasets, grouping data into months or years to observe trends and fluctuations in inflation rates. A challenge we encountered was dealing with periods of low or negative inflation (deflation), particularly during economic downturns, which posed difficulties in identifying clear patterns. Specialized time-series models capable of handling such nuances were therefore essential.\nAdditionally, the data from the early 1970s and 1980s in some of our datasets, such as those from The World Bank, were incomplete or sparse, necessitating techniques for imputing missing values to construct more robust and accurate multivariate models.\nThe data sources provided detailed insights into various components of inflation, such as the Consumer Price Index (CPI) across different categories, unemployment rates, and interest rates. While having extensive data is advantageous, it also introduces the challenge of managing complexity and ensuring that the analysis remains focused and interpretable. With numerous potential variables to consider, such as wage growth, energy prices, and monetary policies, the task was to develop a model that was comprehensive yet not overly complicated. The goal was to balance the richness of the data with the need for a model that is both parsimonious and interpretable, capable of capturing the essential dynamics of inflation without being bogged down by excessive detail.\nA key aspect that could have been explored further in the analysis is the impact of extraordinary global events, such as the Financial Crisis, on inflation rates. The inflation analysis could have included a deeper investigation into the effects of such significant events. However, given the complexity and rarity of these occurrences, their inclusion posed challenges in terms of data availability and the ability to draw generalized conclusions from such exceptional circumstances."
  },
  {
    "objectID": "conclusion.html#results-and-future-work",
    "href": "conclusion.html#results-and-future-work",
    "title": "Conclusions",
    "section": "Results and Future Work",
    "text": "Results and Future Work\nIn the inflation analysis project, the SARIMA model and Deep Recurrent Neural Networks stood out as the most effective in forecasting monthly inflation rates in the United States. For multivariate models like VAR and ARIMAX, yearly aggregated data was used due to the lack of monthly granularity in some of the auxiliary datasets, such as those from the World Bank. This study highlighted that while complex models like Deep Recurrent Neural Networks can handle large datasets and potentially offer nuanced predictions, their performance in simpler tasks can sometimes be matched or even surpassed by less complex models.\nThe project’s findings align with the principle that the complexity of a model should be commensurate with the complexity of the task at hand. This approach was particularly relevant in handling periods of unusual economic activity, like the deflationary and inflationary phases surrounding the COVID-19 pandemic. For future work, considering models specifically tailored to handle unique economic scenarios, such as Zero-Inflated models for periods of low inflation or deflation, could be beneficial. These models might offer more precise predictions in cases where traditional models struggle due to the unique characteristics of the data.\nThis project underscores the wisdom of George Box’s adage, “All models are wrong, but some are useful.” It suggests that the choice of a model should be guided not just by the data available but also by the specific economic phenomena and research questions under consideration. Moving forward, further exploration of models that are particularly adept at handling the complex, dynamic nature of inflation—especially under extraordinary circumstances like global pandemics or financial crises—could provide more insightful and practical forecasts. Additionally, integrating global economic indicators and considering their interplay with U.S. inflation could offer a more comprehensive understanding of the factors driving inflationary trends."
  }
]